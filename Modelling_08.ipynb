{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c310cb",
   "metadata": {},
   "source": [
    "## Modelling notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c906e0d4",
   "metadata": {},
   "source": [
    "### Group 8 Members\n",
    "#### Spring Semester 2024-2025\n",
    "- Alexandre Gonçalves - 20240738\n",
    "- Bráulio Damba - 20240007\n",
    "- Hugo Fonseca - 20240520\n",
    "- Ricardo Pereira - 20240745\n",
    "- Victoria Goon - 20240550"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36caeff",
   "metadata": {},
   "source": [
    "## 0 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f520af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ricardo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ricardo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ricardo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/ricardo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Text extraction \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from collections import Counter\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models import FastText\n",
    "\n",
    "import scipy.sparse\n",
    "from scipy import sparse\n",
    "\n",
    "import contractions\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import pickle\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Deep Learning libraries\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Bidirectional, Dropout, Flatten, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Set pd options to display all columns and rows\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_rows\", 30)\n",
    "pd.set_option('display.max_colwidth', None)  # Show full text without truncation\n",
    "\n",
    "import glob\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Bidirectional, GRU, Flatten, Dense\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "#from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "\n",
    "#import keras_tuner as kt\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"xgboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ad0dfe",
   "metadata": {},
   "source": [
    "### Import the encoded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5055090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory (where the notebook is)\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# Construct full paths to the CSV files\n",
    "train_path = os.path.join(BASE_DIR, \"data\", \"train.csv\")\n",
    "test_path = os.path.join(BASE_DIR, \"data\", \"test.csv\")\n",
    "\n",
    "# Load the datasets\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "702a2ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '.'  # Use '.' if running in the current directory, or the path to your folder\n",
    "\n",
    "# List all .npy and .npz files\n",
    "npy_files = glob.glob(os.path.join(folder, '*.npy'))\n",
    "npz_files = glob.glob(os.path.join(folder, '*.npz'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adfc4a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_embeddings = {}\n",
    "\n",
    "for f in npy_files:\n",
    "    name = os.path.splitext(os.path.basename(f))[0]\n",
    "    npy_embeddings[name] = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f512e",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_embeddings = {}\n",
    "\n",
    "for f in npz_files:\n",
    "    name = os.path.splitext(os.path.basename(f))[0]\n",
    "    try:\n",
    "        npz_embeddings[name] = load_npz(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to load '{f}' as a sparse matrix. Error: {e}\")\n",
    "        try:\n",
    "            npz_embeddings[name] = np.load(f)\n",
    "            print(f\"Loaded '{f}' as dense array (np.load).\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to load '{f}' as dense array too. Skipping. Error: {e2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec447810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['minilm_l12_v2_text_no_lemma_stem_with_stopwords_test', 'w2v_seq_text_no_lemma_stem_with_stopwords_train', 'val_text_no_lemma_stem_with_stopwords', 'distilroberta_v1_text_no_lemma_stem_with_stopwords_train', 'glove_seq_text_no_lemma_stem_with_stopwords_val', 'mpnet_base_v2_text_no_lemma_stem_with_stopwords_val', 'glove_seq_text_no_lemma_stem_with_stopwords_test', 'ft_seq_text_no_lemma_stem_with_stopwords_test', 'mpnet_base_v2_text_no_lemma_stem_with_stopwords_train', 'ft_seq_text_no_lemma_stem_with_stopwords_val', 'all_minilm_l6_v2_text_no_lemma_stem_with_stopwords_test', 'ft_seq_text_no_lemma_stem_with_stopwords_train', 'all_minilm_l6_v2_text_no_lemma_stem_with_stopwords_train', 'glove_seq_text_no_lemma_stem_with_stopwords_train', 'all_minilm_l6_v2_text_no_lemma_stem_with_stopwords_val', 'train_text_no_lemma_stem_with_stopwords', 'minilm_l12_v2_text_no_lemma_stem_with_stopwords_train', 'w2v_seq_text_no_lemma_stem_with_stopwords_val', 'w2v_seq_text_no_lemma_stem_with_stopwords_test', 'minilm_l12_v2_text_no_lemma_stem_with_stopwords_val', 'distilroberta_v1_text_no_lemma_stem_with_stopwords_val', 'distilroberta_v1_text_no_lemma_stem_with_stopwords_test', 'test_text_no_lemma_stem_with_stopwords', 'mpnet_base_v2_text_no_lemma_stem_with_stopwords_test'])\n",
      "dict_keys(['bow_text_no_lemma_stem_with_stopwords_train', 'bow_text_no_lemma_stem_with_stopwords_test', 'bow_text_no_lemma_stem_with_stopwords_val', 'tfidf_text_no_lemma_stem_with_stopwords_val', 'tfidf_text_no_lemma_stem_with_stopwords_train', 'tfidf_text_no_lemma_stem_with_stopwords_test'])\n"
     ]
    }
   ],
   "source": [
    "print(npy_embeddings.keys())  \n",
    "print(npz_embeddings.keys()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed1d0d9",
   "metadata": {},
   "source": [
    "We perform the split again with the same random state, in order to save space, as we don't need to save our y_train, y_val and y_test, and we can simply get it from the reproducible splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "597f487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using stratify to maintain the distribution of classes in the train, validation, and test sets \n",
    "# As our dataset is quite small, we use 80% for training, and split the remaining 20% into validation and test sets (10% each).\n",
    "\n",
    "train_df, val_test_df = train_test_split(df_train, test_size=0.2, stratify=df_train['label'], random_state=42)\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, stratify=val_test_df['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89d99ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['label']\n",
    "y_val = val_df['label']\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e23bc",
   "metadata": {},
   "source": [
    "## 1 - Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf96a41",
   "metadata": {},
   "source": [
    "### 1.1 - Traditional ML Models with Bag of Words and TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc15d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"SVC\": SVC(class_weight='balanced', random_state=42),  \n",
    "    \"XGB\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=300, class_weight='balanced', random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ab884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on bow_text_no_lemma_stem_with_stopwords...\n",
      "Training on tfidf_text_no_lemma_stem_with_stopwords...\n"
     ]
    }
   ],
   "source": [
    "statistical_methods_results = []\n",
    "\n",
    "for variant in npz_embeddings:\n",
    "    if not (variant.startswith(\"bow\") or variant.startswith(\"tfidf\")):\n",
    "        continue\n",
    "    \n",
    "    if not variant.endswith(\"_train\"):\n",
    "        continue  \n",
    "    \n",
    "    base = variant[:-6]  \n",
    "    train_X = npz_embeddings[variant]\n",
    "    val_X = npz_embeddings.get(base + \"_val\")\n",
    "    if val_X is None:\n",
    "        print(f\"Missing val split for {base}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Training on {base}...\")\n",
    "    for name, model in models.items():\n",
    "        # Fit model\n",
    "        model.fit(train_X, y_train)\n",
    "        y_pred = model.predict(val_X)\n",
    "        \n",
    "        # Metrics\n",
    "        report = classification_report(y_val, y_pred, output_dict=True)\n",
    "        statistical_methods_results.append({\n",
    "            \"variant\": base,\n",
    "            \"model\": name,\n",
    "            \"accuracy\": report[\"accuracy\"],\n",
    "            \"macro_f1\": report[\"macro avg\"][\"f1-score\"],\n",
    "            \"macro_precision\": report[\"macro avg\"][\"precision\"],\n",
    "            \"macro_recall\": report[\"macro avg\"][\"recall\"],\n",
    "            \"weighted_f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            \"weighted_precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"weighted_recall\": report[\"weighted avg\"][\"recall\"],\n",
    "        })\n",
    "\n",
    "statistical_methods = pd.DataFrame(statistical_methods_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9103201a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>weighted_precision</th>\n",
       "      <th>weighted_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bow_text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.820755</td>\n",
       "      <td>0.751468</td>\n",
       "      <td>0.768797</td>\n",
       "      <td>0.737156</td>\n",
       "      <td>0.817313</td>\n",
       "      <td>0.815842</td>\n",
       "      <td>0.820755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bow_text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.749985</td>\n",
       "      <td>0.830109</td>\n",
       "      <td>0.707580</td>\n",
       "      <td>0.819236</td>\n",
       "      <td>0.830418</td>\n",
       "      <td>0.830189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bow_text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.821803</td>\n",
       "      <td>0.753013</td>\n",
       "      <td>0.761594</td>\n",
       "      <td>0.745494</td>\n",
       "      <td>0.819788</td>\n",
       "      <td>0.818403</td>\n",
       "      <td>0.821803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bow_text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.699161</td>\n",
       "      <td>0.446410</td>\n",
       "      <td>0.771153</td>\n",
       "      <td>0.435460</td>\n",
       "      <td>0.622715</td>\n",
       "      <td>0.736911</td>\n",
       "      <td>0.699161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tfidf_text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.827044</td>\n",
       "      <td>0.755249</td>\n",
       "      <td>0.798578</td>\n",
       "      <td>0.728004</td>\n",
       "      <td>0.820296</td>\n",
       "      <td>0.822640</td>\n",
       "      <td>0.827044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tfidf_text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.814465</td>\n",
       "      <td>0.726235</td>\n",
       "      <td>0.825597</td>\n",
       "      <td>0.678027</td>\n",
       "      <td>0.800440</td>\n",
       "      <td>0.817435</td>\n",
       "      <td>0.814465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tfidf_text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.824948</td>\n",
       "      <td>0.770459</td>\n",
       "      <td>0.758573</td>\n",
       "      <td>0.784475</td>\n",
       "      <td>0.827684</td>\n",
       "      <td>0.831976</td>\n",
       "      <td>0.824948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tfidf_text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.690776</td>\n",
       "      <td>0.422575</td>\n",
       "      <td>0.790902</td>\n",
       "      <td>0.419796</td>\n",
       "      <td>0.606683</td>\n",
       "      <td>0.740076</td>\n",
       "      <td>0.690776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   variant               model  accuracy  \\\n",
       "0    bow_text_no_lemma_stem_with_stopwords                 SVC  0.820755   \n",
       "1    bow_text_no_lemma_stem_with_stopwords                 XGB  0.830189   \n",
       "2    bow_text_no_lemma_stem_with_stopwords  LogisticRegression  0.821803   \n",
       "3    bow_text_no_lemma_stem_with_stopwords                 KNN  0.699161   \n",
       "4  tfidf_text_no_lemma_stem_with_stopwords                 SVC  0.827044   \n",
       "5  tfidf_text_no_lemma_stem_with_stopwords                 XGB  0.814465   \n",
       "6  tfidf_text_no_lemma_stem_with_stopwords  LogisticRegression  0.824948   \n",
       "7  tfidf_text_no_lemma_stem_with_stopwords                 KNN  0.690776   \n",
       "\n",
       "   macro_f1  macro_precision  macro_recall  weighted_f1  weighted_precision  \\\n",
       "0  0.751468         0.768797      0.737156     0.817313            0.815842   \n",
       "1  0.749985         0.830109      0.707580     0.819236            0.830418   \n",
       "2  0.753013         0.761594      0.745494     0.819788            0.818403   \n",
       "3  0.446410         0.771153      0.435460     0.622715            0.736911   \n",
       "4  0.755249         0.798578      0.728004     0.820296            0.822640   \n",
       "5  0.726235         0.825597      0.678027     0.800440            0.817435   \n",
       "6  0.770459         0.758573      0.784475     0.827684            0.831976   \n",
       "7  0.422575         0.790902      0.419796     0.606683            0.740076   \n",
       "\n",
       "   weighted_recall  \n",
       "0         0.820755  \n",
       "1         0.830189  \n",
       "2         0.821803  \n",
       "3         0.699161  \n",
       "4         0.827044  \n",
       "5         0.814465  \n",
       "6         0.824948  \n",
       "7         0.690776  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statistical_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a52eb69",
   "metadata": {},
   "source": [
    "### 1.2 - Fixed Word Embedding Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "117392a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove = npy_embeddings['glove_seq_text_no_lemma_stem_with_stopwords_train']\n",
    "X_val_glove   = npy_embeddings['glove_seq_text_no_lemma_stem_with_stopwords_val']\n",
    "X_test_glove  = npy_embeddings['glove_seq_text_no_lemma_stem_with_stopwords_test']\n",
    "\n",
    "X_train_ft = npy_embeddings['ft_seq_text_no_lemma_stem_with_stopwords_train']\n",
    "X_val_ft = npy_embeddings['ft_seq_text_no_lemma_stem_with_stopwords_val']\n",
    "X_test_ft = npy_embeddings['ft_seq_text_no_lemma_stem_with_stopwords_test']\n",
    "\n",
    "X_train_w2v = npy_embeddings['w2v_seq_text_no_lemma_stem_with_stopwords_train']\n",
    "X_val_w2v = npy_embeddings['w2v_seq_text_no_lemma_stem_with_stopwords_val']\n",
    "X_test_w2v = npy_embeddings['w2v_seq_text_no_lemma_stem_with_stopwords_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae6eaa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_embedding_variants = {\n",
    "    \"glove\":   (X_train_glove, X_val_glove),\n",
    "    \"ft\":      (X_train_ft,    X_val_ft),\n",
    "    \"w2v\":     (X_train_w2v,   X_val_w2v)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8139f347",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "\n",
    "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
    "y_val_cat = to_categorical(y_val, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b082881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping: stop if val_loss doesn't improve for 3 epochs\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=3, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "callbacks = early_stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a46214",
   "metadata": {},
   "source": [
    "#### 1) BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "669e41df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bilstm(units, input_length, embed_size, num_classes):\n",
    "    input_ = Input(shape=(input_length, embed_size))\n",
    "    x = Bidirectional(LSTM(units, return_sequences=False, dropout=0.25, recurrent_dropout=0.25))(input_)\n",
    "    out = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=input_, outputs=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52e24035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7634, 32, 200), (7634, 32, 200))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ft.shape , X_train_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5c6199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating BiLSTM with GLOVE embeddings...\n",
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 43ms/step - accuracy: 0.6344 - loss: 0.8531 - val_accuracy: 0.7180 - val_loss: 0.6828\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.7241 - loss: 0.6769 - val_accuracy: 0.7453 - val_loss: 0.6016\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.7659 - loss: 0.5957 - val_accuracy: 0.7778 - val_loss: 0.5568\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - accuracy: 0.7804 - loss: 0.5486 - val_accuracy: 0.7987 - val_loss: 0.5314\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 52ms/step - accuracy: 0.8059 - loss: 0.4924 - val_accuracy: 0.7956 - val_loss: 0.5169\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.8159 - loss: 0.4793 - val_accuracy: 0.8103 - val_loss: 0.4881\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 47ms/step - accuracy: 0.8218 - loss: 0.4567 - val_accuracy: 0.8197 - val_loss: 0.4823\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 55ms/step - accuracy: 0.8379 - loss: 0.4096 - val_accuracy: 0.8176 - val_loss: 0.4882\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8592 - loss: 0.3761 - val_accuracy: 0.8249 - val_loss: 0.4800\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8634 - loss: 0.3463 - val_accuracy: 0.8312 - val_loss: 0.4684\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.8818 - loss: 0.3193 - val_accuracy: 0.8187 - val_loss: 0.4903\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8848 - loss: 0.3081 - val_accuracy: 0.8249 - val_loss: 0.4790\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.9004 - loss: 0.2676 - val_accuracy: 0.8239 - val_loss: 0.5075\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7059    0.6667    0.6857       144\n",
      "           1     0.7821    0.6354    0.7011       192\n",
      "           2     0.8686    0.9304    0.8984       618\n",
      "\n",
      "    accuracy                         0.8312       954\n",
      "   macro avg     0.7855    0.7442    0.7618       954\n",
      "weighted avg     0.8266    0.8312    0.8266       954\n",
      "\n",
      "\n",
      "Training and evaluating BiLSTM with FT embeddings...\n",
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 41ms/step - accuracy: 0.6364 - loss: 0.9018 - val_accuracy: 0.6478 - val_loss: 0.8750\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - accuracy: 0.6435 - loss: 0.8826 - val_accuracy: 0.6551 - val_loss: 0.8631\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 44ms/step - accuracy: 0.6554 - loss: 0.8611 - val_accuracy: 0.6520 - val_loss: 0.8555\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6440 - loss: 0.8673 - val_accuracy: 0.6520 - val_loss: 0.8620\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.6521 - loss: 0.8578 - val_accuracy: 0.6562 - val_loss: 0.8507\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.6522 - loss: 0.8540 - val_accuracy: 0.6572 - val_loss: 0.8416\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.6475 - loss: 0.8638 - val_accuracy: 0.6551 - val_loss: 0.8372\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.6527 - loss: 0.8482 - val_accuracy: 0.6541 - val_loss: 0.8278\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.6573 - loss: 0.8368 - val_accuracy: 0.6593 - val_loss: 0.8239\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6509 - loss: 0.8502 - val_accuracy: 0.6583 - val_loss: 0.8245\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.6641 - loss: 0.8291 - val_accuracy: 0.6583 - val_loss: 0.8170\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 51ms/step - accuracy: 0.6691 - loss: 0.8266 - val_accuracy: 0.6625 - val_loss: 0.8178\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 59ms/step - accuracy: 0.6590 - loss: 0.8328 - val_accuracy: 0.6583 - val_loss: 0.8113\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 52ms/step - accuracy: 0.6670 - loss: 0.8272 - val_accuracy: 0.6625 - val_loss: 0.8247\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6625 - loss: 0.8297 - val_accuracy: 0.6583 - val_loss: 0.8127\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.6862 - loss: 0.7990 - val_accuracy: 0.6656 - val_loss: 0.8077\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.6739 - loss: 0.8089 - val_accuracy: 0.6604 - val_loss: 0.8218\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.6749 - loss: 0.8060 - val_accuracy: 0.6677 - val_loss: 0.8040\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6706 - loss: 0.8160 - val_accuracy: 0.6635 - val_loss: 0.8020\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 34ms/step - accuracy: 0.6735 - loss: 0.8073 - val_accuracy: 0.6656 - val_loss: 0.7968\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6695 - loss: 0.8083 - val_accuracy: 0.6688 - val_loss: 0.7948\n",
      "Epoch 22/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.6773 - loss: 0.7980 - val_accuracy: 0.6614 - val_loss: 0.7957\n",
      "Epoch 23/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.6766 - loss: 0.8056 - val_accuracy: 0.6740 - val_loss: 0.8011\n",
      "Epoch 24/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 47ms/step - accuracy: 0.6791 - loss: 0.7997 - val_accuracy: 0.6709 - val_loss: 0.7958\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       144\n",
      "           1     0.4758    0.3073    0.3734       192\n",
      "           2     0.6976    0.9369    0.7997       618\n",
      "\n",
      "    accuracy                         0.6688       954\n",
      "   macro avg     0.3911    0.4147    0.3910       954\n",
      "weighted avg     0.5477    0.6688    0.5932       954\n",
      "\n",
      "\n",
      "Training and evaluating BiLSTM with W2V embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 65ms/step - accuracy: 0.6314 - loss: 0.9055 - val_accuracy: 0.6551 - val_loss: 0.8645\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.6568 - loss: 0.8444 - val_accuracy: 0.6625 - val_loss: 0.8215\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 62ms/step - accuracy: 0.6613 - loss: 0.8348 - val_accuracy: 0.6625 - val_loss: 0.8098\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - accuracy: 0.6736 - loss: 0.8036 - val_accuracy: 0.6709 - val_loss: 0.8007\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 44ms/step - accuracy: 0.6769 - loss: 0.8029 - val_accuracy: 0.6677 - val_loss: 0.8029\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.6708 - loss: 0.8021 - val_accuracy: 0.6698 - val_loss: 0.8007\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.6831 - loss: 0.7891 - val_accuracy: 0.6751 - val_loss: 0.7885\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.6723 - loss: 0.7979 - val_accuracy: 0.6688 - val_loss: 0.7941\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.6810 - loss: 0.7869 - val_accuracy: 0.6845 - val_loss: 0.7822\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.6914 - loss: 0.7622 - val_accuracy: 0.6782 - val_loss: 0.7691\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - accuracy: 0.6811 - loss: 0.7806 - val_accuracy: 0.6698 - val_loss: 0.8040\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6786 - loss: 0.7803 - val_accuracy: 0.6855 - val_loss: 0.7708\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6784 - loss: 0.7739 - val_accuracy: 0.6834 - val_loss: 0.7668\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6793 - loss: 0.7686 - val_accuracy: 0.6887 - val_loss: 0.7672\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6805 - loss: 0.7654 - val_accuracy: 0.6939 - val_loss: 0.7608\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6839 - loss: 0.7654 - val_accuracy: 0.7013 - val_loss: 0.7718\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - accuracy: 0.6941 - loss: 0.7484 - val_accuracy: 0.6929 - val_loss: 0.7558\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - accuracy: 0.6919 - loss: 0.7576 - val_accuracy: 0.6855 - val_loss: 0.7591\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6891 - loss: 0.7494 - val_accuracy: 0.6960 - val_loss: 0.7519\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.6925 - loss: 0.7463 - val_accuracy: 0.6950 - val_loss: 0.7494\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 40ms/step - accuracy: 0.6986 - loss: 0.7483 - val_accuracy: 0.7002 - val_loss: 0.7451\n",
      "Epoch 22/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.6853 - loss: 0.7564 - val_accuracy: 0.6876 - val_loss: 0.7548\n",
      "Epoch 23/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.6839 - loss: 0.7521 - val_accuracy: 0.6866 - val_loss: 0.7790\n",
      "Epoch 24/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.6987 - loss: 0.7384 - val_accuracy: 0.6981 - val_loss: 0.7523\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4286    0.0208    0.0397       144\n",
      "           1     0.5806    0.3750    0.4557       192\n",
      "           2     0.7205    0.9595    0.8230       618\n",
      "\n",
      "    accuracy                         0.7002       954\n",
      "   macro avg     0.5766    0.4518    0.4395       954\n",
      "weighted avg     0.6483    0.7002    0.6309       954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_fixed_encoders_bilstm = {}  \n",
    "\n",
    "for name, (X_train, X_val) in fixed_embedding_variants.items():\n",
    "    print(f\"\\nTraining and evaluating BiLSTM with {name.upper()} embeddings...\")\n",
    "\n",
    "    model = build_bilstm(\n",
    "        units=64,\n",
    "        input_length=X_train.shape[1],\n",
    "        embed_size=X_train.shape[2],\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        epochs=100,  \n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    y_pred_probs = model.predict(X_val)\n",
    "    y_pred = y_pred_probs.argmax(axis=1)\n",
    "    y_val_argmax = y_val.argmax(axis=1) if y_val.ndim > 1 else y_val\n",
    "\n",
    "    print(classification_report(y_val_argmax, y_pred, digits=4))\n",
    "\n",
    "    \n",
    "    results_fixed_encoders_bilstm[name] = classification_report(y_val_argmax, y_pred, digits=4, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67051d4f",
   "metadata": {},
   "source": [
    "#### 2) BiLSTM + Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa1bf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SimpleAttention, self).__init__(**kwargs)\n",
    "        self.dense = Dense(1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        score = tf.nn.softmax(self.dense(inputs), axis=1)  \n",
    "        context = tf.reduce_sum(score * inputs, axis=1)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d83380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bilstm_att(units, input_length, embed_size, num_classes=3):\n",
    "    input_ = Input(shape=(input_length, embed_size)) \n",
    "\n",
    "    # BiLSTM with return_sequences=True so Attention can work on the sequence\n",
    "    x = Bidirectional(LSTM(units, return_sequences=True, dropout=0.25, recurrent_dropout=0.25))(input_)\n",
    "    \n",
    "    # Attention layer\n",
    "    x = SimpleAttention()(x)\n",
    "    \n",
    "    # Dense softmax output\n",
    "    out = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=input_, outputs=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc134ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating BiLSTM with attention layer with GLOVE embeddings...\n",
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 41ms/step - accuracy: 0.6622 - loss: 0.8514 - val_accuracy: 0.7390 - val_loss: 0.6435\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.7369 - loss: 0.6525 - val_accuracy: 0.7851 - val_loss: 0.5524\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.7808 - loss: 0.5690 - val_accuracy: 0.7904 - val_loss: 0.5301\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 60ms/step - accuracy: 0.7995 - loss: 0.5162 - val_accuracy: 0.8103 - val_loss: 0.5054\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.8092 - loss: 0.4870 - val_accuracy: 0.8187 - val_loss: 0.4930\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.8292 - loss: 0.4452 - val_accuracy: 0.8092 - val_loss: 0.4960\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.8460 - loss: 0.4073 - val_accuracy: 0.8239 - val_loss: 0.4903\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.8522 - loss: 0.3757 - val_accuracy: 0.8291 - val_loss: 0.4840\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.8731 - loss: 0.3504 - val_accuracy: 0.8281 - val_loss: 0.4826\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8837 - loss: 0.3034 - val_accuracy: 0.8113 - val_loss: 0.4932\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.8980 - loss: 0.2843 - val_accuracy: 0.8249 - val_loss: 0.5180\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.8960 - loss: 0.2839 - val_accuracy: 0.8208 - val_loss: 0.5094\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 33ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7218    0.6667    0.6931       144\n",
      "           1     0.7871    0.6354    0.7032       192\n",
      "           2     0.8589    0.9256    0.8910       618\n",
      "\n",
      "    accuracy                         0.8281       954\n",
      "   macro avg     0.7893    0.7425    0.7624       954\n",
      "weighted avg     0.8237    0.8281    0.8233       954\n",
      "\n",
      "\n",
      "Training and evaluating BiLSTM with attention layer with FT embeddings...\n",
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 44ms/step - accuracy: 0.6529 - loss: 0.9008 - val_accuracy: 0.6478 - val_loss: 0.8846\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.6500 - loss: 0.8847 - val_accuracy: 0.6478 - val_loss: 0.8771\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.6597 - loss: 0.8649 - val_accuracy: 0.6478 - val_loss: 0.8680\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6526 - loss: 0.8627 - val_accuracy: 0.6520 - val_loss: 0.8579\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6440 - loss: 0.8738 - val_accuracy: 0.6562 - val_loss: 0.8480\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - accuracy: 0.6478 - loss: 0.8615 - val_accuracy: 0.6551 - val_loss: 0.8471\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6485 - loss: 0.8523 - val_accuracy: 0.6551 - val_loss: 0.8337\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6516 - loss: 0.8486 - val_accuracy: 0.6583 - val_loss: 0.8580\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - accuracy: 0.6565 - loss: 0.8414 - val_accuracy: 0.6530 - val_loss: 0.8256\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6607 - loss: 0.8318 - val_accuracy: 0.6520 - val_loss: 0.8202\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6650 - loss: 0.8277 - val_accuracy: 0.6583 - val_loss: 0.8197\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.6581 - loss: 0.8331 - val_accuracy: 0.6656 - val_loss: 0.8094\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.6638 - loss: 0.8234 - val_accuracy: 0.6656 - val_loss: 0.8114\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.6676 - loss: 0.8147 - val_accuracy: 0.6688 - val_loss: 0.8031\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.6737 - loss: 0.8011 - val_accuracy: 0.6667 - val_loss: 0.8061\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.6682 - loss: 0.8109 - val_accuracy: 0.6719 - val_loss: 0.7981\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.6742 - loss: 0.8016 - val_accuracy: 0.6771 - val_loss: 0.7943\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.6718 - loss: 0.8004 - val_accuracy: 0.6667 - val_loss: 0.7994\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 44ms/step - accuracy: 0.6797 - loss: 0.7978 - val_accuracy: 0.6677 - val_loss: 0.7924\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.6769 - loss: 0.7986 - val_accuracy: 0.6698 - val_loss: 0.7933\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.6711 - loss: 0.8046 - val_accuracy: 0.6792 - val_loss: 0.7860\n",
      "Epoch 22/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.6784 - loss: 0.8042 - val_accuracy: 0.6782 - val_loss: 0.7910\n",
      "Epoch 23/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 57ms/step - accuracy: 0.6706 - loss: 0.8109 - val_accuracy: 0.6751 - val_loss: 0.7941\n",
      "Epoch 24/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 54ms/step - accuracy: 0.6846 - loss: 0.7850 - val_accuracy: 0.6719 - val_loss: 0.7878\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 43ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       144\n",
      "           1     0.5182    0.2969    0.3775       192\n",
      "           2     0.7002    0.9563    0.8085       618\n",
      "\n",
      "    accuracy                         0.6792       954\n",
      "   macro avg     0.4061    0.4177    0.3953       954\n",
      "weighted avg     0.5579    0.6792    0.5997       954\n",
      "\n",
      "\n",
      "Training and evaluating BiLSTM with attention layer with W2V embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 46ms/step - accuracy: 0.6364 - loss: 0.9043 - val_accuracy: 0.6478 - val_loss: 0.8605\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.6513 - loss: 0.8526 - val_accuracy: 0.6551 - val_loss: 0.8285\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6627 - loss: 0.8326 - val_accuracy: 0.6551 - val_loss: 0.8164\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.6669 - loss: 0.8102 - val_accuracy: 0.6593 - val_loss: 0.8155\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.6646 - loss: 0.8128 - val_accuracy: 0.6614 - val_loss: 0.8136\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - accuracy: 0.6596 - loss: 0.8196 - val_accuracy: 0.6635 - val_loss: 0.8091\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6655 - loss: 0.8085 - val_accuracy: 0.6688 - val_loss: 0.7952\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.6726 - loss: 0.8007 - val_accuracy: 0.6709 - val_loss: 0.7883\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.6709 - loss: 0.7994 - val_accuracy: 0.6792 - val_loss: 0.7834\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.6744 - loss: 0.7959 - val_accuracy: 0.6761 - val_loss: 0.7752\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.6805 - loss: 0.7852 - val_accuracy: 0.6834 - val_loss: 0.7816\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6784 - loss: 0.7817 - val_accuracy: 0.6719 - val_loss: 0.7999\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6888 - loss: 0.7609 - val_accuracy: 0.6761 - val_loss: 0.7751\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.6848 - loss: 0.7754 - val_accuracy: 0.6929 - val_loss: 0.7586\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.6853 - loss: 0.7683 - val_accuracy: 0.6971 - val_loss: 0.7558\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.6961 - loss: 0.7497 - val_accuracy: 0.6876 - val_loss: 0.7551\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6797 - loss: 0.7827 - val_accuracy: 0.6834 - val_loss: 0.7508\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6812 - loss: 0.7677 - val_accuracy: 0.6824 - val_loss: 0.7493\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6905 - loss: 0.7581 - val_accuracy: 0.6751 - val_loss: 0.7982\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6894 - loss: 0.7554 - val_accuracy: 0.6992 - val_loss: 0.7507\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6917 - loss: 0.7487 - val_accuracy: 0.6981 - val_loss: 0.7570\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       144\n",
      "           1     0.5542    0.2396    0.3345       192\n",
      "           2     0.6946    0.9790    0.8126       618\n",
      "\n",
      "    accuracy                         0.6824       954\n",
      "   macro avg     0.4163    0.4062    0.3824       954\n",
      "weighted avg     0.5615    0.6824    0.5937       954\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "results_fixed_encoders_bilstm_att = {}  \n",
    "\n",
    "for name, (X_train, X_val) in fixed_embedding_variants.items():\n",
    "    print(f\"\\nTraining and evaluating BiLSTM with attention layer with {name.upper()} embeddings...\")\n",
    "\n",
    "    model = build_bilstm_att(\n",
    "        units=64,\n",
    "        input_length=X_train.shape[1],\n",
    "        embed_size=X_train.shape[2],\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        epochs=100,  \n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    y_pred_probs = model.predict(X_val)\n",
    "    y_pred = y_pred_probs.argmax(axis=1)\n",
    "    y_val_argmax = y_val.argmax(axis=1) if y_val.ndim > 1 else y_val\n",
    "\n",
    "    print(classification_report(y_val_argmax, y_pred, digits=4))\n",
    "\n",
    "    results_fixed_encoders_bilstm_att[name] = classification_report(y_val_argmax, y_pred, digits=4, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f795dc",
   "metadata": {},
   "source": [
    "#### 3) BiGRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a03e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigru(units, input_length, embed_size, num_classes=3):\n",
    "    input_ = Input(shape=(input_length, embed_size))\n",
    "    x = Bidirectional(GRU(units, return_sequences=True, dropout=0.25, recurrent_dropout=0.25))(input_)\n",
    "    x = Flatten()(x)\n",
    "    out = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=input_, outputs=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677bab7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating BiGRU with GLOVE embeddings...\n",
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 47ms/step - accuracy: 0.6606 - loss: 0.8313 - val_accuracy: 0.7358 - val_loss: 0.6381\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.7668 - loss: 0.5984 - val_accuracy: 0.7673 - val_loss: 0.5873\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.7976 - loss: 0.5195 - val_accuracy: 0.7935 - val_loss: 0.5668\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.8162 - loss: 0.4623 - val_accuracy: 0.7925 - val_loss: 0.5611\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 56ms/step - accuracy: 0.8523 - loss: 0.3909 - val_accuracy: 0.7977 - val_loss: 0.5711\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.8728 - loss: 0.3352 - val_accuracy: 0.8071 - val_loss: 0.5811\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.8881 - loss: 0.2906 - val_accuracy: 0.8029 - val_loss: 0.5850\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7340    0.4792    0.5798       144\n",
      "           1     0.7361    0.5521    0.6310       192\n",
      "           2     0.8115    0.9401    0.8711       618\n",
      "\n",
      "    accuracy                         0.7925       954\n",
      "   macro avg     0.7605    0.6571    0.6939       954\n",
      "weighted avg     0.7846    0.7925    0.7788       954\n",
      "\n",
      "\n",
      "Training and evaluating BiGRU with FT embeddings...\n",
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 43ms/step - accuracy: 0.6314 - loss: 0.9266 - val_accuracy: 0.6520 - val_loss: 0.8641\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6436 - loss: 0.8841 - val_accuracy: 0.6562 - val_loss: 0.8556\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 40ms/step - accuracy: 0.6402 - loss: 0.8797 - val_accuracy: 0.6614 - val_loss: 0.8525\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.6616 - loss: 0.8544 - val_accuracy: 0.6593 - val_loss: 0.8599\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 63ms/step - accuracy: 0.6543 - loss: 0.8577 - val_accuracy: 0.6635 - val_loss: 0.8600\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6635 - loss: 0.8378 - val_accuracy: 0.6604 - val_loss: 0.8592\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.0000    0.0000    0.0000       144\n",
      "           1     0.5000    0.1510    0.2320       192\n",
      "           2     0.6719    0.9741    0.7952       618\n",
      "\n",
      "    accuracy                         0.6614       954\n",
      "   macro avg     0.3906    0.3751    0.3424       954\n",
      "weighted avg     0.5359    0.6614    0.5619       954\n",
      "\n",
      "\n",
      "Training and evaluating BiGRU with W2V embeddings...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 40ms/step - accuracy: 0.6431 - loss: 0.9076 - val_accuracy: 0.6625 - val_loss: 0.8348\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.6584 - loss: 0.8488 - val_accuracy: 0.6593 - val_loss: 0.8492\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.6651 - loss: 0.8273 - val_accuracy: 0.6677 - val_loss: 0.8109\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.6771 - loss: 0.8095 - val_accuracy: 0.6656 - val_loss: 0.8132\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.6663 - loss: 0.8143 - val_accuracy: 0.6688 - val_loss: 0.8036\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.6666 - loss: 0.8066 - val_accuracy: 0.6667 - val_loss: 0.7969\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 47ms/step - accuracy: 0.6748 - loss: 0.7993 - val_accuracy: 0.6855 - val_loss: 0.8050\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.6844 - loss: 0.7747 - val_accuracy: 0.6908 - val_loss: 0.7902\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.6899 - loss: 0.7736 - val_accuracy: 0.6824 - val_loss: 0.7838\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.6898 - loss: 0.7654 - val_accuracy: 0.6824 - val_loss: 0.7906\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.6999 - loss: 0.7562 - val_accuracy: 0.6939 - val_loss: 0.7782\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 51ms/step - accuracy: 0.6874 - loss: 0.7583 - val_accuracy: 0.6824 - val_loss: 0.7766\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.6822 - loss: 0.7665 - val_accuracy: 0.6824 - val_loss: 0.7843\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.6959 - loss: 0.7450 - val_accuracy: 0.6771 - val_loss: 0.8186\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.6805 - loss: 0.7582 - val_accuracy: 0.6761 - val_loss: 0.7707\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.6919 - loss: 0.7399 - val_accuracy: 0.6782 - val_loss: 0.7944\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6973 - loss: 0.7384 - val_accuracy: 0.6939 - val_loss: 0.7617\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.7042 - loss: 0.7189 - val_accuracy: 0.6866 - val_loss: 0.7688\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.7038 - loss: 0.7148 - val_accuracy: 0.6813 - val_loss: 0.7867\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.7018 - loss: 0.7103 - val_accuracy: 0.6960 - val_loss: 0.7566\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.7048 - loss: 0.7169 - val_accuracy: 0.6866 - val_loss: 0.7736\n",
      "Epoch 22/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.7091 - loss: 0.6964 - val_accuracy: 0.6908 - val_loss: 0.7534\n",
      "Epoch 23/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.7159 - loss: 0.6859 - val_accuracy: 0.6866 - val_loss: 0.7814\n",
      "Epoch 24/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.7212 - loss: 0.6780 - val_accuracy: 0.7013 - val_loss: 0.7638\n",
      "Epoch 25/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.7328 - loss: 0.6571 - val_accuracy: 0.6447 - val_loss: 0.8331\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3143    0.0764    0.1229       144\n",
      "           1     0.5440    0.3542    0.4290       192\n",
      "           2     0.7305    0.9385    0.8215       618\n",
      "\n",
      "    accuracy                         0.6908       954\n",
      "   macro avg     0.5296    0.4564    0.4578       954\n",
      "weighted avg     0.6301    0.6908    0.6371       954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_fixed_encoders_bigru = {}\n",
    "\n",
    "for name, (X_train, X_val) in fixed_embedding_variants.items():\n",
    "    print(f\"\\nTraining and evaluating BiGRU with {name.upper()} embeddings...\")\n",
    "\n",
    "    model = build_bigru(\n",
    "        units=64,\n",
    "        input_length=X_train.shape[1],\n",
    "        embed_size=X_train.shape[2],\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        epochs=100,  \n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    y_pred_probs = model.predict(X_val)\n",
    "    y_pred = y_pred_probs.argmax(axis=1)\n",
    "    y_val_argmax = y_val.argmax(axis=1) if y_val.ndim > 1 else y_val\n",
    "\n",
    "    print(classification_report(y_val_argmax, y_pred, digits=4))\n",
    "\n",
    "    results_fixed_encoders_bigru[name] = classification_report(y_val_argmax, y_pred, digits=4, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d428b0bd",
   "metadata": {},
   "source": [
    "#### 4) BiGRU + Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0d81764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigru_att(units, input_length, embed_size, num_classes=3):\n",
    "    input_ = Input(shape=(input_length, embed_size))\n",
    "    x = Bidirectional(GRU(units, return_sequences=True, dropout=0.25, recurrent_dropout=0.25))(input_)\n",
    "    x = SimpleAttention()(x)\n",
    "    out = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=input_, outputs=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84049aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating BiGRU with GLOVE embeddings...\n",
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - accuracy: 0.6479 - loss: 0.8490 - val_accuracy: 0.7558 - val_loss: 0.6323\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 0.7653 - loss: 0.6158 - val_accuracy: 0.7820 - val_loss: 0.5539\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.7745 - loss: 0.5723 - val_accuracy: 0.7893 - val_loss: 0.5403\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.8030 - loss: 0.5074 - val_accuracy: 0.8040 - val_loss: 0.5213\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 47ms/step - accuracy: 0.8236 - loss: 0.4664 - val_accuracy: 0.8166 - val_loss: 0.5112\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 47ms/step - accuracy: 0.8269 - loss: 0.4349 - val_accuracy: 0.8218 - val_loss: 0.4991\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 54ms/step - accuracy: 0.8440 - loss: 0.4063 - val_accuracy: 0.8270 - val_loss: 0.4938\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.8553 - loss: 0.3767 - val_accuracy: 0.8344 - val_loss: 0.4782\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.8702 - loss: 0.3460 - val_accuracy: 0.8323 - val_loss: 0.4951\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.8792 - loss: 0.3234 - val_accuracy: 0.8417 - val_loss: 0.4867\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 48ms/step - accuracy: 0.8921 - loss: 0.2923 - val_accuracy: 0.8333 - val_loss: 0.4924\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7299    0.6944    0.7117       144\n",
      "           1     0.7556    0.7083    0.7312       192\n",
      "           2     0.8791    0.9061    0.8924       618\n",
      "\n",
      "    accuracy                         0.8344       954\n",
      "   macro avg     0.7882    0.7696    0.7785       954\n",
      "weighted avg     0.8317    0.8344    0.8327       954\n",
      "\n",
      "\n",
      "Training and evaluating BiGRU with FT embeddings...\n",
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 49ms/step - accuracy: 0.6352 - loss: 0.9106 - val_accuracy: 0.6478 - val_loss: 0.8818\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.6471 - loss: 0.8787 - val_accuracy: 0.6478 - val_loss: 0.8663\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 40ms/step - accuracy: 0.6376 - loss: 0.8811 - val_accuracy: 0.6520 - val_loss: 0.8489\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 49ms/step - accuracy: 0.6488 - loss: 0.8688 - val_accuracy: 0.6530 - val_loss: 0.8432\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 65ms/step - accuracy: 0.6423 - loss: 0.8657 - val_accuracy: 0.6551 - val_loss: 0.8375\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.6610 - loss: 0.8369 - val_accuracy: 0.6551 - val_loss: 0.8356\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - accuracy: 0.6591 - loss: 0.8438 - val_accuracy: 0.6562 - val_loss: 0.8410\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.6517 - loss: 0.8486 - val_accuracy: 0.6614 - val_loss: 0.8354\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.6587 - loss: 0.8433 - val_accuracy: 0.6593 - val_loss: 0.8301\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 47ms/step - accuracy: 0.6593 - loss: 0.8322 - val_accuracy: 0.6593 - val_loss: 0.8244\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6571 - loss: 0.8372 - val_accuracy: 0.6730 - val_loss: 0.8134\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.6536 - loss: 0.8385 - val_accuracy: 0.6625 - val_loss: 0.8234\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - accuracy: 0.6755 - loss: 0.8172 - val_accuracy: 0.6646 - val_loss: 0.8140\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6613 - loss: 0.8260 - val_accuracy: 0.6771 - val_loss: 0.8093\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6611 - loss: 0.8298 - val_accuracy: 0.6709 - val_loss: 0.8047\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6672 - loss: 0.8201 - val_accuracy: 0.6792 - val_loss: 0.7991\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6798 - loss: 0.7969 - val_accuracy: 0.6740 - val_loss: 0.8053\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6813 - loss: 0.7913 - val_accuracy: 0.6866 - val_loss: 0.8012\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6825 - loss: 0.7934 - val_accuracy: 0.6792 - val_loss: 0.7965\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6754 - loss: 0.7996 - val_accuracy: 0.6803 - val_loss: 0.7865\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6669 - loss: 0.8094 - val_accuracy: 0.6876 - val_loss: 0.7876\n",
      "Epoch 22/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - accuracy: 0.6914 - loss: 0.7847 - val_accuracy: 0.6824 - val_loss: 0.7945\n",
      "Epoch 23/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.6886 - loss: 0.7820 - val_accuracy: 0.6792 - val_loss: 0.7814\n",
      "Epoch 24/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.6853 - loss: 0.7896 - val_accuracy: 0.6730 - val_loss: 0.7997\n",
      "Epoch 25/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.6788 - loss: 0.7907 - val_accuracy: 0.6771 - val_loss: 0.7955\n",
      "Epoch 26/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6842 - loss: 0.7837 - val_accuracy: 0.6834 - val_loss: 0.7850\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6667    0.0278    0.0533       144\n",
      "           1     0.5464    0.2760    0.3668       192\n",
      "           2     0.6945    0.9563    0.8046       618\n",
      "\n",
      "    accuracy                         0.6792       954\n",
      "   macro avg     0.6358    0.4200    0.4082       954\n",
      "weighted avg     0.6605    0.6792    0.6031       954\n",
      "\n",
      "\n",
      "Training and evaluating BiGRU with W2V embeddings...\n",
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 51ms/step - accuracy: 0.6266 - loss: 0.9156 - val_accuracy: 0.6478 - val_loss: 0.8549\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.6484 - loss: 0.8509 - val_accuracy: 0.6541 - val_loss: 0.8333\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 34ms/step - accuracy: 0.6558 - loss: 0.8372 - val_accuracy: 0.6551 - val_loss: 0.8392\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6572 - loss: 0.8328 - val_accuracy: 0.6604 - val_loss: 0.8198\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6624 - loss: 0.8276 - val_accuracy: 0.6625 - val_loss: 0.8123\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6766 - loss: 0.8052 - val_accuracy: 0.6646 - val_loss: 0.7996\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6811 - loss: 0.7940 - val_accuracy: 0.6677 - val_loss: 0.8030\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.6744 - loss: 0.7986 - val_accuracy: 0.6635 - val_loss: 0.8328\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.6750 - loss: 0.7967 - val_accuracy: 0.6719 - val_loss: 0.7861\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 34ms/step - accuracy: 0.6755 - loss: 0.7951 - val_accuracy: 0.6656 - val_loss: 0.7992\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.6894 - loss: 0.7706 - val_accuracy: 0.6698 - val_loss: 0.7967\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - accuracy: 0.6758 - loss: 0.7877 - val_accuracy: 0.6730 - val_loss: 0.7716\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6861 - loss: 0.7610 - val_accuracy: 0.6792 - val_loss: 0.7691\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6824 - loss: 0.7731 - val_accuracy: 0.6855 - val_loss: 0.7617\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6865 - loss: 0.7698 - val_accuracy: 0.6824 - val_loss: 0.7610\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.6893 - loss: 0.7628 - val_accuracy: 0.6813 - val_loss: 0.7612\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 44ms/step - accuracy: 0.6710 - loss: 0.7802 - val_accuracy: 0.6845 - val_loss: 0.7530\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 44ms/step - accuracy: 0.6840 - loss: 0.7702 - val_accuracy: 0.6918 - val_loss: 0.7510\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6880 - loss: 0.7589 - val_accuracy: 0.6897 - val_loss: 0.7453\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.6935 - loss: 0.7415 - val_accuracy: 0.6929 - val_loss: 0.7579\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.6944 - loss: 0.7401 - val_accuracy: 0.6824 - val_loss: 0.7555\n",
      "Epoch 22/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.6862 - loss: 0.7493 - val_accuracy: 0.7023 - val_loss: 0.7448\n",
      "Epoch 23/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.7026 - loss: 0.7409 - val_accuracy: 0.7002 - val_loss: 0.7392\n",
      "Epoch 24/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.6959 - loss: 0.7424 - val_accuracy: 0.6855 - val_loss: 0.7561\n",
      "Epoch 25/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.7000 - loss: 0.7265 - val_accuracy: 0.6918 - val_loss: 0.7412\n",
      "Epoch 26/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.6904 - loss: 0.7525 - val_accuracy: 0.6950 - val_loss: 0.7515\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3333    0.0069    0.0136       144\n",
      "           1     0.5352    0.3958    0.4551       192\n",
      "           2     0.7305    0.9563    0.8283       618\n",
      "\n",
      "    accuracy                         0.7002       954\n",
      "   macro avg     0.5330    0.4530    0.4323       954\n",
      "weighted avg     0.6313    0.7002    0.6302       954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_fixed_encoders_bigru_att = {}\n",
    "\n",
    "for name, (X_train, X_val) in fixed_embedding_variants.items():\n",
    "    print(f\"\\nTraining and evaluating BiGRU with {name.upper()} embeddings...\")\n",
    "\n",
    "    model = build_bigru_att(\n",
    "        units=64,\n",
    "        input_length=X_train.shape[1],\n",
    "        embed_size=X_train.shape[2],\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        epochs=100,  \n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Predict on validation set\n",
    "    y_pred_probs = model.predict(X_val)\n",
    "    y_pred = y_pred_probs.argmax(axis=1)\n",
    "    y_val_argmax = y_val.argmax(axis=1) if y_val.ndim > 1 else y_val\n",
    "\n",
    "    print(classification_report(y_val_argmax, y_pred, digits=4))\n",
    "\n",
    "    results_fixed_encoders_bigru_att[name] = classification_report(y_val_argmax, y_pred, digits=4, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be253092",
   "metadata": {},
   "source": [
    "### 1.3 - Sentence Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "793496af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mini_lm = npy_embeddings['minilm_l12_v2_text_no_lemma_stem_with_stopwords_train']\n",
    "X_val_mini_lm = npy_embeddings['minilm_l12_v2_text_no_lemma_stem_with_stopwords_val']\n",
    "X_test_mini_lm = npy_embeddings['minilm_l12_v2_text_no_lemma_stem_with_stopwords_test']\n",
    "\n",
    "X_train_mpnet = npy_embeddings['mpnet_base_v2_text_no_lemma_stem_with_stopwords_train']\n",
    "X_val_mpnet = npy_embeddings['mpnet_base_v2_text_no_lemma_stem_with_stopwords_val']\n",
    "X_test_mpnet = npy_embeddings['mpnet_base_v2_text_no_lemma_stem_with_stopwords_test']\n",
    "\n",
    "X_train_distilroberta = npy_embeddings['distilroberta_v1_text_no_lemma_stem_with_stopwords_train']\n",
    "X_val_distilroberta = npy_embeddings['distilroberta_v1_text_no_lemma_stem_with_stopwords_val']\n",
    "X_test_distilroberta = npy_embeddings['distilroberta_v1_text_no_lemma_stem_with_stopwords_test']\n",
    "\n",
    "X_train_allmini = npy_embeddings['all_minilm_l6_v2_text_no_lemma_stem_with_stopwords_train']\n",
    "X_val_allmini = npy_embeddings['all_minilm_l6_v2_text_no_lemma_stem_with_stopwords_val']\n",
    "X_test_allmini = npy_embeddings['all_minilm_l6_v2_text_no_lemma_stem_with_stopwords_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10e4d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_encoders_variants = {\n",
    "    \"mpnet_base_v2\":   (X_train_mini_lm, X_val_mini_lm),\n",
    "    \"distilroberta_v1\":      (X_train_distilroberta,    X_val_distilroberta),\n",
    "    \"minilm_l12_v2\":     (X_train_mini_lm,   X_val_mini_lm),\n",
    "    \"all_minilm_l6_v2\":     (X_train_allmini,   X_val_allmini)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e11dc77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7634, 384), (7634, 768), (7634, 768), (7634, 384))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mini_lm.shape, X_train_mpnet.shape, X_train_distilroberta.shape, X_train_allmini.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a657b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp_model(embedding_dim, num_classes):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(embedding_dim,)),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4848b173",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 384\n",
    "\n",
    "sentence_encoder_models = {\n",
    "    \"SVC\": SVC(class_weight='balanced', random_state=42),\n",
    "    \"XGB\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=300, class_weight='balanced', random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"MLP\": KerasClassifier(\n",
    "        model=create_mlp_model, \n",
    "        embedding_dim=embedding_dim,   \n",
    "        num_classes=num_classes,          \n",
    "        epochs=100, batch_size=32, verbose=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe58e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating SVC with MPNET_BASE_V2 embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6646    0.7569    0.7078       144\n",
      "           1     0.6147    0.6979    0.6537       192\n",
      "           2     0.8986    0.8317    0.8639       618\n",
      "\n",
      "    accuracy                         0.7935       954\n",
      "   macro avg     0.7260    0.7622    0.7418       954\n",
      "weighted avg     0.8061    0.7935    0.7980       954\n",
      "\n",
      "\n",
      "Training and evaluating XGB with MPNET_BASE_V2 embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7222    0.3611    0.4815       144\n",
      "           1     0.6667    0.4792    0.5576       192\n",
      "           2     0.7809    0.9401    0.8532       618\n",
      "\n",
      "    accuracy                         0.7600       954\n",
      "   macro avg     0.7233    0.5935    0.6307       954\n",
      "weighted avg     0.7491    0.7600    0.7376       954\n",
      "\n",
      "\n",
      "Training and evaluating LogisticRegression with MPNET_BASE_V2 embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5419    0.7639    0.6340       144\n",
      "           1     0.5075    0.7083    0.5913       192\n",
      "           2     0.9068    0.7087    0.7956       618\n",
      "\n",
      "    accuracy                         0.7170       954\n",
      "   macro avg     0.6521    0.7270    0.6737       954\n",
      "weighted avg     0.7714    0.7170    0.7301       954\n",
      "\n",
      "\n",
      "Training and evaluating KNN with MPNET_BASE_V2 embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6066    0.5139    0.5564       144\n",
      "           1     0.5485    0.5885    0.5678       192\n",
      "           2     0.8578    0.8689    0.8633       618\n",
      "\n",
      "    accuracy                         0.7589       954\n",
      "   macro avg     0.6710    0.6571    0.6625       954\n",
      "weighted avg     0.7577    0.7589    0.7575       954\n",
      "\n",
      "\n",
      "Training and evaluating MLP with MPNET_BASE_V2 embeddings...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6472 - loss: 0.8661\n",
      "Epoch 2/100\n",
      "\u001b[1m 54/239\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7444 - loss: 0.6304"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\callbacks\\early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7499 - loss: 0.6247\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7821 - loss: 0.5610\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7863 - loss: 0.5459\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8211 - loss: 0.4719\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8257 - loss: 0.4449\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8586 - loss: 0.3877\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8629 - loss: 0.3601\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.3379\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8990 - loss: 0.2821\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9033 - loss: 0.2686\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9210 - loss: 0.2232\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9207 - loss: 0.2127\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9355 - loss: 0.1863\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9413 - loss: 0.1644\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9531 - loss: 0.1496\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9478 - loss: 0.1499\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9568 - loss: 0.1276\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9631 - loss: 0.1133\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9604 - loss: 0.1132\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9646 - loss: 0.1070\n",
      "Epoch 22/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9659 - loss: 0.0979\n",
      "Epoch 23/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9624 - loss: 0.1065\n",
      "Epoch 24/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9728 - loss: 0.0818\n",
      "Epoch 25/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9727 - loss: 0.0790\n",
      "Epoch 26/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9731 - loss: 0.0751\n",
      "Epoch 27/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9695 - loss: 0.0829\n",
      "Epoch 28/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9788 - loss: 0.0693\n",
      "Epoch 29/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9775 - loss: 0.0660\n",
      "Epoch 30/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9734 - loss: 0.0765\n",
      "Epoch 31/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9769 - loss: 0.0709\n",
      "Epoch 32/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9830 - loss: 0.0523\n",
      "Epoch 33/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9828 - loss: 0.0578\n",
      "Epoch 34/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9834 - loss: 0.0513\n",
      "Epoch 35/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9802 - loss: 0.0553\n",
      "Epoch 36/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9836 - loss: 0.0503\n",
      "Epoch 37/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9861 - loss: 0.0428\n",
      "Epoch 38/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9794 - loss: 0.0628\n",
      "Epoch 39/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9781 - loss: 0.0575\n",
      "Epoch 40/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9828 - loss: 0.0521\n",
      "Epoch 41/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9840 - loss: 0.0493\n",
      "Epoch 42/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9825 - loss: 0.0499\n",
      "Epoch 43/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9821 - loss: 0.0547\n",
      "Epoch 44/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9852 - loss: 0.0439\n",
      "Epoch 45/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9815 - loss: 0.0545\n",
      "Epoch 46/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9839 - loss: 0.0455\n",
      "Epoch 47/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9869 - loss: 0.0418\n",
      "Epoch 48/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9843 - loss: 0.0432\n",
      "Epoch 49/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9820 - loss: 0.0497\n",
      "Epoch 50/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9856 - loss: 0.0393\n",
      "Epoch 51/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9866 - loss: 0.0391\n",
      "Epoch 52/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9877 - loss: 0.0405\n",
      "Epoch 53/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9886 - loss: 0.0362\n",
      "Epoch 54/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9838 - loss: 0.0465\n",
      "Epoch 55/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9871 - loss: 0.0400\n",
      "Epoch 56/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9839 - loss: 0.0411\n",
      "Epoch 57/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9851 - loss: 0.0434\n",
      "Epoch 58/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9864 - loss: 0.0387\n",
      "Epoch 59/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9869 - loss: 0.0366\n",
      "Epoch 60/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9878 - loss: 0.0336\n",
      "Epoch 61/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9893 - loss: 0.0296\n",
      "Epoch 62/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9849 - loss: 0.0440\n",
      "Epoch 63/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9872 - loss: 0.0419\n",
      "Epoch 64/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9874 - loss: 0.0359\n",
      "Epoch 65/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9847 - loss: 0.0432\n",
      "Epoch 66/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9882 - loss: 0.0376\n",
      "Epoch 67/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9908 - loss: 0.0278\n",
      "Epoch 68/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9871 - loss: 0.0406\n",
      "Epoch 69/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9883 - loss: 0.0366\n",
      "Epoch 70/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9849 - loss: 0.0397\n",
      "Epoch 71/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9874 - loss: 0.0343\n",
      "Epoch 72/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9901 - loss: 0.0306\n",
      "Epoch 73/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9886 - loss: 0.0362\n",
      "Epoch 74/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9891 - loss: 0.0308\n",
      "Epoch 75/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9881 - loss: 0.0285\n",
      "Epoch 76/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9919 - loss: 0.0277\n",
      "Epoch 77/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9871 - loss: 0.0393\n",
      "Epoch 78/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9871 - loss: 0.0331\n",
      "Epoch 79/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9917 - loss: 0.0279\n",
      "Epoch 80/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9880 - loss: 0.0303\n",
      "Epoch 81/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9924 - loss: 0.0206\n",
      "Epoch 82/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9905 - loss: 0.0258\n",
      "Epoch 83/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9907 - loss: 0.0286\n",
      "Epoch 84/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9893 - loss: 0.0316\n",
      "Epoch 85/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9924 - loss: 0.0239\n",
      "Epoch 86/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9905 - loss: 0.0296\n",
      "Epoch 87/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9927 - loss: 0.0228\n",
      "Epoch 88/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9891 - loss: 0.0282\n",
      "Epoch 89/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9887 - loss: 0.0345\n",
      "Epoch 90/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9878 - loss: 0.0360\n",
      "Epoch 91/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9901 - loss: 0.0313\n",
      "Epoch 92/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9872 - loss: 0.0329\n",
      "Epoch 93/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9915 - loss: 0.0264\n",
      "Epoch 94/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9897 - loss: 0.0301\n",
      "Epoch 95/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9932 - loss: 0.0237\n",
      "Epoch 96/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9919 - loss: 0.0275\n",
      "Epoch 97/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9881 - loss: 0.0282\n",
      "Epoch 98/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9914 - loss: 0.0199\n",
      "Epoch 99/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9902 - loss: 0.0251\n",
      "Epoch 100/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9910 - loss: 0.0248\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6917    0.5764    0.6288       144\n",
      "           1     0.7099    0.5990    0.6497       192\n",
      "           2     0.8289    0.9013    0.8636       618\n",
      "\n",
      "    accuracy                         0.7914       954\n",
      "   macro avg     0.7435    0.6922    0.7140       954\n",
      "weighted avg     0.7842    0.7914    0.7851       954\n",
      "\n",
      "\n",
      "Training and evaluating SVC with DISTILROBERTA_V1 embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6316    0.7500    0.6857       144\n",
      "           1     0.6396    0.7396    0.6860       192\n",
      "           2     0.9037    0.8204    0.8601       618\n",
      "\n",
      "    accuracy                         0.7935       954\n",
      "   macro avg     0.7250    0.7700    0.7439       954\n",
      "weighted avg     0.8095    0.7935    0.7987       954\n",
      "\n",
      "\n",
      "Training and evaluating XGB with DISTILROBERTA_V1 embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8267    0.4306    0.5662       144\n",
      "           1     0.7589    0.5573    0.6426       192\n",
      "           2     0.7967    0.9515    0.8673       618\n",
      "\n",
      "    accuracy                         0.7935       954\n",
      "   macro avg     0.7941    0.6464    0.6920       954\n",
      "weighted avg     0.7936    0.7935    0.7766       954\n",
      "\n",
      "\n",
      "Training and evaluating LogisticRegression with DISTILROBERTA_V1 embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5423    0.7569    0.6319       144\n",
      "           1     0.5779    0.7344    0.6468       192\n",
      "           2     0.9018    0.7427    0.8146       618\n",
      "\n",
      "    accuracy                         0.7432       954\n",
      "   macro avg     0.6740    0.7447    0.6977       954\n",
      "weighted avg     0.7823    0.7432    0.7532       954\n",
      "\n",
      "\n",
      "Training and evaluating KNN with DISTILROBERTA_V1 embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5455    0.5417    0.5436       144\n",
      "           1     0.6129    0.5938    0.6032       192\n",
      "           2     0.8512    0.8608    0.8560       618\n",
      "\n",
      "    accuracy                         0.7589       954\n",
      "   macro avg     0.6699    0.6654    0.6676       954\n",
      "weighted avg     0.7571    0.7589    0.7580       954\n",
      "\n",
      "\n",
      "Training and evaluating MLP with DISTILROBERTA_V1 embeddings...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6502 - loss: 0.8447\n",
      "Epoch 2/100\n",
      "\u001b[1m 28/239\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7680 - loss: 0.5966"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\callbacks\\early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7555 - loss: 0.5989\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7843 - loss: 0.5336\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8037 - loss: 0.4951\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8210 - loss: 0.4558\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8475 - loss: 0.4008\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8668 - loss: 0.3555\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8801 - loss: 0.3277\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9006 - loss: 0.2792\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9076 - loss: 0.2579\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9243 - loss: 0.2175\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9394 - loss: 0.1844\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9339 - loss: 0.1798\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9446 - loss: 0.1540\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9472 - loss: 0.1520\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9591 - loss: 0.1150\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9648 - loss: 0.1040\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9650 - loss: 0.1047\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9665 - loss: 0.0975\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9677 - loss: 0.0840\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9706 - loss: 0.0907\n",
      "Epoch 22/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9773 - loss: 0.0721\n",
      "Epoch 23/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9773 - loss: 0.0712\n",
      "Epoch 24/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9782 - loss: 0.0693\n",
      "Epoch 25/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9768 - loss: 0.0680\n",
      "Epoch 26/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9768 - loss: 0.0695\n",
      "Epoch 27/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9749 - loss: 0.0738\n",
      "Epoch 28/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9798 - loss: 0.0562\n",
      "Epoch 29/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9825 - loss: 0.0508\n",
      "Epoch 30/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9826 - loss: 0.0537\n",
      "Epoch 31/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9800 - loss: 0.0539\n",
      "Epoch 32/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9806 - loss: 0.0599\n",
      "Epoch 33/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9858 - loss: 0.0434\n",
      "Epoch 34/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9809 - loss: 0.0555\n",
      "Epoch 35/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9849 - loss: 0.0417\n",
      "Epoch 36/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9833 - loss: 0.0435\n",
      "Epoch 37/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9832 - loss: 0.0509\n",
      "Epoch 38/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9815 - loss: 0.0520\n",
      "Epoch 39/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9825 - loss: 0.0555\n",
      "Epoch 40/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9842 - loss: 0.0425\n",
      "Epoch 41/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9851 - loss: 0.0411\n",
      "Epoch 42/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9855 - loss: 0.0402\n",
      "Epoch 43/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9854 - loss: 0.0401\n",
      "Epoch 44/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9857 - loss: 0.0408\n",
      "Epoch 45/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9856 - loss: 0.0407\n",
      "Epoch 46/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9900 - loss: 0.0349\n",
      "Epoch 47/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9857 - loss: 0.0383\n",
      "Epoch 48/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9854 - loss: 0.0397\n",
      "Epoch 49/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9927 - loss: 0.0264\n",
      "Epoch 50/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9842 - loss: 0.0422\n",
      "Epoch 51/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9885 - loss: 0.0345\n",
      "Epoch 52/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9890 - loss: 0.0295\n",
      "Epoch 53/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9879 - loss: 0.0361\n",
      "Epoch 54/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9892 - loss: 0.0307\n",
      "Epoch 55/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9876 - loss: 0.0344\n",
      "Epoch 56/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9885 - loss: 0.0346\n",
      "Epoch 57/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9884 - loss: 0.0354\n",
      "Epoch 58/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9864 - loss: 0.0339\n",
      "Epoch 59/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9870 - loss: 0.0314\n",
      "Epoch 60/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9890 - loss: 0.0292\n",
      "Epoch 61/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9837 - loss: 0.0388\n",
      "Epoch 62/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9910 - loss: 0.0279\n",
      "Epoch 63/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9872 - loss: 0.0420\n",
      "Epoch 64/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9873 - loss: 0.0375\n",
      "Epoch 65/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9922 - loss: 0.0257\n",
      "Epoch 66/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9886 - loss: 0.0319\n",
      "Epoch 67/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9872 - loss: 0.0369\n",
      "Epoch 68/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9935 - loss: 0.0210\n",
      "Epoch 69/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9898 - loss: 0.0288\n",
      "Epoch 70/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9899 - loss: 0.0289\n",
      "Epoch 71/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9927 - loss: 0.0219\n",
      "Epoch 72/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9928 - loss: 0.0227\n",
      "Epoch 73/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9899 - loss: 0.0309\n",
      "Epoch 74/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9868 - loss: 0.0375\n",
      "Epoch 75/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9932 - loss: 0.0233\n",
      "Epoch 76/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9922 - loss: 0.0216\n",
      "Epoch 77/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9908 - loss: 0.0282\n",
      "Epoch 78/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9905 - loss: 0.0287\n",
      "Epoch 79/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9876 - loss: 0.0305\n",
      "Epoch 80/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9902 - loss: 0.0277\n",
      "Epoch 81/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9897 - loss: 0.0284\n",
      "Epoch 82/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9906 - loss: 0.0255\n",
      "Epoch 83/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9899 - loss: 0.0284\n",
      "Epoch 84/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9933 - loss: 0.0213\n",
      "Epoch 85/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9902 - loss: 0.0293\n",
      "Epoch 86/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9914 - loss: 0.0240\n",
      "Epoch 87/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9911 - loss: 0.0254\n",
      "Epoch 88/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9922 - loss: 0.0230\n",
      "Epoch 89/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9923 - loss: 0.0198\n",
      "Epoch 90/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9901 - loss: 0.0255\n",
      "Epoch 91/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9889 - loss: 0.0255\n",
      "Epoch 92/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9914 - loss: 0.0233\n",
      "Epoch 93/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9884 - loss: 0.0280\n",
      "Epoch 94/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9911 - loss: 0.0250\n",
      "Epoch 95/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9911 - loss: 0.0221\n",
      "Epoch 96/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9904 - loss: 0.0270\n",
      "Epoch 97/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9903 - loss: 0.0292\n",
      "Epoch 98/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9901 - loss: 0.0281\n",
      "Epoch 99/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9895 - loss: 0.0283\n",
      "Epoch 100/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9920 - loss: 0.0215\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7157    0.5069    0.5935       144\n",
      "           1     0.7273    0.6250    0.6723       192\n",
      "           2     0.8326    0.9256    0.8766       618\n",
      "\n",
      "    accuracy                         0.8019       954\n",
      "   macro avg     0.7585    0.6858    0.7141       954\n",
      "weighted avg     0.7938    0.8019    0.7928       954\n",
      "\n",
      "\n",
      "Training and evaluating SVC with MINILM_L12_V2 embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6646    0.7569    0.7078       144\n",
      "           1     0.6147    0.6979    0.6537       192\n",
      "           2     0.8986    0.8317    0.8639       618\n",
      "\n",
      "    accuracy                         0.7935       954\n",
      "   macro avg     0.7260    0.7622    0.7418       954\n",
      "weighted avg     0.8061    0.7935    0.7980       954\n",
      "\n",
      "\n",
      "Training and evaluating XGB with MINILM_L12_V2 embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7222    0.3611    0.4815       144\n",
      "           1     0.6667    0.4792    0.5576       192\n",
      "           2     0.7809    0.9401    0.8532       618\n",
      "\n",
      "    accuracy                         0.7600       954\n",
      "   macro avg     0.7233    0.5935    0.6307       954\n",
      "weighted avg     0.7491    0.7600    0.7376       954\n",
      "\n",
      "\n",
      "Training and evaluating LogisticRegression with MINILM_L12_V2 embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5419    0.7639    0.6340       144\n",
      "           1     0.5075    0.7083    0.5913       192\n",
      "           2     0.9068    0.7087    0.7956       618\n",
      "\n",
      "    accuracy                         0.7170       954\n",
      "   macro avg     0.6521    0.7270    0.6737       954\n",
      "weighted avg     0.7714    0.7170    0.7301       954\n",
      "\n",
      "\n",
      "Training and evaluating KNN with MINILM_L12_V2 embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6066    0.5139    0.5564       144\n",
      "           1     0.5485    0.5885    0.5678       192\n",
      "           2     0.8578    0.8689    0.8633       618\n",
      "\n",
      "    accuracy                         0.7589       954\n",
      "   macro avg     0.6710    0.6571    0.6625       954\n",
      "weighted avg     0.7577    0.7589    0.7575       954\n",
      "\n",
      "\n",
      "Training and evaluating MLP with MINILM_L12_V2 embeddings...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6289 - loss: 0.8825\n",
      "Epoch 2/100\n",
      "\u001b[1m 50/239\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7624 - loss: 0.5860"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\callbacks\\early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7487 - loss: 0.6170\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7767 - loss: 0.5644\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7958 - loss: 0.5235\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8188 - loss: 0.4784\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8343 - loss: 0.4377\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8470 - loss: 0.4144\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8641 - loss: 0.3786\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8805 - loss: 0.3399\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8934 - loss: 0.2940\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9061 - loss: 0.2646\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9122 - loss: 0.2505\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9333 - loss: 0.2049\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9326 - loss: 0.1896\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9508 - loss: 0.1516\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9489 - loss: 0.1559\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9511 - loss: 0.1458\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9547 - loss: 0.1333\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9615 - loss: 0.1230\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9671 - loss: 0.1023\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9583 - loss: 0.1153\n",
      "Epoch 22/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9671 - loss: 0.0978\n",
      "Epoch 23/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9708 - loss: 0.0920\n",
      "Epoch 24/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9676 - loss: 0.0953\n",
      "Epoch 25/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9739 - loss: 0.0760\n",
      "Epoch 26/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9808 - loss: 0.0649\n",
      "Epoch 27/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9758 - loss: 0.0724\n",
      "Epoch 28/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9757 - loss: 0.0737\n",
      "Epoch 29/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9763 - loss: 0.0690\n",
      "Epoch 30/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9750 - loss: 0.0750\n",
      "Epoch 31/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9798 - loss: 0.0654\n",
      "Epoch 32/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9822 - loss: 0.0587\n",
      "Epoch 33/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9800 - loss: 0.0593\n",
      "Epoch 34/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9811 - loss: 0.0559\n",
      "Epoch 35/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9819 - loss: 0.0560\n",
      "Epoch 36/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9805 - loss: 0.0587\n",
      "Epoch 37/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9778 - loss: 0.0661\n",
      "Epoch 38/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9772 - loss: 0.0608\n",
      "Epoch 39/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9814 - loss: 0.0533\n",
      "Epoch 40/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9820 - loss: 0.0545\n",
      "Epoch 41/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9823 - loss: 0.0466\n",
      "Epoch 42/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9857 - loss: 0.0508\n",
      "Epoch 43/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9854 - loss: 0.0415\n",
      "Epoch 44/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9837 - loss: 0.0458\n",
      "Epoch 45/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9850 - loss: 0.0472\n",
      "Epoch 46/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9859 - loss: 0.0492\n",
      "Epoch 47/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9843 - loss: 0.0460\n",
      "Epoch 48/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9832 - loss: 0.0507\n",
      "Epoch 49/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9886 - loss: 0.0354\n",
      "Epoch 50/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9848 - loss: 0.0421\n",
      "Epoch 51/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9844 - loss: 0.0429\n",
      "Epoch 52/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9871 - loss: 0.0389\n",
      "Epoch 53/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9876 - loss: 0.0381\n",
      "Epoch 54/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9866 - loss: 0.0379\n",
      "Epoch 55/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9875 - loss: 0.0398\n",
      "Epoch 56/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9830 - loss: 0.0467\n",
      "Epoch 57/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9863 - loss: 0.0385\n",
      "Epoch 58/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9893 - loss: 0.0362\n",
      "Epoch 59/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9856 - loss: 0.0430\n",
      "Epoch 60/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9859 - loss: 0.0412\n",
      "Epoch 61/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9854 - loss: 0.0358\n",
      "Epoch 62/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9876 - loss: 0.0336\n",
      "Epoch 63/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9894 - loss: 0.0305\n",
      "Epoch 64/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9892 - loss: 0.0336\n",
      "Epoch 65/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9846 - loss: 0.0419\n",
      "Epoch 66/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9867 - loss: 0.0382\n",
      "Epoch 67/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9878 - loss: 0.0310\n",
      "Epoch 68/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9910 - loss: 0.0298\n",
      "Epoch 69/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9881 - loss: 0.0316\n",
      "Epoch 70/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9885 - loss: 0.0291\n",
      "Epoch 71/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9876 - loss: 0.0349\n",
      "Epoch 72/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9870 - loss: 0.0332\n",
      "Epoch 73/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9868 - loss: 0.0342\n",
      "Epoch 74/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9909 - loss: 0.0261\n",
      "Epoch 75/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9872 - loss: 0.0337\n",
      "Epoch 76/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9900 - loss: 0.0274\n",
      "Epoch 77/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9901 - loss: 0.0286\n",
      "Epoch 78/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9872 - loss: 0.0360\n",
      "Epoch 79/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9897 - loss: 0.0270\n",
      "Epoch 80/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9903 - loss: 0.0284\n",
      "Epoch 81/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9915 - loss: 0.0275\n",
      "Epoch 82/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9892 - loss: 0.0303\n",
      "Epoch 83/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9874 - loss: 0.0401\n",
      "Epoch 84/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9914 - loss: 0.0295\n",
      "Epoch 85/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9935 - loss: 0.0224\n",
      "Epoch 86/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9909 - loss: 0.0254\n",
      "Epoch 87/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9912 - loss: 0.0223\n",
      "Epoch 88/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9902 - loss: 0.0286\n",
      "Epoch 89/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9889 - loss: 0.0315\n",
      "Epoch 90/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9909 - loss: 0.0278\n",
      "Epoch 91/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9885 - loss: 0.0322\n",
      "Epoch 92/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9913 - loss: 0.0264\n",
      "Epoch 93/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9917 - loss: 0.0249\n",
      "Epoch 94/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9896 - loss: 0.0311\n",
      "Epoch 95/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9906 - loss: 0.0280\n",
      "Epoch 96/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9905 - loss: 0.0254\n",
      "Epoch 97/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9916 - loss: 0.0260\n",
      "Epoch 98/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9900 - loss: 0.0261\n",
      "Epoch 99/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9890 - loss: 0.0295\n",
      "Epoch 100/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9904 - loss: 0.0326\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7706    0.5833    0.6640       144\n",
      "           1     0.7202    0.6302    0.6722       192\n",
      "           2     0.8419    0.9223    0.8803       618\n",
      "\n",
      "    accuracy                         0.8124       954\n",
      "   macro avg     0.7776    0.7120    0.7389       954\n",
      "weighted avg     0.8067    0.8124    0.8058       954\n",
      "\n",
      "\n",
      "Training and evaluating SVC with ALL_MINILM_L6_V2 embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6129    0.6597    0.6355       144\n",
      "           1     0.6498    0.7344    0.6895       192\n",
      "           2     0.8883    0.8366    0.8617       618\n",
      "\n",
      "    accuracy                         0.7893       954\n",
      "   macro avg     0.7170    0.7436    0.7289       954\n",
      "weighted avg     0.7987    0.7893    0.7929       954\n",
      "\n",
      "\n",
      "Training and evaluating XGB with ALL_MINILM_L6_V2 embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7719    0.3056    0.4378       144\n",
      "           1     0.7357    0.5365    0.6205       192\n",
      "           2     0.7781    0.9531    0.8567       618\n",
      "\n",
      "    accuracy                         0.7715       954\n",
      "   macro avg     0.7619    0.5984    0.6383       954\n",
      "weighted avg     0.7686    0.7715    0.7459       954\n",
      "\n",
      "\n",
      "Training and evaluating LogisticRegression with ALL_MINILM_L6_V2 embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5198    0.7292    0.6069       144\n",
      "           1     0.5625    0.7031    0.6250       192\n",
      "           2     0.8926    0.7395    0.8088       618\n",
      "\n",
      "    accuracy                         0.7306       954\n",
      "   macro avg     0.6583    0.7239    0.6803       954\n",
      "weighted avg     0.7699    0.7306    0.7414       954\n",
      "\n",
      "\n",
      "Training and evaluating KNN with ALL_MINILM_L6_V2 embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5526    0.4375    0.4884       144\n",
      "           1     0.5403    0.5938    0.5658       192\n",
      "           2     0.8347    0.8495    0.8420       618\n",
      "\n",
      "    accuracy                         0.7358       954\n",
      "   macro avg     0.6425    0.6269    0.6320       954\n",
      "weighted avg     0.7328    0.7358    0.7330       954\n",
      "\n",
      "\n",
      "Training and evaluating MLP with ALL_MINILM_L6_V2 embeddings...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.6581 - loss: 0.8573\n",
      "Epoch 2/100\n",
      "\u001b[1m 39/239\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7531 - loss: 0.6397"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\callbacks\\early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7453 - loss: 0.6356\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7746 - loss: 0.5678\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8000 - loss: 0.5271\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8141 - loss: 0.4830\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8261 - loss: 0.4501\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8395 - loss: 0.4133\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8645 - loss: 0.3582\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8812 - loss: 0.3339\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8884 - loss: 0.3035\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9117 - loss: 0.2588\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9118 - loss: 0.2508\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9208 - loss: 0.2147\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9241 - loss: 0.2067\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9400 - loss: 0.1717\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9487 - loss: 0.1501\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9509 - loss: 0.1415\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9539 - loss: 0.1301\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9540 - loss: 0.1261\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9629 - loss: 0.1120\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9654 - loss: 0.1043\n",
      "Epoch 22/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9673 - loss: 0.0961\n",
      "Epoch 23/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9657 - loss: 0.0989\n",
      "Epoch 24/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9715 - loss: 0.0849\n",
      "Epoch 25/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9652 - loss: 0.0876\n",
      "Epoch 26/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9739 - loss: 0.0739\n",
      "Epoch 27/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9751 - loss: 0.0759\n",
      "Epoch 28/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9746 - loss: 0.0721\n",
      "Epoch 29/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9776 - loss: 0.0662\n",
      "Epoch 30/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9768 - loss: 0.0667\n",
      "Epoch 31/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9726 - loss: 0.0732\n",
      "Epoch 32/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9838 - loss: 0.0580\n",
      "Epoch 33/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9754 - loss: 0.0677\n",
      "Epoch 34/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9746 - loss: 0.0689\n",
      "Epoch 35/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9819 - loss: 0.0601\n",
      "Epoch 36/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9800 - loss: 0.0574\n",
      "Epoch 37/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9810 - loss: 0.0501\n",
      "Epoch 38/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9800 - loss: 0.0561\n",
      "Epoch 39/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9791 - loss: 0.0605\n",
      "Epoch 40/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9817 - loss: 0.0558\n",
      "Epoch 41/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9813 - loss: 0.0540\n",
      "Epoch 42/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9867 - loss: 0.0432\n",
      "Epoch 43/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9811 - loss: 0.0568\n",
      "Epoch 44/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9839 - loss: 0.0451\n",
      "Epoch 45/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9817 - loss: 0.0536\n",
      "Epoch 46/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9858 - loss: 0.0449\n",
      "Epoch 47/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9866 - loss: 0.0438\n",
      "Epoch 48/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9863 - loss: 0.0415\n",
      "Epoch 49/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9800 - loss: 0.0486\n",
      "Epoch 50/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9842 - loss: 0.0469\n",
      "Epoch 51/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9870 - loss: 0.0437\n",
      "Epoch 52/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9840 - loss: 0.0435\n",
      "Epoch 53/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9825 - loss: 0.0463\n",
      "Epoch 54/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9832 - loss: 0.0505\n",
      "Epoch 55/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9810 - loss: 0.0532\n",
      "Epoch 56/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9834 - loss: 0.0504\n",
      "Epoch 57/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9859 - loss: 0.0428\n",
      "Epoch 58/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9853 - loss: 0.0427\n",
      "Epoch 59/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9864 - loss: 0.0434\n",
      "Epoch 60/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9836 - loss: 0.0399\n",
      "Epoch 61/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9835 - loss: 0.0477\n",
      "Epoch 62/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9838 - loss: 0.0485\n",
      "Epoch 63/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9903 - loss: 0.0335\n",
      "Epoch 64/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9860 - loss: 0.0391\n",
      "Epoch 65/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9890 - loss: 0.0344\n",
      "Epoch 66/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9887 - loss: 0.0349\n",
      "Epoch 67/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9895 - loss: 0.0282\n",
      "Epoch 68/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9840 - loss: 0.0450\n",
      "Epoch 69/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9820 - loss: 0.0496\n",
      "Epoch 70/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9864 - loss: 0.0434\n",
      "Epoch 71/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9850 - loss: 0.0395\n",
      "Epoch 72/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9867 - loss: 0.0415\n",
      "Epoch 73/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9883 - loss: 0.0361\n",
      "Epoch 74/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9889 - loss: 0.0319\n",
      "Epoch 75/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9840 - loss: 0.0416\n",
      "Epoch 76/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9878 - loss: 0.0371\n",
      "Epoch 77/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9900 - loss: 0.0297\n",
      "Epoch 78/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9873 - loss: 0.0403\n",
      "Epoch 79/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9884 - loss: 0.0336\n",
      "Epoch 80/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9875 - loss: 0.0345\n",
      "Epoch 81/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9861 - loss: 0.0395\n",
      "Epoch 82/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9917 - loss: 0.0288\n",
      "Epoch 83/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9894 - loss: 0.0303\n",
      "Epoch 84/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9905 - loss: 0.0260\n",
      "Epoch 85/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9892 - loss: 0.0343\n",
      "Epoch 86/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9894 - loss: 0.0335\n",
      "Epoch 87/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9888 - loss: 0.0349\n",
      "Epoch 88/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9890 - loss: 0.0295\n",
      "Epoch 89/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9920 - loss: 0.0235\n",
      "Epoch 90/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9879 - loss: 0.0327\n",
      "Epoch 91/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9870 - loss: 0.0331\n",
      "Epoch 92/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9870 - loss: 0.0384\n",
      "Epoch 93/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9915 - loss: 0.0302\n",
      "Epoch 94/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9913 - loss: 0.0260\n",
      "Epoch 95/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9888 - loss: 0.0308\n",
      "Epoch 96/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9890 - loss: 0.0277\n",
      "Epoch 97/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9880 - loss: 0.0356\n",
      "Epoch 98/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9891 - loss: 0.0274\n",
      "Epoch 99/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9859 - loss: 0.0366\n",
      "Epoch 100/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9915 - loss: 0.0271\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6638    0.5347    0.5923       144\n",
      "           1     0.6836    0.6302    0.6558       192\n",
      "           2     0.8472    0.9061    0.8757       618\n",
      "\n",
      "    accuracy                         0.7945       954\n",
      "   macro avg     0.7315    0.6904    0.7079       954\n",
      "weighted avg     0.7866    0.7945    0.7887       954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_sentence_encoders = {} \n",
    "\n",
    "for name, (X_train, X_val) in sentence_encoders_variants.items():\n",
    "    y_train_cls = y_train.argmax(axis=1) if y_train.ndim > 1 else y_train\n",
    "    y_val_cls   = y_val.argmax(axis=1) if y_val.ndim > 1 else y_val\n",
    "\n",
    "    for model_name, model in sentence_encoder_models.items():\n",
    "        print(f\"\\nTraining and evaluating {model_name} with {name.upper()} embeddings...\")\n",
    "\n",
    "        if model_name == \"MLP\":\n",
    "            model = KerasClassifier(\n",
    "                model=create_mlp_model, \n",
    "                embedding_dim=X_train.shape[1], \n",
    "                num_classes=num_classes,\n",
    "                epochs=100, batch_size=32, verbose=1, callbacks=callbacks\n",
    "            )\n",
    "        else:\n",
    "            model = clone(model)\n",
    "\n",
    "        model.fit(X_train, y_train_cls)\n",
    "        y_pred = model.predict(X_val)\n",
    "        print(classification_report(y_val_cls, y_pred, digits=4))\n",
    "        results_sentence_encoders.setdefault(name, {})[model_name] = classification_report(\n",
    "            y_val_cls, y_pred, digits=4, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9d1e63",
   "metadata": {},
   "source": [
    "### 1.4 - Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96813522",
   "metadata": {},
   "source": [
    "#### 1) Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "42273016",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bert_large_uncased = npy_embeddings['bert-large-uncased_text_no_lemma_stem_with_stopwords_train_meanpooled']\n",
    "X_val_bert_large_uncased = npy_embeddings['bert-large-uncased_text_no_lemma_stem_with_stopwords_val_meanpooled']\n",
    "X_test_bert_large_uncased = npy_embeddings['bert-large-uncased_text_no_lemma_stem_with_stopwords_test_meanpooled']\n",
    "\n",
    "X_train_roberta_large_uncased = npy_embeddings['roberta-large_text_no_lemma_stem_with_stopwords_train_meanpooled']\n",
    "X_val_roberta_large_uncased = npy_embeddings['roberta-large_text_no_lemma_stem_with_stopwords_val_meanpooled']\n",
    "X_test_roberta_large_uncased = npy_embeddings['roberta-large_text_no_lemma_stem_with_stopwords_test_meanpooled']\n",
    "\n",
    "X_train_fb_bart_large = npy_embeddings['facebook-bart-large_text_no_lemma_stem_with_stopwords_train_meanpooled']\n",
    "X_val_fb_bart_large = npy_embeddings['facebook-bart-large_text_no_lemma_stem_with_stopwords_val_meanpooled']\n",
    "X_test_fb_bart_large = npy_embeddings['facebook-bart-large_text_no_lemma_stem_with_stopwords_test_meanpooled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3a732200",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_fe_variants = {\n",
    "    \"bert_large_uncased\":   (X_train_bert_large_uncased, X_val_bert_large_uncased),\n",
    "    \"roberta_large_uncased\":      (X_train_roberta_large_uncased,    X_val_roberta_large_uncased),\n",
    "    \"fb_bart_large\":     (X_train_fb_bart_large,   X_val_fb_bart_large),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48cee2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating SVC with BERT_LARGE_UNCASED embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4305    0.6667    0.5232       144\n",
      "           1     0.5316    0.6562    0.5874       192\n",
      "           2     0.9008    0.7201    0.8004       618\n",
      "\n",
      "    accuracy                         0.6992       954\n",
      "   macro avg     0.6210    0.6810    0.6370       954\n",
      "weighted avg     0.7555    0.6992    0.7157       954\n",
      "\n",
      "\n",
      "Training and evaluating XGB with BERT_LARGE_UNCASED embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7164    0.3333    0.4550       144\n",
      "           1     0.6887    0.3802    0.4899       192\n",
      "           2     0.7554    0.9547    0.8435       618\n",
      "\n",
      "    accuracy                         0.7453       954\n",
      "   macro avg     0.7202    0.5561    0.5961       954\n",
      "weighted avg     0.7361    0.7453    0.7137       954\n",
      "\n",
      "\n",
      "Training and evaluating LogisticRegression with BERT_LARGE_UNCASED embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4245    0.6250    0.5056       144\n",
      "           1     0.5491    0.6406    0.5913       192\n",
      "           2     0.8861    0.7427    0.8081       618\n",
      "\n",
      "    accuracy                         0.7044       954\n",
      "   macro avg     0.6199    0.6694    0.6350       954\n",
      "weighted avg     0.7486    0.7044    0.7188       954\n",
      "\n",
      "\n",
      "Training and evaluating KNN with BERT_LARGE_UNCASED embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4488    0.3958    0.4207       144\n",
      "           1     0.4649    0.4479    0.4562       192\n",
      "           2     0.8084    0.8398    0.8238       618\n",
      "\n",
      "    accuracy                         0.6939       954\n",
      "   macro avg     0.5740    0.5612    0.5669       954\n",
      "weighted avg     0.6850    0.6939    0.6890       954\n",
      "\n",
      "\n",
      "Training and evaluating MLP with BERT_LARGE_UNCASED embeddings...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6431 - loss: 0.8684\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6953 - loss: 0.7319\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7085 - loss: 0.6865\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7376 - loss: 0.6373\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7410 - loss: 0.6265\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7485 - loss: 0.6062\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7664 - loss: 0.5753\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7679 - loss: 0.5708\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7869 - loss: 0.5381\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7749 - loss: 0.5347\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7921 - loss: 0.5207\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8008 - loss: 0.4852\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8039 - loss: 0.4947\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8159 - loss: 0.4519\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8170 - loss: 0.4508\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8334 - loss: 0.4263\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8347 - loss: 0.4224\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8330 - loss: 0.3977\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8382 - loss: 0.3997\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8447 - loss: 0.3888\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8496 - loss: 0.3714\n",
      "Epoch 22/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8525 - loss: 0.3701\n",
      "Epoch 23/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8641 - loss: 0.3341\n",
      "Epoch 24/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8590 - loss: 0.3405\n",
      "Epoch 25/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8726 - loss: 0.3239\n",
      "Epoch 26/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8567 - loss: 0.3367\n",
      "Epoch 27/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8728 - loss: 0.3105\n",
      "Epoch 28/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8759 - loss: 0.2955\n",
      "Epoch 29/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8936 - loss: 0.2708\n",
      "Epoch 30/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8887 - loss: 0.2798\n",
      "Epoch 31/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8873 - loss: 0.2717\n",
      "Epoch 32/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8907 - loss: 0.2742\n",
      "Epoch 33/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8869 - loss: 0.2746\n",
      "Epoch 34/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8975 - loss: 0.2544\n",
      "Epoch 35/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8970 - loss: 0.2488\n",
      "Epoch 36/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8947 - loss: 0.2666\n",
      "Epoch 37/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9084 - loss: 0.2312\n",
      "Epoch 38/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9108 - loss: 0.2193\n",
      "Epoch 39/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9135 - loss: 0.2302\n",
      "Epoch 40/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9038 - loss: 0.2453\n",
      "Epoch 41/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9058 - loss: 0.2314\n",
      "Epoch 42/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9128 - loss: 0.2184\n",
      "Epoch 43/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9116 - loss: 0.2215\n",
      "Epoch 44/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9192 - loss: 0.2072\n",
      "Epoch 45/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9121 - loss: 0.2202\n",
      "Epoch 46/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9089 - loss: 0.2198\n",
      "Epoch 47/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9169 - loss: 0.2072\n",
      "Epoch 48/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9239 - loss: 0.1873\n",
      "Epoch 49/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9142 - loss: 0.2107\n",
      "Epoch 50/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8998 - loss: 0.2287\n",
      "Epoch 51/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9243 - loss: 0.1895\n",
      "Epoch 52/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9280 - loss: 0.1827\n",
      "Epoch 53/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9210 - loss: 0.1954\n",
      "Epoch 54/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9318 - loss: 0.1762\n",
      "Epoch 55/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9183 - loss: 0.1927\n",
      "Epoch 56/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9285 - loss: 0.1813\n",
      "Epoch 57/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9230 - loss: 0.1834\n",
      "Epoch 58/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9186 - loss: 0.1920\n",
      "Epoch 59/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9357 - loss: 0.1675\n",
      "Epoch 60/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9258 - loss: 0.1743\n",
      "Epoch 61/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9251 - loss: 0.1841\n",
      "Epoch 62/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9344 - loss: 0.1634\n",
      "Epoch 63/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9276 - loss: 0.1758\n",
      "Epoch 64/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9194 - loss: 0.1948\n",
      "Epoch 65/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9296 - loss: 0.1678\n",
      "Epoch 66/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9375 - loss: 0.1466\n",
      "Epoch 67/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9431 - loss: 0.1487\n",
      "Epoch 68/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9236 - loss: 0.1824\n",
      "Epoch 69/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9372 - loss: 0.1642\n",
      "Epoch 70/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9336 - loss: 0.1661\n",
      "Epoch 71/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9396 - loss: 0.1510\n",
      "Epoch 72/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9433 - loss: 0.1369\n",
      "Epoch 73/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9341 - loss: 0.1632\n",
      "Epoch 74/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9332 - loss: 0.1677\n",
      "Epoch 75/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9426 - loss: 0.1496\n",
      "Epoch 76/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9418 - loss: 0.1471\n",
      "Epoch 77/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9327 - loss: 0.1614\n",
      "Epoch 78/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9409 - loss: 0.1438\n",
      "Epoch 79/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9274 - loss: 0.1546\n",
      "Epoch 80/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9397 - loss: 0.1471\n",
      "Epoch 81/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9355 - loss: 0.1590\n",
      "Epoch 82/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9508 - loss: 0.1276\n",
      "Epoch 83/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9454 - loss: 0.1337\n",
      "Epoch 84/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9422 - loss: 0.1444\n",
      "Epoch 85/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9502 - loss: 0.1297\n",
      "Epoch 86/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9425 - loss: 0.1380\n",
      "Epoch 87/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9437 - loss: 0.1442\n",
      "Epoch 88/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9407 - loss: 0.1458\n",
      "Epoch 89/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9399 - loss: 0.1471\n",
      "Epoch 90/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9515 - loss: 0.1189\n",
      "Epoch 91/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9446 - loss: 0.1323\n",
      "Epoch 92/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9409 - loss: 0.1478\n",
      "Epoch 93/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9416 - loss: 0.1353\n",
      "Epoch 94/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9512 - loss: 0.1240\n",
      "Epoch 95/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9452 - loss: 0.1305\n",
      "Epoch 96/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9476 - loss: 0.1220\n",
      "Epoch 97/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9462 - loss: 0.1379\n",
      "Epoch 98/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9546 - loss: 0.1134\n",
      "Epoch 99/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9543 - loss: 0.1146\n",
      "Epoch 100/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9442 - loss: 0.1314\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6018    0.4722    0.5292       144\n",
      "           1     0.5699    0.5729    0.5714       192\n",
      "           2     0.8333    0.8738    0.8531       618\n",
      "\n",
      "    accuracy                         0.7526       954\n",
      "   macro avg     0.6684    0.6396    0.6512       954\n",
      "weighted avg     0.7454    0.7526    0.7475       954\n",
      "\n",
      "\n",
      "Training and evaluating SVC with ROBERTA_LARGE_UNCASED embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5547    0.4931    0.5221       144\n",
      "           1     0.5665    0.5104    0.5370       192\n",
      "           2     0.8285    0.8754    0.8513       618\n",
      "\n",
      "    accuracy                         0.7442       954\n",
      "   macro avg     0.6499    0.6263    0.6368       954\n",
      "weighted avg     0.7344    0.7442    0.7383       954\n",
      "\n",
      "\n",
      "Training and evaluating XGB with ROBERTA_LARGE_UNCASED embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8194    0.4097    0.5463       144\n",
      "           1     0.7254    0.5365    0.6168       192\n",
      "           2     0.7986    0.9563    0.8704       618\n",
      "\n",
      "    accuracy                         0.7893       954\n",
      "   macro avg     0.7811    0.6342    0.6778       954\n",
      "weighted avg     0.7870    0.7893    0.7704       954\n",
      "\n",
      "\n",
      "Training and evaluating LogisticRegression with ROBERTA_LARGE_UNCASED embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5590    0.7569    0.6431       144\n",
      "           1     0.6360    0.7552    0.6905       192\n",
      "           2     0.9115    0.7832    0.8425       618\n",
      "\n",
      "    accuracy                         0.7736       954\n",
      "   macro avg     0.7021    0.7651    0.7253       954\n",
      "weighted avg     0.8028    0.7736    0.7818       954\n",
      "\n",
      "\n",
      "Training and evaluating KNN with ROBERTA_LARGE_UNCASED embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5124    0.4306    0.4679       144\n",
      "           1     0.5663    0.5781    0.5722       192\n",
      "           2     0.8462    0.8722    0.8590       618\n",
      "\n",
      "    accuracy                         0.7463       954\n",
      "   macro avg     0.6416    0.6269    0.6330       954\n",
      "weighted avg     0.7395    0.7463    0.7422       954\n",
      "\n",
      "\n",
      "Training and evaluating MLP with ROBERTA_LARGE_UNCASED embeddings...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6049 - loss: 0.9298\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7385 - loss: 0.6629\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7710 - loss: 0.5830\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7915 - loss: 0.5276\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7983 - loss: 0.5079\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8101 - loss: 0.4717\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8215 - loss: 0.4555\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8250 - loss: 0.4439\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8437 - loss: 0.4017\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8378 - loss: 0.4064\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8383 - loss: 0.4038\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8539 - loss: 0.3745\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8518 - loss: 0.3792\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8608 - loss: 0.3483\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8629 - loss: 0.3411\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8783 - loss: 0.3174\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8833 - loss: 0.2969\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8882 - loss: 0.2868\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8937 - loss: 0.2776\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8972 - loss: 0.2628\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9084 - loss: 0.2516\n",
      "Epoch 22/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9140 - loss: 0.2218\n",
      "Epoch 23/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9055 - loss: 0.2350\n",
      "Epoch 24/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9050 - loss: 0.2397\n",
      "Epoch 25/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9139 - loss: 0.2237\n",
      "Epoch 26/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9257 - loss: 0.1889\n",
      "Epoch 27/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9235 - loss: 0.1967\n",
      "Epoch 28/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9326 - loss: 0.1733\n",
      "Epoch 29/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9288 - loss: 0.1979\n",
      "Epoch 30/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9302 - loss: 0.1818\n",
      "Epoch 31/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9346 - loss: 0.1719\n",
      "Epoch 32/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9299 - loss: 0.1689\n",
      "Epoch 33/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9363 - loss: 0.1635\n",
      "Epoch 34/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9423 - loss: 0.1428\n",
      "Epoch 35/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9390 - loss: 0.1554\n",
      "Epoch 36/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9406 - loss: 0.1552\n",
      "Epoch 37/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9509 - loss: 0.1325\n",
      "Epoch 38/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9472 - loss: 0.1304\n",
      "Epoch 39/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9453 - loss: 0.1381\n",
      "Epoch 40/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9457 - loss: 0.1357\n",
      "Epoch 41/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9433 - loss: 0.1441\n",
      "Epoch 42/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9506 - loss: 0.1212\n",
      "Epoch 43/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9490 - loss: 0.1318\n",
      "Epoch 44/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9476 - loss: 0.1267\n",
      "Epoch 45/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9580 - loss: 0.1080\n",
      "Epoch 46/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9631 - loss: 0.1050\n",
      "Epoch 47/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9562 - loss: 0.1103\n",
      "Epoch 48/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9515 - loss: 0.1221\n",
      "Epoch 49/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9536 - loss: 0.1200\n",
      "Epoch 50/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9528 - loss: 0.1217\n",
      "Epoch 51/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9564 - loss: 0.1045\n",
      "Epoch 52/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9560 - loss: 0.1104\n",
      "Epoch 53/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9521 - loss: 0.1205\n",
      "Epoch 54/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9558 - loss: 0.1139\n",
      "Epoch 55/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9612 - loss: 0.1011\n",
      "Epoch 56/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9591 - loss: 0.1029\n",
      "Epoch 57/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9633 - loss: 0.0996\n",
      "Epoch 58/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9598 - loss: 0.1016\n",
      "Epoch 59/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9655 - loss: 0.0965\n",
      "Epoch 60/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9653 - loss: 0.0942\n",
      "Epoch 61/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9557 - loss: 0.1044\n",
      "Epoch 62/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9603 - loss: 0.1025\n",
      "Epoch 63/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9615 - loss: 0.0989\n",
      "Epoch 64/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9589 - loss: 0.1012\n",
      "Epoch 65/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9670 - loss: 0.0865\n",
      "Epoch 66/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9658 - loss: 0.0918\n",
      "Epoch 67/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9717 - loss: 0.0779\n",
      "Epoch 68/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9656 - loss: 0.0862\n",
      "Epoch 69/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9616 - loss: 0.0950\n",
      "Epoch 70/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9640 - loss: 0.0896\n",
      "Epoch 71/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9708 - loss: 0.0860\n",
      "Epoch 72/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9654 - loss: 0.0845\n",
      "Epoch 73/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9701 - loss: 0.0745\n",
      "Epoch 74/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9642 - loss: 0.0957\n",
      "Epoch 75/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9737 - loss: 0.0759\n",
      "Epoch 76/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9603 - loss: 0.0956\n",
      "Epoch 77/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9676 - loss: 0.0850\n",
      "Epoch 78/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9617 - loss: 0.1016\n",
      "Epoch 79/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9639 - loss: 0.0922\n",
      "Epoch 80/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9691 - loss: 0.0803\n",
      "Epoch 81/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9681 - loss: 0.0751\n",
      "Epoch 82/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9647 - loss: 0.0835\n",
      "Epoch 83/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9741 - loss: 0.0693\n",
      "Epoch 84/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9735 - loss: 0.0700\n",
      "Epoch 85/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9683 - loss: 0.0775\n",
      "Epoch 86/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9699 - loss: 0.0743\n",
      "Epoch 87/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9658 - loss: 0.0948\n",
      "Epoch 88/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9654 - loss: 0.0920\n",
      "Epoch 89/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9750 - loss: 0.0681\n",
      "Epoch 90/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9784 - loss: 0.0566\n",
      "Epoch 91/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9665 - loss: 0.0860\n",
      "Epoch 92/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9641 - loss: 0.0959\n",
      "Epoch 93/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9780 - loss: 0.0602\n",
      "Epoch 94/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9693 - loss: 0.0781\n",
      "Epoch 95/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9678 - loss: 0.0866\n",
      "Epoch 96/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9702 - loss: 0.0804\n",
      "Epoch 97/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9741 - loss: 0.0682\n",
      "Epoch 98/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9675 - loss: 0.0844\n",
      "Epoch 99/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9707 - loss: 0.0691\n",
      "Epoch 100/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9761 - loss: 0.0689\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6894    0.6319    0.6594       144\n",
      "           1     0.7643    0.6250    0.6877       192\n",
      "           2     0.8541    0.9191    0.8854       618\n",
      "\n",
      "    accuracy                         0.8166       954\n",
      "   macro avg     0.7693    0.7253    0.7442       954\n",
      "weighted avg     0.8112    0.8166    0.8115       954\n",
      "\n",
      "\n",
      "Training and evaluating SVC with FB_BART_LARGE embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6353    0.7500    0.6879       144\n",
      "           1     0.6744    0.7552    0.7125       192\n",
      "           2     0.9244    0.8511    0.8863       618\n",
      "\n",
      "    accuracy                         0.8166       954\n",
      "   macro avg     0.7447    0.7854    0.7622       954\n",
      "weighted avg     0.8305    0.8166    0.8214       954\n",
      "\n",
      "\n",
      "Training and evaluating XGB with FB_BART_LARGE embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7500    0.3958    0.5182       144\n",
      "           1     0.7192    0.5469    0.6213       192\n",
      "           2     0.8033    0.9515    0.8711       618\n",
      "\n",
      "    accuracy                         0.7862       954\n",
      "   macro avg     0.7575    0.6314    0.6702       954\n",
      "weighted avg     0.7783    0.7862    0.7676       954\n",
      "\n",
      "\n",
      "Training and evaluating LogisticRegression with FB_BART_LARGE embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5372    0.7014    0.6084       144\n",
      "           1     0.5921    0.7031    0.6429       192\n",
      "           2     0.8941    0.7783    0.8322       618\n",
      "\n",
      "    accuracy                         0.7516       954\n",
      "   macro avg     0.6745    0.7276    0.6945       954\n",
      "weighted avg     0.7794    0.7516    0.7603       954\n",
      "\n",
      "\n",
      "Training and evaluating KNN with FB_BART_LARGE embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4656    0.4236    0.4436       144\n",
      "           1     0.5399    0.5990    0.5679       192\n",
      "           2     0.8607    0.8495    0.8550       618\n",
      "\n",
      "    accuracy                         0.7348       954\n",
      "   macro avg     0.6221    0.6240    0.6222       954\n",
      "weighted avg     0.7365    0.7348    0.7352       954\n",
      "\n",
      "\n",
      "Training and evaluating MLP with FB_BART_LARGE embeddings...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.6589 - loss: 0.8327\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7549 - loss: 0.6003\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7982 - loss: 0.5182\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8071 - loss: 0.4926\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8194 - loss: 0.4659\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8239 - loss: 0.4270\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8393 - loss: 0.4053\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8583 - loss: 0.3557\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8632 - loss: 0.3532\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8724 - loss: 0.3195\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.8874 - loss: 0.2948\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8900 - loss: 0.2871\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8964 - loss: 0.2683\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8999 - loss: 0.2529\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8990 - loss: 0.2527\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9087 - loss: 0.2312\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9073 - loss: 0.2411\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9115 - loss: 0.2197\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9200 - loss: 0.1943\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9315 - loss: 0.1855\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9258 - loss: 0.1850\n",
      "Epoch 22/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9276 - loss: 0.1768\n",
      "Epoch 23/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9277 - loss: 0.1746\n",
      "Epoch 24/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9335 - loss: 0.1657\n",
      "Epoch 25/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9272 - loss: 0.1819\n",
      "Epoch 26/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9411 - loss: 0.1550\n",
      "Epoch 27/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9430 - loss: 0.1360\n",
      "Epoch 28/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9496 - loss: 0.1341\n",
      "Epoch 29/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9480 - loss: 0.1426\n",
      "Epoch 30/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9429 - loss: 0.1498\n",
      "Epoch 31/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9485 - loss: 0.1300\n",
      "Epoch 32/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9463 - loss: 0.1295\n",
      "Epoch 33/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9557 - loss: 0.1082\n",
      "Epoch 34/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9537 - loss: 0.1190\n",
      "Epoch 35/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9581 - loss: 0.1037\n",
      "Epoch 36/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9441 - loss: 0.1324\n",
      "Epoch 37/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9601 - loss: 0.1054\n",
      "Epoch 38/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9538 - loss: 0.1180\n",
      "Epoch 39/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9570 - loss: 0.1063\n",
      "Epoch 40/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9614 - loss: 0.1018\n",
      "Epoch 41/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9480 - loss: 0.1208\n",
      "Epoch 42/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9610 - loss: 0.0992\n",
      "Epoch 43/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9590 - loss: 0.0995\n",
      "Epoch 44/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9617 - loss: 0.0959\n",
      "Epoch 45/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9602 - loss: 0.1093\n",
      "Epoch 46/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9599 - loss: 0.1060\n",
      "Epoch 47/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9641 - loss: 0.0926\n",
      "Epoch 48/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9577 - loss: 0.1017\n",
      "Epoch 49/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9672 - loss: 0.0901\n",
      "Epoch 50/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9645 - loss: 0.0959\n",
      "Epoch 51/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9720 - loss: 0.0776\n",
      "Epoch 52/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9616 - loss: 0.0910\n",
      "Epoch 53/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9632 - loss: 0.0971\n",
      "Epoch 54/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9650 - loss: 0.0894\n",
      "Epoch 55/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9667 - loss: 0.0908\n",
      "Epoch 56/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9704 - loss: 0.0832\n",
      "Epoch 57/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9633 - loss: 0.0950\n",
      "Epoch 58/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9695 - loss: 0.0834\n",
      "Epoch 59/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9732 - loss: 0.0717\n",
      "Epoch 60/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9559 - loss: 0.1058\n",
      "Epoch 61/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9677 - loss: 0.0816\n",
      "Epoch 62/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9700 - loss: 0.0826\n",
      "Epoch 63/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9709 - loss: 0.0764\n",
      "Epoch 64/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9704 - loss: 0.0726\n",
      "Epoch 65/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9658 - loss: 0.0856\n",
      "Epoch 66/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9661 - loss: 0.0832\n",
      "Epoch 67/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9698 - loss: 0.0810\n",
      "Epoch 68/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9694 - loss: 0.0819\n",
      "Epoch 69/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9726 - loss: 0.0742\n",
      "Epoch 70/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9690 - loss: 0.0794\n",
      "Epoch 71/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9735 - loss: 0.0742\n",
      "Epoch 72/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9741 - loss: 0.0701\n",
      "Epoch 73/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9775 - loss: 0.0607\n",
      "Epoch 74/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9714 - loss: 0.0767\n",
      "Epoch 75/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9724 - loss: 0.0735\n",
      "Epoch 76/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9689 - loss: 0.0785\n",
      "Epoch 77/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9696 - loss: 0.0711\n",
      "Epoch 78/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9692 - loss: 0.0797\n",
      "Epoch 79/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9704 - loss: 0.0748\n",
      "Epoch 80/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9750 - loss: 0.0652\n",
      "Epoch 81/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9775 - loss: 0.0637\n",
      "Epoch 82/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9733 - loss: 0.0641\n",
      "Epoch 83/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9724 - loss: 0.0674\n",
      "Epoch 84/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9775 - loss: 0.0582\n",
      "Epoch 85/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9783 - loss: 0.0617\n",
      "Epoch 86/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9789 - loss: 0.0605\n",
      "Epoch 87/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9751 - loss: 0.0624\n",
      "Epoch 88/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9775 - loss: 0.0575\n",
      "Epoch 89/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9754 - loss: 0.0663\n",
      "Epoch 90/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9774 - loss: 0.0616\n",
      "Epoch 91/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9755 - loss: 0.0587\n",
      "Epoch 92/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9737 - loss: 0.0738\n",
      "Epoch 93/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9761 - loss: 0.0653\n",
      "Epoch 94/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9806 - loss: 0.0552\n",
      "Epoch 95/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9776 - loss: 0.0612\n",
      "Epoch 96/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9686 - loss: 0.0827\n",
      "Epoch 97/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9747 - loss: 0.0623\n",
      "Epoch 98/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9824 - loss: 0.0513\n",
      "Epoch 99/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9810 - loss: 0.0522\n",
      "Epoch 100/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9768 - loss: 0.0683\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7287    0.6528    0.6886       144\n",
      "           1     0.7263    0.6771    0.7008       192\n",
      "           2     0.8700    0.9094    0.8892       618\n",
      "\n",
      "    accuracy                         0.8239       954\n",
      "   macro avg     0.7750    0.7464    0.7596       954\n",
      "weighted avg     0.8197    0.8239    0.8210       954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_transformers_fe = {} \n",
    "\n",
    "for name, (X_train, X_val) in transformers_fe_variants.items():\n",
    "    y_train_cls = y_train.argmax(axis=1) if y_train.ndim > 1 else y_train\n",
    "    y_val_cls   = y_val.argmax(axis=1) if y_val.ndim > 1 else y_val\n",
    "\n",
    "    for model_name, model in sentence_encoder_models.items():\n",
    "        print(f\"\\nTraining and evaluating {model_name} with {name.upper()} embeddings...\")\n",
    "\n",
    "        if model_name == \"MLP\":\n",
    "            model = KerasClassifier(\n",
    "                model=create_mlp_model, \n",
    "                embedding_dim=X_train.shape[1], \n",
    "                num_classes=num_classes,\n",
    "                epochs=100, batch_size=32, verbose=1\n",
    "            )\n",
    "        else:\n",
    "            model = clone(model)\n",
    "\n",
    "        model.fit(X_train, y_train_cls)\n",
    "        y_pred = model.predict(X_val)\n",
    "        print(classification_report(y_val_cls, y_pred, digits=4))\n",
    "        results_transformers_fe.setdefault(name, {})[model_name] = classification_report(\n",
    "            y_val_cls, y_pred, digits=4, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc44f50",
   "metadata": {},
   "source": [
    "#### 2) Fine Tunning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c3e3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 19513.51 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 20138.94 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_1953/2019632396.py:79: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3820' max='95500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3820/95500 12:26 < 4:58:37, 5.12 it/s, Epoch 4/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.548800</td>\n",
       "      <td>0.454671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.321900</td>\n",
       "      <td>0.542742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.407600</td>\n",
       "      <td>0.785694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.323200</td>\n",
       "      <td>0.760308</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7583    0.6319    0.6894       144\n",
      "           1     0.7448    0.7448    0.7448       192\n",
      "           2     0.8801    0.9142    0.8968       618\n",
      "\n",
      "    accuracy                         0.8375       954\n",
      "   macro avg     0.7944    0.7637    0.7770       954\n",
      "weighted avg     0.8345    0.8375    0.8349       954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Build HuggingFace datasets from NumPy arrays and label Series\n",
    "train_df = Dataset.from_dict({\n",
    "    \"text_no_lemma_stem_with_stopwords\": npy_embeddings[\"train_text_no_lemma_stem_with_stopwords\"],\n",
    "    \"label\": y_train.tolist()\n",
    "})\n",
    "\n",
    "val_df = Dataset.from_dict({\n",
    "    \"text_no_lemma_stem_with_stopwords\": npy_embeddings[\"val_text_no_lemma_stem_with_stopwords\"],\n",
    "    \"label\": y_val.tolist()\n",
    "})\n",
    "\n",
    "test_df = Dataset.from_dict({\n",
    "    \"text_no_lemma_stem_with_stopwords\": npy_embeddings[\"test_text_no_lemma_stem_with_stopwords\"],\n",
    "    \"label\": y_test.tolist()\n",
    "})\n",
    "\n",
    "# Disable tokenizer parallelism \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text_no_lemma_stem_with_stopwords\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "train_dataset = train_df.map(tokenize_function, batched=True)\n",
    "val_dataset = val_df.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "val_dataset = val_dataset.rename_column(\"label\", \"labels\")\n",
    "columns_to_keep = ['input_ids', 'attention_mask', 'labels']\n",
    "train_dataset.set_format(type=\"torch\", columns=columns_to_keep)\n",
    "val_dataset.set_format(type=\"torch\", columns=columns_to_keep)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_bert_base\",\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    disable_tqdm=False,\n",
    "    logging_dir='./logs_bert_base',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "preds = trainer.predict(val_dataset)\n",
    "y_pred = np.argmax(preds.predictions, axis=1)\n",
    "y_true = val_df[\"label\"]  \n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babbc39a",
   "metadata": {},
   "source": [
    "### 1.5 - Domain Specific Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76972b",
   "metadata": {},
   "source": [
    "#### 1) Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ae752b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_finbert = npy_embeddings['finbert_text_no_lemma_stem_with_stopwords_train_meanpooled']\n",
    "X_val_finbert = npy_embeddings['finbert_text_no_lemma_stem_with_stopwords_val_meanpooled']\n",
    "X_test_finbert = npy_embeddings['finbert_text_no_lemma_stem_with_stopwords_test_meanpooled']\n",
    "\n",
    "X_train_bertweet = npy_embeddings['berttweet_text_no_lemma_stem_with_stopwords_train_meanpooled']\n",
    "X_val_bertweet = npy_embeddings['berttweet_text_no_lemma_stem_with_stopwords_val_meanpooled']\n",
    "X_test_bertweet = npy_embeddings['berttweet_text_no_lemma_stem_with_stopwords_test_meanpooled']\n",
    "\n",
    "X_train_fintwitbert = npy_embeddings['fintwitbert_text_no_lemma_stem_with_stopwords_train_meanpooled']\n",
    "X_val_fintwitbert = npy_embeddings['fintwitbert_text_no_lemma_stem_with_stopwords_val_meanpooled']\n",
    "X_test_fintwitbert = npy_embeddings['fintwitbert_text_no_lemma_stem_with_stopwords_test_meanpooled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f364a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_transformers_fe_variants = {\n",
    "    \"finbert\":   (X_train_finbert, X_val_finbert),\n",
    "    \"bert_tweet\":      (X_train_bertweet,    X_val_bertweet),\n",
    "    \"fintwitbert\":     (X_train_fintwitbert,   X_val_fintwitbert),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071f4a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating SVC with FINBERT embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5714    0.7500    0.6486       144\n",
      "           1     0.6186    0.6927    0.6536       192\n",
      "           2     0.8982    0.7994    0.8459       618\n",
      "\n",
      "    accuracy                         0.7704       954\n",
      "   macro avg     0.6961    0.7474    0.7160       954\n",
      "weighted avg     0.7926    0.7704    0.7774       954\n",
      "\n",
      "\n",
      "Training and evaluating XGB with FINBERT embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7732    0.5208    0.6224       144\n",
      "           1     0.6993    0.5573    0.6203       192\n",
      "           2     0.8239    0.9385    0.8775       618\n",
      "\n",
      "    accuracy                         0.7987       954\n",
      "   macro avg     0.7655    0.6722    0.7067       954\n",
      "weighted avg     0.7912    0.7987    0.7872       954\n",
      "\n",
      "\n",
      "Training and evaluating LogisticRegression with FINBERT embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4974    0.6736    0.5723       144\n",
      "           1     0.5909    0.7448    0.6590       192\n",
      "           2     0.9091    0.7605    0.8282       618\n",
      "\n",
      "    accuracy                         0.7442       954\n",
      "   macro avg     0.6658    0.7263    0.6865       954\n",
      "weighted avg     0.7829    0.7442    0.7555       954\n",
      "\n",
      "\n",
      "Training and evaluating KNN with FINBERT embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6613    0.5694    0.6119       144\n",
      "           1     0.5951    0.6354    0.6146       192\n",
      "           2     0.8560    0.8657    0.8608       618\n",
      "\n",
      "    accuracy                         0.7746       954\n",
      "   macro avg     0.7041    0.6902    0.6958       954\n",
      "weighted avg     0.7741    0.7746    0.7737       954\n",
      "\n",
      "\n",
      "Training and evaluating MLP with FINBERT embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6702 - loss: 0.7706\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7423 - loss: 0.6435\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7543 - loss: 0.6242\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7675 - loss: 0.5945\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7730 - loss: 0.5688\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7903 - loss: 0.5496\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8007 - loss: 0.5329\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8072 - loss: 0.5124\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8086 - loss: 0.4957\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8160 - loss: 0.4836\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8241 - loss: 0.4605\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8321 - loss: 0.4374\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8327 - loss: 0.4355\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8353 - loss: 0.4209\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8413 - loss: 0.4145\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8516 - loss: 0.3961\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8583 - loss: 0.3765\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8576 - loss: 0.3723\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8663 - loss: 0.3576\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8776 - loss: 0.3261\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8701 - loss: 0.3393\n",
      "Epoch 22/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8781 - loss: 0.3222\n",
      "Epoch 23/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.8798 - loss: 0.3083\n",
      "Epoch 24/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8818 - loss: 0.3077\n",
      "Epoch 25/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8922 - loss: 0.2937\n",
      "Epoch 26/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8866 - loss: 0.3017\n",
      "Epoch 27/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8863 - loss: 0.2980\n",
      "Epoch 28/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8961 - loss: 0.2737\n",
      "Epoch 29/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8942 - loss: 0.2675\n",
      "Epoch 30/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8950 - loss: 0.2767\n",
      "Epoch 31/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9074 - loss: 0.2500\n",
      "Epoch 32/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9074 - loss: 0.2470\n",
      "Epoch 33/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9082 - loss: 0.2421\n",
      "Epoch 34/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9091 - loss: 0.2325\n",
      "Epoch 35/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9061 - loss: 0.2472\n",
      "Epoch 36/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9081 - loss: 0.2288\n",
      "Epoch 37/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9061 - loss: 0.2409\n",
      "Epoch 38/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9215 - loss: 0.2062\n",
      "Epoch 39/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9244 - loss: 0.2018\n",
      "Epoch 40/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9158 - loss: 0.2193\n",
      "Epoch 41/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9229 - loss: 0.2070\n",
      "Epoch 42/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9204 - loss: 0.2081\n",
      "Epoch 43/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9244 - loss: 0.1971\n",
      "Epoch 44/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9251 - loss: 0.1918\n",
      "Epoch 45/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9192 - loss: 0.2022\n",
      "Epoch 46/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9206 - loss: 0.1975\n",
      "Epoch 47/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9210 - loss: 0.2030\n",
      "Epoch 48/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9264 - loss: 0.1911\n",
      "Epoch 49/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9346 - loss: 0.1729\n",
      "Epoch 50/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9280 - loss: 0.1905\n",
      "Epoch 51/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9316 - loss: 0.1729\n",
      "Epoch 52/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9410 - loss: 0.1558\n",
      "Epoch 53/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9220 - loss: 0.1928\n",
      "Epoch 54/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9317 - loss: 0.1682\n",
      "Epoch 55/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9302 - loss: 0.1639\n",
      "Epoch 56/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.9423 - loss: 0.1531\n",
      "Epoch 57/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9374 - loss: 0.1579\n",
      "Epoch 58/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9390 - loss: 0.1527\n",
      "Epoch 59/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9328 - loss: 0.1753\n",
      "Epoch 60/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9452 - loss: 0.1425\n",
      "Epoch 61/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9395 - loss: 0.1544\n",
      "Epoch 62/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9384 - loss: 0.1533\n",
      "Epoch 63/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9362 - loss: 0.1621\n",
      "Epoch 64/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9344 - loss: 0.1498\n",
      "Epoch 65/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9371 - loss: 0.1594\n",
      "Epoch 66/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9422 - loss: 0.1481\n",
      "Epoch 67/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9463 - loss: 0.1460\n",
      "Epoch 68/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9378 - loss: 0.1584\n",
      "Epoch 69/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9392 - loss: 0.1476\n",
      "Epoch 70/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9395 - loss: 0.1483\n",
      "Epoch 71/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9426 - loss: 0.1414\n",
      "Epoch 72/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9460 - loss: 0.1324\n",
      "Epoch 73/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9447 - loss: 0.1408\n",
      "Epoch 74/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9447 - loss: 0.1357\n",
      "Epoch 75/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9473 - loss: 0.1350\n",
      "Epoch 76/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9468 - loss: 0.1291\n",
      "Epoch 77/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9488 - loss: 0.1243\n",
      "Epoch 78/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9504 - loss: 0.1229\n",
      "Epoch 79/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9543 - loss: 0.1164\n",
      "Epoch 80/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9527 - loss: 0.1180\n",
      "Epoch 81/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9459 - loss: 0.1389\n",
      "Epoch 82/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9476 - loss: 0.1318\n",
      "Epoch 83/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9569 - loss: 0.1081\n",
      "Epoch 84/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9488 - loss: 0.1258\n",
      "Epoch 85/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9431 - loss: 0.1397\n",
      "Epoch 86/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9570 - loss: 0.1086\n",
      "Epoch 87/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9536 - loss: 0.1195\n",
      "Epoch 88/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9532 - loss: 0.1192\n",
      "Epoch 89/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9513 - loss: 0.1347\n",
      "Epoch 90/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9568 - loss: 0.1108\n",
      "Epoch 91/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9524 - loss: 0.1138\n",
      "Epoch 92/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9511 - loss: 0.1254\n",
      "Epoch 93/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9576 - loss: 0.1075\n",
      "Epoch 94/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9545 - loss: 0.1135\n",
      "Epoch 95/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9551 - loss: 0.1094\n",
      "Epoch 96/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9566 - loss: 0.1118\n",
      "Epoch 97/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9572 - loss: 0.1130\n",
      "Epoch 98/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9538 - loss: 0.1238\n",
      "Epoch 99/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9462 - loss: 0.1316\n",
      "Epoch 100/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9540 - loss: 0.1169\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6304    0.6042    0.6170       144\n",
      "           1     0.7315    0.5677    0.6393       192\n",
      "           2     0.8381    0.9045    0.8700       618\n",
      "\n",
      "    accuracy                         0.7914       954\n",
      "   macro avg     0.7334    0.6921    0.7088       954\n",
      "weighted avg     0.7853    0.7914    0.7854       954\n",
      "\n",
      "\n",
      "Training and evaluating SVC with BERT_TWEET embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5093    0.7569    0.6089       144\n",
      "           1     0.6121    0.6823    0.6453       192\n",
      "           2     0.9068    0.7718    0.8339       618\n",
      "\n",
      "    accuracy                         0.7516       954\n",
      "   macro avg     0.6761    0.7370    0.6961       954\n",
      "weighted avg     0.7875    0.7516    0.7620       954\n",
      "\n",
      "\n",
      "Training and evaluating XGB with BERT_TWEET embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6986    0.3542    0.4700       144\n",
      "           1     0.7109    0.4740    0.5687       192\n",
      "           2     0.7822    0.9531    0.8592       618\n",
      "\n",
      "    accuracy                         0.7662       954\n",
      "   macro avg     0.7306    0.5937    0.6327       954\n",
      "weighted avg     0.7552    0.7662    0.7420       954\n",
      "\n",
      "\n",
      "Training and evaluating LogisticRegression with BERT_TWEET embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5217    0.7500    0.6154       144\n",
      "           1     0.6018    0.7083    0.6507       192\n",
      "           2     0.9117    0.7686    0.8341       618\n",
      "\n",
      "    accuracy                         0.7537       954\n",
      "   macro avg     0.6784    0.7423    0.7001       954\n",
      "weighted avg     0.7905    0.7537    0.7642       954\n",
      "\n",
      "\n",
      "Training and evaluating KNN with BERT_TWEET embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4865    0.5000    0.4932       144\n",
      "           1     0.5484    0.5312    0.5397       192\n",
      "           2     0.8403    0.8430    0.8417       618\n",
      "\n",
      "    accuracy                         0.7285       954\n",
      "   macro avg     0.6251    0.6248    0.6248       954\n",
      "weighted avg     0.7282    0.7285    0.7283       954\n",
      "\n",
      "\n",
      "Training and evaluating MLP with BERT_TWEET embeddings...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6621 - loss: 0.8191\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7247 - loss: 0.6674\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7515 - loss: 0.6051\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7627 - loss: 0.5735\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7770 - loss: 0.5508\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7855 - loss: 0.5298\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7906 - loss: 0.5070\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8069 - loss: 0.4891\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8086 - loss: 0.4752\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8192 - loss: 0.4632\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8197 - loss: 0.4401\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8331 - loss: 0.4220\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8470 - loss: 0.3955\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8523 - loss: 0.3851\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8552 - loss: 0.3676\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8626 - loss: 0.3490\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8633 - loss: 0.3413\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8723 - loss: 0.3278\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8814 - loss: 0.3041\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8787 - loss: 0.3070\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8914 - loss: 0.2867\n",
      "Epoch 22/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8890 - loss: 0.2851\n",
      "Epoch 23/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8885 - loss: 0.2801\n",
      "Epoch 24/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8944 - loss: 0.2742\n",
      "Epoch 25/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8935 - loss: 0.2603\n",
      "Epoch 26/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8987 - loss: 0.2463\n",
      "Epoch 27/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8953 - loss: 0.2587\n",
      "Epoch 28/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9127 - loss: 0.2318\n",
      "Epoch 29/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9187 - loss: 0.2134\n",
      "Epoch 30/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9125 - loss: 0.2217\n",
      "Epoch 31/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9092 - loss: 0.2252\n",
      "Epoch 32/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9140 - loss: 0.2104\n",
      "Epoch 33/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9108 - loss: 0.2213\n",
      "Epoch 34/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9151 - loss: 0.2148\n",
      "Epoch 35/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9224 - loss: 0.1971\n",
      "Epoch 36/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9204 - loss: 0.1965\n",
      "Epoch 37/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9306 - loss: 0.1781\n",
      "Epoch 38/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9308 - loss: 0.1808\n",
      "Epoch 39/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9186 - loss: 0.2054\n",
      "Epoch 40/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9306 - loss: 0.1791\n",
      "Epoch 41/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9329 - loss: 0.1656\n",
      "Epoch 42/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9391 - loss: 0.1555\n",
      "Epoch 43/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9430 - loss: 0.1525\n",
      "Epoch 44/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9319 - loss: 0.1675\n",
      "Epoch 45/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9333 - loss: 0.1653\n",
      "Epoch 46/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9364 - loss: 0.1565\n",
      "Epoch 47/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9403 - loss: 0.1564\n",
      "Epoch 48/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9351 - loss: 0.1596\n",
      "Epoch 49/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9341 - loss: 0.1563\n",
      "Epoch 50/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9372 - loss: 0.1566\n",
      "Epoch 51/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9392 - loss: 0.1611\n",
      "Epoch 52/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9491 - loss: 0.1300\n",
      "Epoch 53/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9496 - loss: 0.1330\n",
      "Epoch 54/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9535 - loss: 0.1207\n",
      "Epoch 55/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9512 - loss: 0.1267\n",
      "Epoch 56/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9574 - loss: 0.1191\n",
      "Epoch 57/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9451 - loss: 0.1408\n",
      "Epoch 58/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9431 - loss: 0.1352\n",
      "Epoch 59/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9505 - loss: 0.1244\n",
      "Epoch 60/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9518 - loss: 0.1227\n",
      "Epoch 61/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9507 - loss: 0.1326\n",
      "Epoch 62/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9590 - loss: 0.1161\n",
      "Epoch 63/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9520 - loss: 0.1265\n",
      "Epoch 64/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9592 - loss: 0.1063\n",
      "Epoch 65/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9512 - loss: 0.1216\n",
      "Epoch 66/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9635 - loss: 0.0995\n",
      "Epoch 67/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9568 - loss: 0.1173\n",
      "Epoch 68/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9552 - loss: 0.1287\n",
      "Epoch 69/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9501 - loss: 0.1276\n",
      "Epoch 70/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9532 - loss: 0.1203\n",
      "Epoch 71/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9513 - loss: 0.1178\n",
      "Epoch 72/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9569 - loss: 0.1081\n",
      "Epoch 73/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9622 - loss: 0.1053\n",
      "Epoch 74/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9549 - loss: 0.1086\n",
      "Epoch 75/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9615 - loss: 0.1008\n",
      "Epoch 76/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9615 - loss: 0.0988\n",
      "Epoch 77/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9596 - loss: 0.1137\n",
      "Epoch 78/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9514 - loss: 0.1303\n",
      "Epoch 79/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9600 - loss: 0.1075\n",
      "Epoch 80/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9618 - loss: 0.1035\n",
      "Epoch 81/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9605 - loss: 0.1033\n",
      "Epoch 82/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9570 - loss: 0.1154\n",
      "Epoch 83/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9582 - loss: 0.1096\n",
      "Epoch 84/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9701 - loss: 0.0778\n",
      "Epoch 85/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9623 - loss: 0.0939\n",
      "Epoch 86/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9671 - loss: 0.0924\n",
      "Epoch 87/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9587 - loss: 0.1071\n",
      "Epoch 88/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9613 - loss: 0.0955\n",
      "Epoch 89/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9617 - loss: 0.1017\n",
      "Epoch 90/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9684 - loss: 0.0827\n",
      "Epoch 91/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9660 - loss: 0.0942\n",
      "Epoch 92/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9591 - loss: 0.0955\n",
      "Epoch 93/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9646 - loss: 0.0996\n",
      "Epoch 94/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9630 - loss: 0.0963\n",
      "Epoch 95/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9625 - loss: 0.0977\n",
      "Epoch 96/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9645 - loss: 0.1002\n",
      "Epoch 97/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9690 - loss: 0.0878\n",
      "Epoch 98/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9692 - loss: 0.0773\n",
      "Epoch 99/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9647 - loss: 0.0910\n",
      "Epoch 100/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9583 - loss: 0.1016\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6331    0.6111    0.6219       144\n",
      "           1     0.6875    0.6302    0.6576       192\n",
      "           2     0.8576    0.8867    0.8719       618\n",
      "\n",
      "    accuracy                         0.7935       954\n",
      "   macro avg     0.7261    0.7094    0.7171       954\n",
      "weighted avg     0.7895    0.7935    0.7910       954\n",
      "\n",
      "\n",
      "Training and evaluating SVC with FINTWITBERT embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5544    0.7431    0.6350       144\n",
      "           1     0.6566    0.6771    0.6667       192\n",
      "           2     0.8917    0.8123    0.8501       618\n",
      "\n",
      "    accuracy                         0.7746       954\n",
      "   macro avg     0.7009    0.7441    0.7173       954\n",
      "weighted avg     0.7934    0.7746    0.7807       954\n",
      "\n",
      "\n",
      "Training and evaluating XGB with FINTWITBERT embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7835    0.5278    0.6307       144\n",
      "           1     0.7226    0.5156    0.6018       192\n",
      "           2     0.8014    0.9337    0.8625       618\n",
      "\n",
      "    accuracy                         0.7883       954\n",
      "   macro avg     0.7692    0.6590    0.6983       954\n",
      "weighted avg     0.7828    0.7883    0.7750       954\n",
      "\n",
      "\n",
      "Training and evaluating LogisticRegression with FINTWITBERT embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4829    0.6875    0.5673       144\n",
      "           1     0.5588    0.6927    0.6186       192\n",
      "           2     0.8787    0.7265    0.7954       618\n",
      "\n",
      "    accuracy                         0.7138       954\n",
      "   macro avg     0.6401    0.7022    0.6604       954\n",
      "weighted avg     0.7546    0.7138    0.7254       954\n",
      "\n",
      "\n",
      "Training and evaluating KNN with FINTWITBERT embeddings...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5944    0.5903    0.5923       144\n",
      "           1     0.7115    0.5781    0.6379       192\n",
      "           2     0.8244    0.8738    0.8484       618\n",
      "\n",
      "    accuracy                         0.7715       954\n",
      "   macro avg     0.7101    0.6807    0.6929       954\n",
      "weighted avg     0.7670    0.7715    0.7674       954\n",
      "\n",
      "\n",
      "Training and evaluating MLP with FINTWITBERT embeddings...\n",
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6729 - loss: 0.7815\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7481 - loss: 0.6452\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7600 - loss: 0.6127\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7676 - loss: 0.5723\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7750 - loss: 0.5594\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7876 - loss: 0.5317\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7938 - loss: 0.5189\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8011 - loss: 0.4977\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8083 - loss: 0.4793\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8217 - loss: 0.4564\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8185 - loss: 0.4546\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8339 - loss: 0.4152\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8390 - loss: 0.4001\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8417 - loss: 0.3876\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8512 - loss: 0.3628\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8541 - loss: 0.3520\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8599 - loss: 0.3452\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8644 - loss: 0.3358\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8682 - loss: 0.3286\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8792 - loss: 0.3016\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8780 - loss: 0.2985\n",
      "Epoch 22/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8835 - loss: 0.2859\n",
      "Epoch 23/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8824 - loss: 0.2865\n",
      "Epoch 24/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8914 - loss: 0.2684\n",
      "Epoch 25/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8936 - loss: 0.2556\n",
      "Epoch 26/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8894 - loss: 0.2744\n",
      "Epoch 27/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9004 - loss: 0.2460\n",
      "Epoch 28/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9040 - loss: 0.2351\n",
      "Epoch 29/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9027 - loss: 0.2329\n",
      "Epoch 30/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9112 - loss: 0.2337\n",
      "Epoch 31/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9060 - loss: 0.2302\n",
      "Epoch 32/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9027 - loss: 0.2271\n",
      "Epoch 33/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9128 - loss: 0.2101\n",
      "Epoch 34/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9119 - loss: 0.2183\n",
      "Epoch 35/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9268 - loss: 0.1913\n",
      "Epoch 36/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9237 - loss: 0.1977\n",
      "Epoch 37/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9265 - loss: 0.1962\n",
      "Epoch 38/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9211 - loss: 0.1949\n",
      "Epoch 39/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9257 - loss: 0.1870\n",
      "Epoch 40/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9261 - loss: 0.1753\n",
      "Epoch 41/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9245 - loss: 0.1810\n",
      "Epoch 42/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9420 - loss: 0.1596\n",
      "Epoch 43/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9337 - loss: 0.1648\n",
      "Epoch 44/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9319 - loss: 0.1772\n",
      "Epoch 45/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9362 - loss: 0.1537\n",
      "Epoch 46/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9372 - loss: 0.1602\n",
      "Epoch 47/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9334 - loss: 0.1725\n",
      "Epoch 48/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9378 - loss: 0.1559\n",
      "Epoch 49/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9372 - loss: 0.1577\n",
      "Epoch 50/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9341 - loss: 0.1651\n",
      "Epoch 51/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9391 - loss: 0.1572\n",
      "Epoch 52/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9384 - loss: 0.1535\n",
      "Epoch 53/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9467 - loss: 0.1270\n",
      "Epoch 54/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9431 - loss: 0.1396\n",
      "Epoch 55/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9353 - loss: 0.1567\n",
      "Epoch 56/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9469 - loss: 0.1421\n",
      "Epoch 57/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9419 - loss: 0.1431\n",
      "Epoch 58/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9411 - loss: 0.1454\n",
      "Epoch 59/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9489 - loss: 0.1331\n",
      "Epoch 60/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9407 - loss: 0.1417\n",
      "Epoch 61/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9469 - loss: 0.1380\n",
      "Epoch 62/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9417 - loss: 0.1363\n",
      "Epoch 63/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9474 - loss: 0.1298\n",
      "Epoch 64/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9543 - loss: 0.1144\n",
      "Epoch 65/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9512 - loss: 0.1196\n",
      "Epoch 66/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9492 - loss: 0.1235\n",
      "Epoch 67/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9457 - loss: 0.1337\n",
      "Epoch 68/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9542 - loss: 0.1124\n",
      "Epoch 69/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9534 - loss: 0.1140\n",
      "Epoch 70/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9539 - loss: 0.1199\n",
      "Epoch 71/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9501 - loss: 0.1179\n",
      "Epoch 72/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9497 - loss: 0.1242\n",
      "Epoch 73/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9520 - loss: 0.1202\n",
      "Epoch 74/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9552 - loss: 0.1231\n",
      "Epoch 75/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9566 - loss: 0.1154\n",
      "Epoch 76/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9500 - loss: 0.1357\n",
      "Epoch 77/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9524 - loss: 0.1275\n",
      "Epoch 78/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9577 - loss: 0.1049\n",
      "Epoch 79/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9605 - loss: 0.1014\n",
      "Epoch 80/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9598 - loss: 0.1113\n",
      "Epoch 81/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9581 - loss: 0.1068\n",
      "Epoch 82/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9566 - loss: 0.1082\n",
      "Epoch 83/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9637 - loss: 0.1040\n",
      "Epoch 84/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9546 - loss: 0.1115\n",
      "Epoch 85/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9614 - loss: 0.1040\n",
      "Epoch 86/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9651 - loss: 0.0966\n",
      "Epoch 87/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9620 - loss: 0.0982\n",
      "Epoch 88/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9555 - loss: 0.1116\n",
      "Epoch 89/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9614 - loss: 0.0997\n",
      "Epoch 90/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9622 - loss: 0.1059\n",
      "Epoch 91/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9581 - loss: 0.1019\n",
      "Epoch 92/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9645 - loss: 0.0908\n",
      "Epoch 93/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9627 - loss: 0.0976\n",
      "Epoch 94/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9629 - loss: 0.0959\n",
      "Epoch 95/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9580 - loss: 0.1079\n",
      "Epoch 96/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9654 - loss: 0.0900\n",
      "Epoch 97/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9677 - loss: 0.0845\n",
      "Epoch 98/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9668 - loss: 0.0931\n",
      "Epoch 99/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9656 - loss: 0.1017\n",
      "Epoch 100/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9710 - loss: 0.0815\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5897    0.6389    0.6133       144\n",
      "           1     0.7160    0.6302    0.6704       192\n",
      "           2     0.8585    0.8738    0.8661       618\n",
      "\n",
      "    accuracy                         0.7893       954\n",
      "   macro avg     0.7214    0.7143    0.7166       954\n",
      "weighted avg     0.7893    0.7893    0.7885       954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_domain_transformers_fe = {} \n",
    "\n",
    "for name, (X_train, X_val) in domain_transformers_fe_variants.items():\n",
    "    y_train_cls = y_train.argmax(axis=1) if y_train.ndim > 1 else y_train\n",
    "    y_val_cls   = y_val.argmax(axis=1) if y_val.ndim > 1 else y_val\n",
    "\n",
    "    for model_name, model in sentence_encoder_models.items():\n",
    "        print(f\"\\nTraining and evaluating {model_name} with {name.upper()} embeddings...\")\n",
    "\n",
    "        if model_name == \"MLP\":\n",
    "            model = KerasClassifier(\n",
    "                model=create_mlp_model, \n",
    "                embedding_dim=X_train.shape[1], \n",
    "                num_classes=num_classes,\n",
    "                epochs=100, batch_size=32, verbose=1\n",
    "            )\n",
    "        else:\n",
    "            model = clone(model)\n",
    "\n",
    "        model.fit(X_train, y_train_cls)\n",
    "        y_pred = model.predict(X_val)\n",
    "        print(classification_report(y_val_cls, y_pred, digits=4))\n",
    "        results_domain_transformers_fe.setdefault(name, {})[model_name] = classification_report(\n",
    "            y_val_cls, y_pred, digits=4, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ab5a90",
   "metadata": {},
   "source": [
    "#### 2) Fine Tunning -> FinTwit BERT , BART "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d7fc5",
   "metadata": {},
   "source": [
    "Fine Tunning FinTwit BERT ( encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5142fdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at StephanAkkerman/FinTwitBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 21366.21 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 22288.26 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_1953/2061078254.py:75: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3820' max='95500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 3820/95500 07:31 < 3:00:49, 8.45 it/s, Epoch 4/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.584000</td>\n",
       "      <td>0.567262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.319500</td>\n",
       "      <td>0.616466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.219500</td>\n",
       "      <td>0.696817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.144700</td>\n",
       "      <td>0.644307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7542    0.6181    0.6794       144\n",
      "           1     0.8662    0.6406    0.7365       192\n",
      "           2     0.8473    0.9515    0.8963       618\n",
      "\n",
      "    accuracy                         0.8386       954\n",
      "   macro avg     0.8226    0.7367    0.7708       954\n",
      "weighted avg     0.8370    0.8386    0.8314       954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Disable tokenizer parallelism\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Device\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"StephanAkkerman/FinTwitBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text_no_lemma_stem_with_stopwords\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "    \n",
    "train_dataset = train_df.map(tokenize_function, batched=True)\n",
    "val_dataset = val_df.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "val_dataset = val_dataset.rename_column(\"label\", \"labels\")\n",
    "columns_to_keep = ['input_ids', 'attention_mask', 'labels']\n",
    "train_dataset.set_format(type=\"torch\", columns=columns_to_keep)\n",
    "val_dataset.set_format(type=\"torch\", columns=columns_to_keep)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "preds = trainer.predict(val_dataset)\n",
    "y_pred = np.argmax(preds.predictions, axis=1)\n",
    "y_true = val_df[\"label\"] \n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dd3671",
   "metadata": {},
   "source": [
    "Fine tuning BART (encoder-decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d824576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 25407.36 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 25256.05 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_1953/3851088187.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4775' max='95500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4775/95500 12:53 < 4:05:05, 6.17 it/s, Epoch 5/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.681900</td>\n",
       "      <td>0.552077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.478800</td>\n",
       "      <td>0.479978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.364200</td>\n",
       "      <td>0.626906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.309800</td>\n",
       "      <td>0.726914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.241500</td>\n",
       "      <td>0.740788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7731    0.6389    0.6996       144\n",
      "           1     0.8000    0.7083    0.7514       192\n",
      "           2     0.8647    0.9304    0.8963       618\n",
      "\n",
      "    accuracy                         0.8417       954\n",
      "   macro avg     0.8126    0.7592    0.7824       954\n",
      "weighted avg     0.8378    0.8417    0.8375       954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"facebook/bart-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text_no_lemma_stem_with_stopwords\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    )\n",
    "    \n",
    "train_dataset = train_df.map(tokenize_function, batched=True)\n",
    "val_dataset = val_df.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.rename_column(\"label\", \"labels\")\n",
    "val_dataset = val_dataset.rename_column(\"label\", \"labels\")\n",
    "columns_to_keep = ['input_ids', 'attention_mask', 'labels']\n",
    "train_dataset.set_format(type=\"torch\", columns=columns_to_keep)\n",
    "val_dataset.set_format(type=\"torch\", columns=columns_to_keep)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_BART\",\n",
    "    num_train_epochs=100,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir='./logs_BART',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "preds = trainer.predict(val_dataset)\n",
    "logits = preds.predictions[0] if isinstance(preds.predictions, tuple) else preds.predictions\n",
    "y_pred = np.argmax(logits, axis=1)\n",
    "y_true = np.array(val_df[\"label\"])\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f2c90",
   "metadata": {},
   "source": [
    "### 2 - Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886eff0b",
   "metadata": {},
   "source": [
    "We decided to perform hyperparameter tuning only to our best model that it is not a transformer, due to computational limitations. Nonetheless, we recognize that in an ideal experiment we should have done hyperparameter tuning for each model and its respective embedding technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbef9aa",
   "metadata": {},
   "source": [
    "1) Try different embedding dimensions for GLOVE-Twitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4673b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove_100d = npy_embeddings[\"glove_seq_text_no_lemma_stem_with_stopwords_100d_train\"]\n",
    "X_val_glove_100d = npy_embeddings[\"glove_seq_text_no_lemma_stem_with_stopwords_100d_val\"]\n",
    "\n",
    "X_train_glove_50d = npy_embeddings[\"glove_seq_text_no_lemma_stem_with_stopwords_50d_train\"]\n",
    "X_val_glove_50d = npy_embeddings[\"glove_seq_text_no_lemma_stem_with_stopwords_50d_val\"]\n",
    "\n",
    "X_train_glove_25d = npy_embeddings[\"glove_seq_text_no_lemma_stem_with_stopwords_25d_train\"]\n",
    "X_val_glove_25d = npy_embeddings[\"glove_seq_text_no_lemma_stem_with_stopwords_25d_val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac84327",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dimensions_variants = {\n",
    "    \"glove_100d\":   (X_train_glove_100d, X_val_glove_100d),\n",
    "    \"glove_50d\":      (X_train_glove_50d,    X_val_glove_50d),\n",
    "    \"glove_25d\":     (X_train_glove_25d,   X_val_glove_25d),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542978c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating BiGRU with GLOVE_200D embeddings...\n",
      "WARNING:tensorflow:From c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 67ms/step - accuracy: 0.6443 - loss: 0.8496 - val_accuracy: 0.7589 - val_loss: 0.6351\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.7517 - loss: 0.6366 - val_accuracy: 0.7851 - val_loss: 0.5553\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.7777 - loss: 0.5701 - val_accuracy: 0.8019 - val_loss: 0.5243\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7958 - loss: 0.5192 - val_accuracy: 0.8124 - val_loss: 0.5069\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.8183 - loss: 0.4803 - val_accuracy: 0.8166 - val_loss: 0.5016\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - accuracy: 0.8280 - loss: 0.4485 - val_accuracy: 0.8197 - val_loss: 0.4901\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.8446 - loss: 0.4137 - val_accuracy: 0.8187 - val_loss: 0.4895\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.8557 - loss: 0.3807 - val_accuracy: 0.8197 - val_loss: 0.4844\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.8684 - loss: 0.3524 - val_accuracy: 0.8239 - val_loss: 0.4841\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.8759 - loss: 0.3204 - val_accuracy: 0.8239 - val_loss: 0.5068\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.8936 - loss: 0.2925 - val_accuracy: 0.8323 - val_loss: 0.5114\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - accuracy: 0.9007 - loss: 0.2657 - val_accuracy: 0.8270 - val_loss: 0.5121\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 35ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7259    0.6806    0.7025       144\n",
      "           1     0.7546    0.6406    0.6930       192\n",
      "           2     0.8613    0.9142    0.8870       618\n",
      "\n",
      "    accuracy                         0.8239       954\n",
      "   macro avg     0.7806    0.7451    0.7608       954\n",
      "weighted avg     0.8194    0.8239    0.8201       954\n",
      "\n",
      "\n",
      "Training and evaluating BiGRU with GLOVE_100D embeddings...\n",
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 42ms/step - accuracy: 0.6252 - loss: 0.8810 - val_accuracy: 0.7212 - val_loss: 0.6783\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 35ms/step - accuracy: 0.7189 - loss: 0.6999 - val_accuracy: 0.7725 - val_loss: 0.5917\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - accuracy: 0.7533 - loss: 0.6408 - val_accuracy: 0.7851 - val_loss: 0.5563\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 32ms/step - accuracy: 0.7692 - loss: 0.5951 - val_accuracy: 0.7956 - val_loss: 0.5328\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.7877 - loss: 0.5566 - val_accuracy: 0.7998 - val_loss: 0.5206\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.8016 - loss: 0.5200 - val_accuracy: 0.8082 - val_loss: 0.5103\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.8052 - loss: 0.4993 - val_accuracy: 0.8092 - val_loss: 0.5082\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.8150 - loss: 0.4758 - val_accuracy: 0.8134 - val_loss: 0.5002\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.8283 - loss: 0.4549 - val_accuracy: 0.8176 - val_loss: 0.4935\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.8335 - loss: 0.4392 - val_accuracy: 0.8176 - val_loss: 0.4910\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.8395 - loss: 0.4137 - val_accuracy: 0.8208 - val_loss: 0.5023\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.8535 - loss: 0.3851 - val_accuracy: 0.8197 - val_loss: 0.5056\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 34ms/step - accuracy: 0.8581 - loss: 0.3797 - val_accuracy: 0.8249 - val_loss: 0.4941\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7414    0.5972    0.6615       144\n",
      "           1     0.7516    0.6146    0.6762       192\n",
      "           2     0.8458    0.9320    0.8868       618\n",
      "\n",
      "    accuracy                         0.8176       954\n",
      "   macro avg     0.7796    0.7146    0.7415       954\n",
      "weighted avg     0.8111    0.8176    0.8104       954\n",
      "\n",
      "\n",
      "Training and evaluating BiGRU with GLOVE_50D embeddings...\n",
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 37ms/step - accuracy: 0.6257 - loss: 0.9031 - val_accuracy: 0.6656 - val_loss: 0.7625\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - accuracy: 0.6729 - loss: 0.7706 - val_accuracy: 0.7180 - val_loss: 0.6991\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - accuracy: 0.6968 - loss: 0.7303 - val_accuracy: 0.7390 - val_loss: 0.6490\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 31ms/step - accuracy: 0.7291 - loss: 0.6824 - val_accuracy: 0.7505 - val_loss: 0.6211\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 32ms/step - accuracy: 0.7409 - loss: 0.6576 - val_accuracy: 0.7537 - val_loss: 0.6059\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 32ms/step - accuracy: 0.7542 - loss: 0.6314 - val_accuracy: 0.7547 - val_loss: 0.5931\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - accuracy: 0.7622 - loss: 0.6029 - val_accuracy: 0.7683 - val_loss: 0.5812\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 32ms/step - accuracy: 0.7757 - loss: 0.5826 - val_accuracy: 0.7715 - val_loss: 0.5740\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 32ms/step - accuracy: 0.7789 - loss: 0.5638 - val_accuracy: 0.7704 - val_loss: 0.5719\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - accuracy: 0.7854 - loss: 0.5456 - val_accuracy: 0.7820 - val_loss: 0.5618\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - accuracy: 0.7954 - loss: 0.5278 - val_accuracy: 0.7851 - val_loss: 0.5537\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - accuracy: 0.8020 - loss: 0.5125 - val_accuracy: 0.7893 - val_loss: 0.5562\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.8073 - loss: 0.4973 - val_accuracy: 0.7893 - val_loss: 0.5422\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.8142 - loss: 0.4808 - val_accuracy: 0.7904 - val_loss: 0.5435\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.8145 - loss: 0.4753 - val_accuracy: 0.7914 - val_loss: 0.5370\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.8226 - loss: 0.4592 - val_accuracy: 0.7998 - val_loss: 0.5424\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.8270 - loss: 0.4451 - val_accuracy: 0.7925 - val_loss: 0.5357\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 65ms/step - accuracy: 0.8267 - loss: 0.4376 - val_accuracy: 0.8040 - val_loss: 0.5300\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.8330 - loss: 0.4182 - val_accuracy: 0.7977 - val_loss: 0.5320\n",
      "Epoch 20/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.8429 - loss: 0.4028 - val_accuracy: 0.7987 - val_loss: 0.5411\n",
      "Epoch 21/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.8440 - loss: 0.4130 - val_accuracy: 0.8019 - val_loss: 0.5456\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6838    0.5556    0.6130       144\n",
      "           1     0.7362    0.6250    0.6761       192\n",
      "           2     0.8412    0.9175    0.8777       618\n",
      "\n",
      "    accuracy                         0.8040       954\n",
      "   macro avg     0.7537    0.6993    0.7223       954\n",
      "weighted avg     0.7963    0.8040    0.7972       954\n",
      "\n",
      "\n",
      "Training and evaluating BiGRU with GLOVE_25D embeddings...\n",
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 47ms/step - accuracy: 0.6374 - loss: 0.9009 - val_accuracy: 0.6478 - val_loss: 0.8153\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.6494 - loss: 0.8334 - val_accuracy: 0.6667 - val_loss: 0.7623\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 39ms/step - accuracy: 0.6679 - loss: 0.7955 - val_accuracy: 0.6866 - val_loss: 0.7236\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 33ms/step - accuracy: 0.6831 - loss: 0.7616 - val_accuracy: 0.7159 - val_loss: 0.6846\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - accuracy: 0.7004 - loss: 0.7295 - val_accuracy: 0.7243 - val_loss: 0.6723\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.7039 - loss: 0.7133 - val_accuracy: 0.7453 - val_loss: 0.6507\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7146 - loss: 0.6914 - val_accuracy: 0.7463 - val_loss: 0.6392\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.7314 - loss: 0.6808 - val_accuracy: 0.7495 - val_loss: 0.6382\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.7258 - loss: 0.6741 - val_accuracy: 0.7537 - val_loss: 0.6315\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 50ms/step - accuracy: 0.7413 - loss: 0.6586 - val_accuracy: 0.7579 - val_loss: 0.6239\n",
      "Epoch 11/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 42ms/step - accuracy: 0.7449 - loss: 0.6415 - val_accuracy: 0.7526 - val_loss: 0.6231\n",
      "Epoch 12/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.7497 - loss: 0.6271 - val_accuracy: 0.7547 - val_loss: 0.6146\n",
      "Epoch 13/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - accuracy: 0.7541 - loss: 0.6196 - val_accuracy: 0.7526 - val_loss: 0.6102\n",
      "Epoch 14/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.7587 - loss: 0.6065 - val_accuracy: 0.7568 - val_loss: 0.6052\n",
      "Epoch 15/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - accuracy: 0.7633 - loss: 0.5902 - val_accuracy: 0.7589 - val_loss: 0.6030\n",
      "Epoch 16/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 44ms/step - accuracy: 0.7677 - loss: 0.5880 - val_accuracy: 0.7589 - val_loss: 0.6021\n",
      "Epoch 17/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - accuracy: 0.7700 - loss: 0.5837 - val_accuracy: 0.7631 - val_loss: 0.6145\n",
      "Epoch 18/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - accuracy: 0.7668 - loss: 0.5802 - val_accuracy: 0.7621 - val_loss: 0.6113\n",
      "Epoch 19/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 47ms/step - accuracy: 0.7734 - loss: 0.5716 - val_accuracy: 0.7683 - val_loss: 0.6053\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 84ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6275    0.4444    0.5203       144\n",
      "           1     0.6870    0.4688    0.5573       192\n",
      "           2     0.7906    0.9223    0.8514       618\n",
      "\n",
      "    accuracy                         0.7589       954\n",
      "   macro avg     0.7017    0.6118    0.6430       954\n",
      "weighted avg     0.7451    0.7589    0.7422       954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_glove_dimensions_bigru_att = {}\n",
    "\n",
    "for name, (X_train, X_val) in glove_dimensions_variants.items():\n",
    "    print(f\"\\nTraining and evaluating BiGRU with {name.upper()} embeddings...\")\n",
    "\n",
    "    model = build_bigru_att(\n",
    "        units=64,\n",
    "        input_length=X_train.shape[1],\n",
    "        embed_size=X_train.shape[2],\n",
    "        num_classes=num_classes\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train_cat,\n",
    "        validation_data=(X_val, y_val_cat),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    y_pred_probs = model.predict(X_val)\n",
    "    y_pred = y_pred_probs.argmax(axis=1)\n",
    "    y_val_argmax = y_val.argmax(axis=1) if y_val.ndim > 1 else y_val\n",
    "\n",
    "    print(classification_report(y_val_argmax, y_pred, digits=4))\n",
    "\n",
    "    results_glove_dimensions_bigru_att[name] = classification_report(y_val_argmax, y_pred, digits=4, output_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8c20ec",
   "metadata": {},
   "source": [
    "2) Test different attention layer configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc218b",
   "metadata": {},
   "source": [
    "2.1 - Bahdanau (Additive) Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd3f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, values):\n",
    "        score = tf.nn.tanh(self.W1(values))  \n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)  \n",
    "        context_vector = attention_weights * values  \n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)  \n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd437ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigru_bahdanau_att(units, input_length, embed_size, num_classes=3):\n",
    "    input_ = Input(shape=(input_length, embed_size))\n",
    "    x = Bidirectional(GRU(units, return_sequences=True, dropout=0.25, recurrent_dropout=0.25))(input_)\n",
    "    x = BahdanauAttention(units)(x) \n",
    "    out = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=input_, outputs=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c35788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating BiGRU + Bahdanau Attention with GloVe embeddings...\n",
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 39ms/step - accuracy: 0.6506 - loss: 0.8427 - val_accuracy: 0.7600 - val_loss: 0.6189\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.7490 - loss: 0.6407 - val_accuracy: 0.7830 - val_loss: 0.5671\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 58ms/step - accuracy: 0.7791 - loss: 0.5746 - val_accuracy: 0.7966 - val_loss: 0.5304\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 61ms/step - accuracy: 0.7989 - loss: 0.5341 - val_accuracy: 0.8113 - val_loss: 0.5163\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - accuracy: 0.8122 - loss: 0.4886 - val_accuracy: 0.8061 - val_loss: 0.5146\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.8291 - loss: 0.4583 - val_accuracy: 0.8239 - val_loss: 0.5036\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 35ms/step - accuracy: 0.8323 - loss: 0.4276 - val_accuracy: 0.8291 - val_loss: 0.4907\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 36ms/step - accuracy: 0.8496 - loss: 0.3977 - val_accuracy: 0.8333 - val_loss: 0.5011\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 38ms/step - accuracy: 0.8638 - loss: 0.3655 - val_accuracy: 0.8323 - val_loss: 0.4995\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 37ms/step - accuracy: 0.8718 - loss: 0.3414 - val_accuracy: 0.8407 - val_loss: 0.5074\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 40ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7705    0.6528    0.7068       144\n",
      "           1     0.7625    0.6354    0.6932       192\n",
      "           2     0.8557    0.9304    0.8915       618\n",
      "\n",
      "    accuracy                         0.8291       954\n",
      "   macro avg     0.7962    0.7395    0.7638       954\n",
      "weighted avg     0.8241    0.8291    0.8237       954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_bigru_bahdanau_att = {}\n",
    "\n",
    "print(f\"\\nTraining and evaluating BiGRU + Bahdanau Attention with GloVe embeddings...\")\n",
    "\n",
    "model = build_bigru_bahdanau_att(\n",
    "    units=64,\n",
    "    input_length=X_train_glove.shape[1],\n",
    "    embed_size=X_train_glove.shape[2],\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train_glove, y_train_cat,\n",
    "    validation_data=(X_val_glove, y_val_cat),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "y_pred_probs = model.predict(X_val_glove)\n",
    "y_pred = y_pred_probs.argmax(axis=1)\n",
    "y_val_argmax = y_val.argmax(axis=1) if y_val.ndim > 1 else y_val\n",
    "\n",
    "print(classification_report(y_val_argmax, y_pred, digits=4))\n",
    "\n",
    "results_bigru_bahdanau_att[\"glove\"] = classification_report(\n",
    "    y_val_argmax, y_pred, digits=4, output_dict=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94487215",
   "metadata": {},
   "source": [
    "2.2 - Multi-Head Self-Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e8770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.mha(inputs, inputs) \n",
    "        return tf.reduce_mean(attn_output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29b1d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigru_multiheadself_att(units, input_length, embed_size, num_classes=3):\n",
    "    input_ = Input(shape=(input_length, embed_size))\n",
    "    x = Bidirectional(GRU(units, return_sequences=True, dropout=0.25, recurrent_dropout=0.25))(input_)\n",
    "    x = MultiHeadSelfAttention(units)(x) \n",
    "    out = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=input_, outputs=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5822e5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating BiGRU + Multi Head Self Attention with GloVe embeddings...\n",
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 84ms/step - accuracy: 0.6477 - loss: 0.8447 - val_accuracy: 0.7201 - val_loss: 0.6542\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 76ms/step - accuracy: 0.7283 - loss: 0.6791 - val_accuracy: 0.7736 - val_loss: 0.5704\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 78ms/step - accuracy: 0.7685 - loss: 0.5955 - val_accuracy: 0.7893 - val_loss: 0.5402\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 71ms/step - accuracy: 0.7945 - loss: 0.5350 - val_accuracy: 0.8050 - val_loss: 0.5165\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 74ms/step - accuracy: 0.8066 - loss: 0.4963 - val_accuracy: 0.8197 - val_loss: 0.5023\n",
      "Epoch 6/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 88ms/step - accuracy: 0.8324 - loss: 0.4417 - val_accuracy: 0.8113 - val_loss: 0.5036\n",
      "Epoch 7/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 87ms/step - accuracy: 0.8429 - loss: 0.4100 - val_accuracy: 0.8249 - val_loss: 0.4987\n",
      "Epoch 8/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 82ms/step - accuracy: 0.8603 - loss: 0.3683 - val_accuracy: 0.8281 - val_loss: 0.5040\n",
      "Epoch 9/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 79ms/step - accuracy: 0.8766 - loss: 0.3374 - val_accuracy: 0.8229 - val_loss: 0.5358\n",
      "Epoch 10/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 86ms/step - accuracy: 0.8861 - loss: 0.3080 - val_accuracy: 0.8229 - val_loss: 0.5876\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 59ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7190    0.6042    0.6566       144\n",
      "           1     0.7898    0.6458    0.7106       192\n",
      "           2     0.8521    0.9320    0.8903       618\n",
      "\n",
      "    accuracy                         0.8249       954\n",
      "   macro avg     0.7870    0.7273    0.7525       954\n",
      "weighted avg     0.8195    0.8249    0.8188       954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_bigru_multiheadself_att = {}\n",
    "\n",
    "print(f\"\\nTraining and evaluating BiGRU + Multi Head Self Attention with GloVe embeddings...\")\n",
    "\n",
    "model = build_bigru_multiheadself_att(\n",
    "    units=64,\n",
    "    input_length=X_train_glove.shape[1],\n",
    "    embed_size=X_train_glove.shape[2],\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train_glove, y_train_cat,\n",
    "    validation_data=(X_val_glove, y_val_cat),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "y_pred_probs = model.predict(X_val_glove)\n",
    "y_pred = y_pred_probs.argmax(axis=1)\n",
    "y_val_argmax = y_val.argmax(axis=1) if y_val.ndim > 1 else y_val\n",
    "\n",
    "print(classification_report(y_val_argmax, y_pred, digits=4))\n",
    "\n",
    "results_bigru_multiheadself_att[\"glove\"] = classification_report(\n",
    "    y_val_argmax, y_pred, digits=4, output_dict=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd420149",
   "metadata": {},
   "source": [
    "3) Test different hyperparameters configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b381e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f1(y_true, y_pred):\n",
    "    num_classes = tf.shape(y_true)[-1]\n",
    "    y_pred = K.one_hot(K.argmax(y_pred, axis=-1), num_classes)\n",
    "    y_true = K.cast(y_true, tf.float32)\n",
    "    tp = K.sum(y_true * y_pred, axis=0)\n",
    "    fp = K.sum(y_pred, axis=0) - tp\n",
    "    fn = K.sum(y_true, axis=0) - tp\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "    f1 = 2 * precision * recall / (precision + recall + K.epsilon())\n",
    "    macro_f1 = K.mean(f1)\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d58c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigru_tuner(hp):\n",
    "    input_ = Input(shape=(input_length, embed_size))\n",
    "    x = Dropout(hp.Choice('embedding_dropout', [0.0, 0.2]))(input_)\n",
    "\n",
    "    for i in range(hp.Choice('num_layers', [1, 2])):\n",
    "        return_sequences = (i != hp.get('num_layers') - 1)\n",
    "        x = Bidirectional(\n",
    "            GRU(\n",
    "                units=hp.Choice('units', [64, 128]),\n",
    "                return_sequences=return_sequences,\n",
    "                dropout=hp.Choice('dropout', [0.2, 0.4]),\n",
    "                recurrent_dropout=hp.Choice('recurrent_dropout', [0.2, 0.4]),\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(hp.Choice('l2', [0.0, 1e-4]))\n",
    "            )\n",
    "        )(x)\n",
    "\n",
    "    if hp.Boolean('extra_dense'):\n",
    "        x = Dense(\n",
    "            64, activation='relu',\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(hp.Choice('l2', [0.0, 1e-4]))\n",
    "        )(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "\n",
    "    out = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=input_, outputs=out)\n",
    "\n",
    "    optimizer_choice = hp.Choice('optimizer', ['adam', 'rmsprop', 'nadam'])\n",
    "    learning_rate = hp.Choice('learning_rate', [1e-3, 5e-4])\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'nadam':\n",
    "        optimizer = tf.keras.optimizers.Nadam(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['accuracy', macro_f1]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3372e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = X_train_glove.shape[1]\n",
    "embed_size = X_train_glove.shape[2]\n",
    "num_classes = y_train_cat.shape[1]\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_bigru_tuner,\n",
    "    objective=kt.Objective(\"val_macro_f1\", direction=\"max\"),\n",
    "    max_trials=50, \n",
    "    directory='tuner_dir',\n",
    "    project_name='bigru_att'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfe693d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 50 Complete [00h 18m 45s]\n",
      "val_macro_f1: 0.7721172571182251\n",
      "\n",
      "Best val_macro_f1 So Far: 0.7761452794075012\n",
      "Total elapsed time: 11h 22m 24s\n"
     ]
    }
   ],
   "source": [
    "tuner.search(\n",
    "    X_train_glove, y_train_cat,\n",
    "    validation_data=(X_val_glove, y_val_cat),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b99f0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'embedding_dropout': 0.0, 'num_layers': 1, 'units': 128, 'dropout': 0.4, 'recurrent_dropout': 0.2, 'l2': 0.0, 'extra_dense': True, 'optimizer': 'nadam', 'learning_rate': 0.0005}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexg\\Ambiente de Trabalho\\Mestrado Data Science NOVA IMS\\2nd semester\\Text Mining\\tm_venv\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'nadam', because it has 2 variables whereas the saved optimizer has 23 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "print(\"Best hyperparameters:\", best_hps.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2d3747d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_bigru_att(\n",
    "    units,\n",
    "    input_length,\n",
    "    embed_size,\n",
    "    num_classes=3,\n",
    "    embedding_dropout=0.0,\n",
    "    dropout=0.25,\n",
    "    recurrent_dropout=0.25,\n",
    "    l2=0.0,\n",
    "    extra_dense=False,\n",
    "    optimizer='adam',\n",
    "    learning_rate=0.001\n",
    "):\n",
    "\n",
    "    input_ = Input(shape=(input_length, embed_size))\n",
    "    x = Dropout(embedding_dropout)(input_)\n",
    "\n",
    "    x = Bidirectional(GRU(\n",
    "        units, \n",
    "        return_sequences=True, \n",
    "        dropout=dropout, \n",
    "        recurrent_dropout=recurrent_dropout, \n",
    "        kernel_regularizer=regularizers.l2(l2)\n",
    "    ))(x)\n",
    "    x = SimpleAttention()(x)\n",
    "\n",
    "    if extra_dense:\n",
    "        x = Dense(64, activation='relu', kernel_regularizer=regularizers.l2(l2))(x)\n",
    "        x = Dropout(0.25)(x)\n",
    "\n",
    "    out = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Set optimizer\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'nadam':\n",
    "        opt = Nadam(learning_rate=learning_rate)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported optimizer.\")\n",
    "\n",
    "    model = Model(inputs=input_, outputs=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = build_bigru_att(\n",
    "    units=best_hps['units'],\n",
    "    input_length=input_length,\n",
    "    embed_size=embed_size,\n",
    "    num_classes=3,\n",
    "    embedding_dropout=best_hps['embedding_dropout'],\n",
    "    dropout=best_hps['dropout'],\n",
    "    recurrent_dropout=best_hps['recurrent_dropout'],\n",
    "    l2=best_hps['l2'],\n",
    "    extra_dense=best_hps['extra_dense'],\n",
    "    optimizer=best_hps['optimizer'],\n",
    "    learning_rate=best_hps['learning_rate']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5caa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training and evaluating Tuned BiGRU with GLOVE embeddings...\n",
      "Epoch 1/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 359ms/step - accuracy: 0.8465 - loss: 0.4847 - val_accuracy: 0.8208 - val_loss: 0.5799\n",
      "Epoch 2/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 380ms/step - accuracy: 0.8593 - loss: 0.4457 - val_accuracy: 0.8344 - val_loss: 0.5771\n",
      "Epoch 3/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 306ms/step - accuracy: 0.8759 - loss: 0.3969 - val_accuracy: 0.8291 - val_loss: 0.5772\n",
      "Epoch 4/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 342ms/step - accuracy: 0.8879 - loss: 0.3851 - val_accuracy: 0.8270 - val_loss: 0.6128\n",
      "Epoch 5/100\n",
      "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 356ms/step - accuracy: 0.8937 - loss: 0.3676 - val_accuracy: 0.8239 - val_loss: 0.6064\n",
      "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 209ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7154    0.6458    0.6788       144\n",
      "           1     0.7803    0.7031    0.7397       192\n",
      "           2     0.8725    0.9191    0.8952       618\n",
      "\n",
      "    accuracy                         0.8344       954\n",
      "   macro avg     0.7894    0.7560    0.7713       954\n",
      "weighted avg     0.8302    0.8344    0.8312       954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_bigru_att_tuned_glove = {}\n",
    "\n",
    "print(f\"\\nTraining and evaluating Tuned BiGRU with GLOVE embeddings...\")\n",
    "\n",
    "tuned_model.fit(\n",
    "    X_train_glove, y_train_cat,\n",
    "    validation_data=(X_val_glove, y_val_cat),\n",
    "    epochs=100,  \n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "y_pred_probs = tuned_model.predict(X_val_glove)\n",
    "y_pred = y_pred_probs.argmax(axis=1)\n",
    "y_val_argmax = y_val.argmax(axis=1) if y_val.ndim > 1 else y_val\n",
    "\n",
    "print(classification_report(y_val_argmax, y_pred, digits=4))\n",
    "\n",
    "results_bigru_att_tuned_glove[\"glove\"] = classification_report(y_val_argmax, y_pred, digits=4, output_dict=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
