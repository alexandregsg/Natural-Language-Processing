{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e7f469",
   "metadata": {},
   "source": [
    "## Best text variant notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d42df2f",
   "metadata": {},
   "source": [
    "### Group 8 Members\n",
    "#### Spring Semester 2024-2025\n",
    "- Alexandre Gonçalves - 20240738\n",
    "- Bráulio Damba - 20240007\n",
    "- Hugo Fonseca - 20240520\n",
    "- Ricardo Pereira - 20240745\n",
    "- Victoria Goon - 20240550"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678e8d06",
   "metadata": {},
   "source": [
    "## 1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "201a2b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/ricardo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ricardo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ricardo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/ricardo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Text extraction \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from collections import Counter\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models import FastText\n",
    "\n",
    "import scipy.sparse\n",
    "from scipy import sparse\n",
    "\n",
    "import contractions\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import pickle\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Deep Learning libraries\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Bidirectional, Dropout, Flatten, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Set pd options to display all columns and rows\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_rows\", 30)\n",
    "pd.set_option('display.max_colwidth', None)  # Show full text without truncation\n",
    "\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "666a54e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory (where the notebook is)\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# Construct full paths to the CSV files\n",
    "train_path = os.path.join(BASE_DIR, \"data\", \"train.csv\")\n",
    "test_path = os.path.join(BASE_DIR, \"data\", \"test.csv\")\n",
    "\n",
    "# Load the datasets\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f8a979",
   "metadata": {},
   "source": [
    "## 2 - Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d40183ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.nltk.org/api/nltk.stem.WordNetLemmatizer.html?highlight=wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Source: https://www.nltk.org/api/nltk.tokenize.casual.html\n",
    "# Difference between TweetTokenizer and Word_Tokenize: https://stackoverflow.com/questions/61919670/how-nltk-tweettokenizer-different-from-nltk-word-tokenize\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "# Source: https://www.nltk.org/_modules/nltk/stem/porter.html\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set of English stop words from NLTK\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc69d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_column(text,lemmatizer=None, stemmer=None, remove_stopwords=None):\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace URLs and user mentions\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"URL\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"USER\", text)\n",
    "\n",
    "    # Expand contractions (we use contractions library for this)\n",
    "    # Contractions library Source: https://pypi.org/project/contractions/\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # # Replace numbers with [NUM]\n",
    "    # text = re.sub(r\"\\d+(\\.\\d+)?\", \"[NUM]\", text)\n",
    "\n",
    "    # Convert to tickers (e.g., $AAPL to [TICKER])\n",
    "    text = re.sub(r\"\\$[a-z]{1,5}\", \"[TICKER]\", text)\n",
    "\n",
    "    #Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Normalize punctuation repetitions\n",
    "    text = re.sub(r\"([!?\\.])\\1+\", r\"\\1\", text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Remove stop words and punctuation\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    else:\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Lemmatization OR stemming \n",
    "    if lemmatizer is not None and stemmer is None:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    elif stemmer is not None and lemmatizer is None:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    elif lemmatizer is not None and stemmer is not None:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Else, leave tokens as is\n",
    "\n",
    "    # Source: https://www.nltk.org/api/nltk.tokenize.treebank.html \n",
    "    # TreebankWordDetokenizer from NLTK takes care of the correct spacing and formatting, \n",
    "    # and we get a well-formed sentence that looks like natural English (e.g. without TreebankWordDetokinzer: This is an example tweet ! , With: This is an example tweet!)\n",
    "    return TreebankWordDetokenizer().detokenize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5628e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train.copy()\n",
    "df_test_copy = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71b02efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the different pre-processing combinations to try\n",
    "\n",
    "preproc_combinations = []\n",
    "\n",
    "for lem, stm, rm_stop in product([None, lemmatizer], [None, stemmer], [False, True]):\n",
    "    name = []\n",
    "    name.append('lemma' if lem else 'no_lemma')\n",
    "    name.append('stem' if stm else 'no_stem')\n",
    "    name.append('no_stopwords' if rm_stop else 'with_stopwords')\n",
    "    preproc_combinations.append({\n",
    "        \"lemmatizer\": lem,\n",
    "        \"stemmer\": stm,\n",
    "        \"remove_stopwords\": rm_stop,\n",
    "        \"name\": '_'.join(name)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93268197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preproc_combinations(df, combinations, text_col=\"text\"):\n",
    "    for combo in combinations:\n",
    "        column_name = f\"text_{combo['name']}\"\n",
    "        print(f\"Processing {column_name}...\")\n",
    "        df[column_name] = df[text_col].apply(\n",
    "            lambda x: clean_text_column(\n",
    "                x, \n",
    "                lemmatizer=combo['lemmatizer'], \n",
    "                stemmer=combo['stemmer'], \n",
    "                remove_stopwords=combo['remove_stopwords']\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1348a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text_no_lemma_no_stem_with_stopwords...\n",
      "Processing text_no_lemma_no_stem_no_stopwords...\n",
      "Processing text_no_lemma_stem_with_stopwords...\n",
      "Processing text_no_lemma_stem_no_stopwords...\n",
      "Processing text_lemma_no_stem_with_stopwords...\n",
      "Processing text_lemma_no_stem_no_stopwords...\n",
      "Processing text_lemma_stem_with_stopwords...\n",
      "Processing text_lemma_stem_no_stopwords...\n"
     ]
    }
   ],
   "source": [
    "df_train_copy = apply_preproc_combinations(df_train_copy, preproc_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b16c2736",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cleaned = df_train_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "258a4f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using stratify to maintain the distribution of classes in the train, validation, and test sets \n",
    "# Change this to 80% train, 10% val and 10% test\n",
    "train_df, val_test_df = train_test_split(df_train_cleaned, test_size=0.3, stratify=df_train_cleaned['label'], random_state=42)\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, stratify=val_test_df['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2a21157",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['label']\n",
    "y_val = val_df['label']\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa99c40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_lemma_no_stem_with_stopwords</th>\n",
       "      <th>text_no_lemma_no_stem_no_stopwords</th>\n",
       "      <th>text_no_lemma_stem_with_stopwords</th>\n",
       "      <th>text_no_lemma_stem_no_stopwords</th>\n",
       "      <th>text_lemma_no_stem_with_stopwords</th>\n",
       "      <th>text_lemma_no_stem_no_stopwords</th>\n",
       "      <th>text_lemma_stem_with_stopwords</th>\n",
       "      <th>text_lemma_stem_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7384</th>\n",
       "      <td>Today in Brexit: European Union members are ratcheting up their negotiating demands https://t.co/Qnh48BCc2l</td>\n",
       "      <td>today in brexit european union members are ratcheting up their negotiating demands URL</td>\n",
       "      <td>today brexit european union members ratcheting negotiating demands URL</td>\n",
       "      <td>today in brexit european union member are ratchet up their negoti demand url</td>\n",
       "      <td>today brexit european union member ratchet negoti demand url</td>\n",
       "      <td>today in brexit european union member are ratcheting up their negotiating demand URL</td>\n",
       "      <td>today brexit european union member ratcheting negotiating demand URL</td>\n",
       "      <td>today in brexit european union member are ratcheting up their negotiating demand URL</td>\n",
       "      <td>today brexit european union member ratcheting negotiating demand URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8465</th>\n",
       "      <td>Did Service Corporation International's (NYSE:SCI) Share Price Deserve to Gain 96%?</td>\n",
       "      <td>did service corporation international's nyse sci share price deserve to gain</td>\n",
       "      <td>service corporation international's nyse sci share price deserve gain</td>\n",
       "      <td>did servic corpor international' nyse sci share price deserv to gain</td>\n",
       "      <td>servic corpor international' nyse sci share price deserv gain</td>\n",
       "      <td>did service corporation international's nyse sci share price deserve to gain</td>\n",
       "      <td>service corporation international's nyse sci share price deserve gain</td>\n",
       "      <td>did service corporation international's nyse sci share price deserve to gain</td>\n",
       "      <td>service corporation international's nyse sci share price deserve gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4010</th>\n",
       "      <td>4 Money Moves That'll Make You Richer in 2020</td>\n",
       "      <td>money moves that will make you richer in</td>\n",
       "      <td>money moves make richer</td>\n",
       "      <td>money move that will make you richer in</td>\n",
       "      <td>money move make richer</td>\n",
       "      <td>money move that will make you richer in</td>\n",
       "      <td>money move make richer</td>\n",
       "      <td>money move that will make you richer in</td>\n",
       "      <td>money move make richer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3915</th>\n",
       "      <td>#HunterBiden is just killing my life .... it's so hard to be White now in America ..... it's like being William hun… https://t.co/uHk95qx1Ig</td>\n",
       "      <td>#hunterbiden is just killing my life it is so hard to be white now in america it is like being william hun … URL</td>\n",
       "      <td>#hunterbiden killing life hard white america like william hun … URL</td>\n",
       "      <td>#hunterbiden is just kill my life it is so hard to be white now in america it is like be william hun … url</td>\n",
       "      <td>#hunterbiden kill life hard white america like william hun … url</td>\n",
       "      <td>#hunterbiden is just killing my life it is so hard to be white now in america it is like being william hun … URL</td>\n",
       "      <td>#hunterbiden killing life hard white america like william hun … URL</td>\n",
       "      <td>#hunterbiden is just killing my life it is so hard to be white now in america it is like being william hun … URL</td>\n",
       "      <td>#hunterbiden killing life hard white america like william hun … URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>Seattle Genetics earns drug approval in Canada</td>\n",
       "      <td>seattle genetics earns drug approval in canada</td>\n",
       "      <td>seattle genetics earns drug approval canada</td>\n",
       "      <td>seattl genet earn drug approv in canada</td>\n",
       "      <td>seattl genet earn drug approv canada</td>\n",
       "      <td>seattle genetics earns drug approval in canada</td>\n",
       "      <td>seattle genetics earns drug approval canada</td>\n",
       "      <td>seattle genetics earns drug approval in canada</td>\n",
       "      <td>seattle genetics earns drug approval canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2722</th>\n",
       "      <td>Edited Transcript of LEE earnings conference call or presentation 12-Dec-19 3:00pm GMT</td>\n",
       "      <td>edited transcript of lee earnings conference call or presentation dec:p m gmt</td>\n",
       "      <td>edited transcript lee earnings conference call presentation dec:p gmt</td>\n",
       "      <td>edit transcript of lee earn confer call or present dec:p m gmt</td>\n",
       "      <td>edit transcript lee earn confer call present dec:p gmt</td>\n",
       "      <td>edited transcript of lee earnings conference call or presentation dec:p m gmt</td>\n",
       "      <td>edited transcript lee earnings conference call presentation dec:p gmt</td>\n",
       "      <td>edited transcript of lee earnings conference call or presentation dec:p m gmt</td>\n",
       "      <td>edited transcript lee earnings conference call presentation dec:p gmt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>The Best Books of 2019</td>\n",
       "      <td>the best books of</td>\n",
       "      <td>best books</td>\n",
       "      <td>the best book of</td>\n",
       "      <td>best book</td>\n",
       "      <td>the best book of</td>\n",
       "      <td>best book</td>\n",
       "      <td>the best book of</td>\n",
       "      <td>best book</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5618</th>\n",
       "      <td>$HG1:COM $FCX $TECK - Copper pops to biggest gain in six years on stimulus bet https://t.co/RMjaEyvsZ8</td>\n",
       "      <td>TICKER]: com TICKER TICKER copper pops to biggest gain in six years on stimulus bet URL</td>\n",
       "      <td>TICKER]: com TICKER TICKER copper pops biggest gain six years stimulus bet URL</td>\n",
       "      <td>ticker]: com ticker ticker copper pop to biggest gain in six year on stimulu bet url</td>\n",
       "      <td>ticker]: com ticker ticker copper pop biggest gain six year stimulu bet url</td>\n",
       "      <td>TICKER]: com TICKER TICKER copper pop to biggest gain in six year on stimulus bet URL</td>\n",
       "      <td>TICKER]: com TICKER TICKER copper pop biggest gain six year stimulus bet URL</td>\n",
       "      <td>TICKER]: com TICKER TICKER copper pop to biggest gain in six year on stimulus bet URL</td>\n",
       "      <td>TICKER]: com TICKER TICKER copper pop biggest gain six year stimulus bet URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8867</th>\n",
       "      <td>took $PLAY #4</td>\n",
       "      <td>took TICKER</td>\n",
       "      <td>took TICKER</td>\n",
       "      <td>took ticker</td>\n",
       "      <td>took ticker</td>\n",
       "      <td>took TICKER</td>\n",
       "      <td>took TICKER</td>\n",
       "      <td>took TICKER</td>\n",
       "      <td>took TICKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7650</th>\n",
       "      <td>T-Mobile US says Sievert will replace John Legere whose contract ends April 30, 2020</td>\n",
       "      <td>t-mobile us says sievert will replace john legere whose contract ends april</td>\n",
       "      <td>t-mobile us says sievert replace john legere whose contract ends april</td>\n",
       "      <td>t-mobil us say sievert will replac john leger whose contract end april</td>\n",
       "      <td>t-mobil us say sievert replac john leger whose contract end april</td>\n",
       "      <td>t-mobile u say sievert will replace john legere whose contract end april</td>\n",
       "      <td>t-mobile u say sievert replace john legere whose contract end april</td>\n",
       "      <td>t-mobile u say sievert will replace john legere whose contract end april</td>\n",
       "      <td>t-mobile u say sievert replace john legere whose contract end april</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6680 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                              text  \\\n",
       "7384                                   Today in Brexit: European Union members are ratcheting up their negotiating demands https://t.co/Qnh48BCc2l   \n",
       "8465                                                           Did Service Corporation International's (NYSE:SCI) Share Price Deserve to Gain 96%?   \n",
       "4010                                                                                                 4 Money Moves That'll Make You Richer in 2020   \n",
       "3915  #HunterBiden is just killing my life .... it's so hard to be White now in America ..... it's like being William hun… https://t.co/uHk95qx1Ig   \n",
       "1022                                                                                                Seattle Genetics earns drug approval in Canada   \n",
       "...                                                                                                                                            ...   \n",
       "2722                                                        Edited Transcript of LEE earnings conference call or presentation 12-Dec-19 3:00pm GMT   \n",
       "5170                                                                                                                        The Best Books of 2019   \n",
       "5618                                        $HG1:COM $FCX $TECK - Copper pops to biggest gain in six years on stimulus bet https://t.co/RMjaEyvsZ8   \n",
       "8867                                                                                                                                 took $PLAY #4   \n",
       "7650                                                          T-Mobile US says Sievert will replace John Legere whose contract ends April 30, 2020   \n",
       "\n",
       "                                                                                  text_no_lemma_no_stem_with_stopwords  \\\n",
       "7384                            today in brexit european union members are ratcheting up their negotiating demands URL   \n",
       "8465                                      did service corporation international's nyse sci share price deserve to gain   \n",
       "4010                                                                          money moves that will make you richer in   \n",
       "3915  #hunterbiden is just killing my life it is so hard to be white now in america it is like being william hun … URL   \n",
       "1022                                                                    seattle genetics earns drug approval in canada   \n",
       "...                                                                                                                ...   \n",
       "2722                                     edited transcript of lee earnings conference call or presentation dec:p m gmt   \n",
       "5170                                                                                                 the best books of   \n",
       "5618                           TICKER]: com TICKER TICKER copper pops to biggest gain in six years on stimulus bet URL   \n",
       "8867                                                                                                       took TICKER   \n",
       "7650                                       t-mobile us says sievert will replace john legere whose contract ends april   \n",
       "\n",
       "                                                  text_no_lemma_no_stem_no_stopwords  \\\n",
       "7384          today brexit european union members ratcheting negotiating demands URL   \n",
       "8465           service corporation international's nyse sci share price deserve gain   \n",
       "4010                                                         money moves make richer   \n",
       "3915             #hunterbiden killing life hard white america like william hun … URL   \n",
       "1022                                     seattle genetics earns drug approval canada   \n",
       "...                                                                              ...   \n",
       "2722           edited transcript lee earnings conference call presentation dec:p gmt   \n",
       "5170                                                                      best books   \n",
       "5618  TICKER]: com TICKER TICKER copper pops biggest gain six years stimulus bet URL   \n",
       "8867                                                                     took TICKER   \n",
       "7650          t-mobile us says sievert replace john legere whose contract ends april   \n",
       "\n",
       "                                                                               text_no_lemma_stem_with_stopwords  \\\n",
       "7384                                today in brexit european union member are ratchet up their negoti demand url   \n",
       "8465                                        did servic corpor international' nyse sci share price deserv to gain   \n",
       "4010                                                                     money move that will make you richer in   \n",
       "3915  #hunterbiden is just kill my life it is so hard to be white now in america it is like be william hun … url   \n",
       "1022                                                                     seattl genet earn drug approv in canada   \n",
       "...                                                                                                          ...   \n",
       "2722                                              edit transcript of lee earn confer call or present dec:p m gmt   \n",
       "5170                                                                                            the best book of   \n",
       "5618                        ticker]: com ticker ticker copper pop to biggest gain in six year on stimulu bet url   \n",
       "8867                                                                                                 took ticker   \n",
       "7650                                      t-mobil us say sievert will replac john leger whose contract end april   \n",
       "\n",
       "                                                  text_no_lemma_stem_no_stopwords  \\\n",
       "7384                 today brexit european union member ratchet negoti demand url   \n",
       "8465                servic corpor international' nyse sci share price deserv gain   \n",
       "4010                                                       money move make richer   \n",
       "3915             #hunterbiden kill life hard white america like william hun … url   \n",
       "1022                                         seattl genet earn drug approv canada   \n",
       "...                                                                           ...   \n",
       "2722                       edit transcript lee earn confer call present dec:p gmt   \n",
       "5170                                                                    best book   \n",
       "5618  ticker]: com ticker ticker copper pop biggest gain six year stimulu bet url   \n",
       "8867                                                                  took ticker   \n",
       "7650            t-mobil us say sievert replac john leger whose contract end april   \n",
       "\n",
       "                                                                                     text_lemma_no_stem_with_stopwords  \\\n",
       "7384                              today in brexit european union member are ratcheting up their negotiating demand URL   \n",
       "8465                                      did service corporation international's nyse sci share price deserve to gain   \n",
       "4010                                                                           money move that will make you richer in   \n",
       "3915  #hunterbiden is just killing my life it is so hard to be white now in america it is like being william hun … URL   \n",
       "1022                                                                    seattle genetics earns drug approval in canada   \n",
       "...                                                                                                                ...   \n",
       "2722                                     edited transcript of lee earnings conference call or presentation dec:p m gmt   \n",
       "5170                                                                                                  the best book of   \n",
       "5618                             TICKER]: com TICKER TICKER copper pop to biggest gain in six year on stimulus bet URL   \n",
       "8867                                                                                                       took TICKER   \n",
       "7650                                          t-mobile u say sievert will replace john legere whose contract end april   \n",
       "\n",
       "                                                   text_lemma_no_stem_no_stopwords  \\\n",
       "7384          today brexit european union member ratcheting negotiating demand URL   \n",
       "8465         service corporation international's nyse sci share price deserve gain   \n",
       "4010                                                        money move make richer   \n",
       "3915           #hunterbiden killing life hard white america like william hun … URL   \n",
       "1022                                   seattle genetics earns drug approval canada   \n",
       "...                                                                            ...   \n",
       "2722         edited transcript lee earnings conference call presentation dec:p gmt   \n",
       "5170                                                                     best book   \n",
       "5618  TICKER]: com TICKER TICKER copper pop biggest gain six year stimulus bet URL   \n",
       "8867                                                                   took TICKER   \n",
       "7650           t-mobile u say sievert replace john legere whose contract end april   \n",
       "\n",
       "                                                                                        text_lemma_stem_with_stopwords  \\\n",
       "7384                              today in brexit european union member are ratcheting up their negotiating demand URL   \n",
       "8465                                      did service corporation international's nyse sci share price deserve to gain   \n",
       "4010                                                                           money move that will make you richer in   \n",
       "3915  #hunterbiden is just killing my life it is so hard to be white now in america it is like being william hun … URL   \n",
       "1022                                                                    seattle genetics earns drug approval in canada   \n",
       "...                                                                                                                ...   \n",
       "2722                                     edited transcript of lee earnings conference call or presentation dec:p m gmt   \n",
       "5170                                                                                                  the best book of   \n",
       "5618                             TICKER]: com TICKER TICKER copper pop to biggest gain in six year on stimulus bet URL   \n",
       "8867                                                                                                       took TICKER   \n",
       "7650                                          t-mobile u say sievert will replace john legere whose contract end april   \n",
       "\n",
       "                                                      text_lemma_stem_no_stopwords  \n",
       "7384          today brexit european union member ratcheting negotiating demand URL  \n",
       "8465         service corporation international's nyse sci share price deserve gain  \n",
       "4010                                                        money move make richer  \n",
       "3915           #hunterbiden killing life hard white america like william hun … URL  \n",
       "1022                                   seattle genetics earns drug approval canada  \n",
       "...                                                                            ...  \n",
       "2722         edited transcript lee earnings conference call presentation dec:p gmt  \n",
       "5170                                                                     best book  \n",
       "5618  TICKER]: com TICKER TICKER copper pop biggest gain six year stimulus bet URL  \n",
       "8867                                                                   took TICKER  \n",
       "7650           t-mobile u say sievert replace john legere whose contract end april  \n",
       "\n",
       "[6680 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbbada9",
   "metadata": {},
   "source": [
    "### 3.1 - Statistical Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c021149",
   "metadata": {},
   "source": [
    "### 3.1.1 - Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7265ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = [\n",
    "    (\"text_no_lemma_no_stem_with_stopwords\", train_df[\"text_no_lemma_no_stem_with_stopwords\"]),\n",
    "    (\"text_lemma_no_stem_with_stopwords\", train_df[\"text_lemma_no_stem_with_stopwords\"]),\n",
    "    (\"text_no_lemma_stem_with_stopwords\", train_df[\"text_no_lemma_stem_with_stopwords\"]),\n",
    "    (\"text_no_lemma_no_stem_no_stopwords\", train_df[\"text_no_lemma_no_stem_no_stopwords\"]),\n",
    "    (\"text_lemma_no_stem_no_stopwords\", train_df[\"text_lemma_no_stem_no_stopwords\"]),\n",
    "    (\"text_no_lemma_stem_no_stopwords\", train_df[\"text_no_lemma_stem_no_stopwords\"]),\n",
    "    (\"text_lemma_stem_with_stopwords\", train_df[\"text_lemma_stem_with_stopwords\"]),\n",
    "    (\"text_lemma_stem_no_stopwords\", train_df[\"text_lemma_stem_no_stopwords\"]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c7ee42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting BOW vectorizer for text_no_lemma_no_stem_with_stopwords...\n",
      "Fitting BOW vectorizer for text_lemma_no_stem_with_stopwords...\n",
      "Fitting BOW vectorizer for text_no_lemma_stem_with_stopwords...\n",
      "Fitting BOW vectorizer for text_no_lemma_no_stem_no_stopwords...\n",
      "Fitting BOW vectorizer for text_lemma_no_stem_no_stopwords...\n",
      "Fitting BOW vectorizer for text_no_lemma_stem_no_stopwords...\n",
      "Fitting BOW vectorizer for text_lemma_stem_with_stopwords...\n",
      "Fitting BOW vectorizer for text_lemma_stem_no_stopwords...\n"
     ]
    }
   ],
   "source": [
    "bow_vectors = {}\n",
    "\n",
    "for column_name, _ in combinations:\n",
    "    print(f\"Fitting BOW vectorizer for {column_name}...\")\n",
    "\n",
    "    bow_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=15_000)\n",
    "    bow_vectorizer.fit(train_df[column_name]) \n",
    "\n",
    "    bow_vectors[column_name] = {\n",
    "        \"train\": bow_vectorizer.transform(train_df[column_name]),\n",
    "        \"val\": bow_vectorizer.transform(val_df[column_name]),\n",
    "        \"test\": bow_vectorizer.transform(test_df[column_name]),\n",
    "    }\n",
    "\n",
    "bow_labels = {\n",
    "\"train\": y_train,\n",
    "\"val\": y_val,\n",
    "\"test\": y_test\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6f5588",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"SVC\": SVC(class_weight='balanced', random_state=42),\n",
    "    \"XGB\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=300, class_weight='balanced', random_state=42),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77325181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating text variant: text_no_lemma_no_stem_with_stopwords\n",
      "Training SVC on original data...\n",
      "Training XGB on original data...\n",
      "Training LogisticRegression on original data...\n",
      "Training KNN on original data...\n",
      "\n",
      "Evaluating text variant: text_lemma_no_stem_with_stopwords\n",
      "Training SVC on original data...\n",
      "Training XGB on original data...\n",
      "Training LogisticRegression on original data...\n",
      "Training KNN on original data...\n",
      "\n",
      "Evaluating text variant: text_no_lemma_stem_with_stopwords\n",
      "Training SVC on original data...\n",
      "Training XGB on original data...\n",
      "Training LogisticRegression on original data...\n",
      "Training KNN on original data...\n",
      "\n",
      "Evaluating text variant: text_no_lemma_no_stem_no_stopwords\n",
      "Training SVC on original data...\n",
      "Training XGB on original data...\n",
      "Training LogisticRegression on original data...\n",
      "Training KNN on original data...\n",
      "\n",
      "Evaluating text variant: text_lemma_no_stem_no_stopwords\n",
      "Training SVC on original data...\n",
      "Training XGB on original data...\n",
      "Training LogisticRegression on original data...\n",
      "Training KNN on original data...\n",
      "\n",
      "Evaluating text variant: text_no_lemma_stem_no_stopwords\n",
      "Training SVC on original data...\n",
      "Training XGB on original data...\n",
      "Training LogisticRegression on original data...\n",
      "Training KNN on original data...\n",
      "\n",
      "Evaluating text variant: text_lemma_stem_with_stopwords\n",
      "Training SVC on original data...\n",
      "Training XGB on original data...\n",
      "Training LogisticRegression on original data...\n",
      "Training KNN on original data...\n",
      "\n",
      "Evaluating text variant: text_lemma_stem_no_stopwords\n",
      "Training SVC on original data...\n",
      "Training XGB on original data...\n",
      "Training LogisticRegression on original data...\n",
      "Training KNN on original data...\n"
     ]
    }
   ],
   "source": [
    "results_bow = []\n",
    "\n",
    "for col in combinations:\n",
    "    column_name = col[0]\n",
    "    print(f\"\\nEvaluating text variant: {column_name}\")\n",
    "\n",
    "    X_train = bow_vectors[column_name][\"train\"]\n",
    "    X_val   = bow_vectors[column_name][\"val\"]\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name} on original data...\")\n",
    "\n",
    "        model_instance = clone(model)\n",
    "        model_instance.fit(X_train, y_train)\n",
    "        y_pred = model_instance.predict(X_val)\n",
    "\n",
    "        report = classification_report(y_val, y_pred, output_dict=True)\n",
    "\n",
    "        results_bow.append({\n",
    "            \"variant\": column_name,\n",
    "            \"model\": model_name,\n",
    "            \"accuracy\": report[\"accuracy\"],\n",
    "            \"macro_f1\": report[\"macro avg\"][\"f1-score\"],\n",
    "            \"macro_precision\": report[\"macro avg\"][\"precision\"],\n",
    "            \"macro_recall\": report[\"macro avg\"][\"recall\"],\n",
    "            \"weighted_f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            \"weighted_precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"weighted_recall\": report[\"weighted avg\"][\"recall\"],\n",
    "        })\n",
    "\n",
    "traditional_ml_bow = pd.DataFrame(results_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f52a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>weighted_precision</th>\n",
       "      <th>weighted_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.804333</td>\n",
       "      <td>0.729893</td>\n",
       "      <td>0.733927</td>\n",
       "      <td>0.726412</td>\n",
       "      <td>0.803083</td>\n",
       "      <td>0.802119</td>\n",
       "      <td>0.804333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.804333</td>\n",
       "      <td>0.728305</td>\n",
       "      <td>0.734357</td>\n",
       "      <td>0.722844</td>\n",
       "      <td>0.802591</td>\n",
       "      <td>0.801212</td>\n",
       "      <td>0.804333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.801537</td>\n",
       "      <td>0.724020</td>\n",
       "      <td>0.730592</td>\n",
       "      <td>0.718254</td>\n",
       "      <td>0.799584</td>\n",
       "      <td>0.798097</td>\n",
       "      <td>0.801537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.801537</td>\n",
       "      <td>0.724020</td>\n",
       "      <td>0.730592</td>\n",
       "      <td>0.718254</td>\n",
       "      <td>0.799584</td>\n",
       "      <td>0.798097</td>\n",
       "      <td>0.801537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.796646</td>\n",
       "      <td>0.722437</td>\n",
       "      <td>0.723031</td>\n",
       "      <td>0.721913</td>\n",
       "      <td>0.796537</td>\n",
       "      <td>0.796470</td>\n",
       "      <td>0.796646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.789658</td>\n",
       "      <td>0.711204</td>\n",
       "      <td>0.714500</td>\n",
       "      <td>0.708107</td>\n",
       "      <td>0.788621</td>\n",
       "      <td>0.787707</td>\n",
       "      <td>0.789658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.789658</td>\n",
       "      <td>0.711204</td>\n",
       "      <td>0.714500</td>\n",
       "      <td>0.708107</td>\n",
       "      <td>0.788621</td>\n",
       "      <td>0.787707</td>\n",
       "      <td>0.789658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.795248</td>\n",
       "      <td>0.708807</td>\n",
       "      <td>0.733814</td>\n",
       "      <td>0.690534</td>\n",
       "      <td>0.788879</td>\n",
       "      <td>0.786820</td>\n",
       "      <td>0.795248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.795248</td>\n",
       "      <td>0.708807</td>\n",
       "      <td>0.733814</td>\n",
       "      <td>0.690534</td>\n",
       "      <td>0.788879</td>\n",
       "      <td>0.786820</td>\n",
       "      <td>0.795248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.786862</td>\n",
       "      <td>0.707967</td>\n",
       "      <td>0.712828</td>\n",
       "      <td>0.703438</td>\n",
       "      <td>0.785430</td>\n",
       "      <td>0.784258</td>\n",
       "      <td>0.786862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.803634</td>\n",
       "      <td>0.706439</td>\n",
       "      <td>0.793473</td>\n",
       "      <td>0.662359</td>\n",
       "      <td>0.788428</td>\n",
       "      <td>0.801250</td>\n",
       "      <td>0.803634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.793850</td>\n",
       "      <td>0.706168</td>\n",
       "      <td>0.731985</td>\n",
       "      <td>0.686963</td>\n",
       "      <td>0.787265</td>\n",
       "      <td>0.785231</td>\n",
       "      <td>0.793850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.791055</td>\n",
       "      <td>0.703110</td>\n",
       "      <td>0.719451</td>\n",
       "      <td>0.691943</td>\n",
       "      <td>0.786712</td>\n",
       "      <td>0.785097</td>\n",
       "      <td>0.791055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.791055</td>\n",
       "      <td>0.703110</td>\n",
       "      <td>0.719451</td>\n",
       "      <td>0.691943</td>\n",
       "      <td>0.786712</td>\n",
       "      <td>0.785097</td>\n",
       "      <td>0.791055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.788260</td>\n",
       "      <td>0.697505</td>\n",
       "      <td>0.710938</td>\n",
       "      <td>0.687732</td>\n",
       "      <td>0.784155</td>\n",
       "      <td>0.782031</td>\n",
       "      <td>0.788260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.788260</td>\n",
       "      <td>0.696358</td>\n",
       "      <td>0.722241</td>\n",
       "      <td>0.677824</td>\n",
       "      <td>0.781179</td>\n",
       "      <td>0.778831</td>\n",
       "      <td>0.788260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.783368</td>\n",
       "      <td>0.693948</td>\n",
       "      <td>0.713456</td>\n",
       "      <td>0.681265</td>\n",
       "      <td>0.778399</td>\n",
       "      <td>0.777025</td>\n",
       "      <td>0.783368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.793152</td>\n",
       "      <td>0.684711</td>\n",
       "      <td>0.778643</td>\n",
       "      <td>0.641550</td>\n",
       "      <td>0.775179</td>\n",
       "      <td>0.788909</td>\n",
       "      <td>0.793152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.793152</td>\n",
       "      <td>0.684711</td>\n",
       "      <td>0.778643</td>\n",
       "      <td>0.641550</td>\n",
       "      <td>0.775179</td>\n",
       "      <td>0.788909</td>\n",
       "      <td>0.793152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.793152</td>\n",
       "      <td>0.683973</td>\n",
       "      <td>0.786771</td>\n",
       "      <td>0.637981</td>\n",
       "      <td>0.774109</td>\n",
       "      <td>0.791356</td>\n",
       "      <td>0.793152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.791754</td>\n",
       "      <td>0.680158</td>\n",
       "      <td>0.781548</td>\n",
       "      <td>0.635747</td>\n",
       "      <td>0.772481</td>\n",
       "      <td>0.788734</td>\n",
       "      <td>0.791754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.777079</td>\n",
       "      <td>0.654559</td>\n",
       "      <td>0.765019</td>\n",
       "      <td>0.612439</td>\n",
       "      <td>0.754866</td>\n",
       "      <td>0.773446</td>\n",
       "      <td>0.777079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.777079</td>\n",
       "      <td>0.654559</td>\n",
       "      <td>0.765019</td>\n",
       "      <td>0.612439</td>\n",
       "      <td>0.754866</td>\n",
       "      <td>0.773446</td>\n",
       "      <td>0.777079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.769392</td>\n",
       "      <td>0.643963</td>\n",
       "      <td>0.756890</td>\n",
       "      <td>0.601302</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.765352</td>\n",
       "      <td>0.769392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.691824</td>\n",
       "      <td>0.425190</td>\n",
       "      <td>0.774690</td>\n",
       "      <td>0.422158</td>\n",
       "      <td>0.608885</td>\n",
       "      <td>0.732466</td>\n",
       "      <td>0.691824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.691824</td>\n",
       "      <td>0.424945</td>\n",
       "      <td>0.729050</td>\n",
       "      <td>0.422537</td>\n",
       "      <td>0.609742</td>\n",
       "      <td>0.710497</td>\n",
       "      <td>0.691824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.689727</td>\n",
       "      <td>0.423141</td>\n",
       "      <td>0.701842</td>\n",
       "      <td>0.421497</td>\n",
       "      <td>0.608681</td>\n",
       "      <td>0.697303</td>\n",
       "      <td>0.689727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.689727</td>\n",
       "      <td>0.423141</td>\n",
       "      <td>0.701842</td>\n",
       "      <td>0.421497</td>\n",
       "      <td>0.608681</td>\n",
       "      <td>0.697303</td>\n",
       "      <td>0.689727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.691125</td>\n",
       "      <td>0.421091</td>\n",
       "      <td>0.770968</td>\n",
       "      <td>0.419825</td>\n",
       "      <td>0.606864</td>\n",
       "      <td>0.730511</td>\n",
       "      <td>0.691125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.691125</td>\n",
       "      <td>0.421091</td>\n",
       "      <td>0.770968</td>\n",
       "      <td>0.419825</td>\n",
       "      <td>0.606864</td>\n",
       "      <td>0.730511</td>\n",
       "      <td>0.691125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.686233</td>\n",
       "      <td>0.414729</td>\n",
       "      <td>0.679428</td>\n",
       "      <td>0.416129</td>\n",
       "      <td>0.603450</td>\n",
       "      <td>0.684915</td>\n",
       "      <td>0.686233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.687631</td>\n",
       "      <td>0.413012</td>\n",
       "      <td>0.768899</td>\n",
       "      <td>0.414456</td>\n",
       "      <td>0.601183</td>\n",
       "      <td>0.728253</td>\n",
       "      <td>0.687631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 variant               model  accuracy  \\\n",
       "10     text_no_lemma_stem_with_stopwords  LogisticRegression  0.804333   \n",
       "2   text_no_lemma_no_stem_with_stopwords  LogisticRegression  0.804333   \n",
       "6      text_lemma_no_stem_with_stopwords  LogisticRegression  0.801537   \n",
       "26        text_lemma_stem_with_stopwords  LogisticRegression  0.801537   \n",
       "22       text_no_lemma_stem_no_stopwords  LogisticRegression  0.796646   \n",
       "18       text_lemma_no_stem_no_stopwords  LogisticRegression  0.789658   \n",
       "30          text_lemma_stem_no_stopwords  LogisticRegression  0.789658   \n",
       "4      text_lemma_no_stem_with_stopwords                 SVC  0.795248   \n",
       "24        text_lemma_stem_with_stopwords                 SVC  0.795248   \n",
       "14    text_no_lemma_no_stem_no_stopwords  LogisticRegression  0.786862   \n",
       "9      text_no_lemma_stem_with_stopwords                 XGB  0.803634   \n",
       "8      text_no_lemma_stem_with_stopwords                 SVC  0.793850   \n",
       "16       text_lemma_no_stem_no_stopwords                 SVC  0.791055   \n",
       "28          text_lemma_stem_no_stopwords                 SVC  0.791055   \n",
       "20       text_no_lemma_stem_no_stopwords                 SVC  0.788260   \n",
       "0   text_no_lemma_no_stem_with_stopwords                 SVC  0.788260   \n",
       "12    text_no_lemma_no_stem_no_stopwords                 SVC  0.783368   \n",
       "25        text_lemma_stem_with_stopwords                 XGB  0.793152   \n",
       "5      text_lemma_no_stem_with_stopwords                 XGB  0.793152   \n",
       "1   text_no_lemma_no_stem_with_stopwords                 XGB  0.793152   \n",
       "21       text_no_lemma_stem_no_stopwords                 XGB  0.791754   \n",
       "17       text_lemma_no_stem_no_stopwords                 XGB  0.777079   \n",
       "29          text_lemma_stem_no_stopwords                 XGB  0.777079   \n",
       "13    text_no_lemma_no_stem_no_stopwords                 XGB  0.769392   \n",
       "11     text_no_lemma_stem_with_stopwords                 KNN  0.691824   \n",
       "23       text_no_lemma_stem_no_stopwords                 KNN  0.691824   \n",
       "19       text_lemma_no_stem_no_stopwords                 KNN  0.689727   \n",
       "31          text_lemma_stem_no_stopwords                 KNN  0.689727   \n",
       "7      text_lemma_no_stem_with_stopwords                 KNN  0.691125   \n",
       "27        text_lemma_stem_with_stopwords                 KNN  0.691125   \n",
       "15    text_no_lemma_no_stem_no_stopwords                 KNN  0.686233   \n",
       "3   text_no_lemma_no_stem_with_stopwords                 KNN  0.687631   \n",
       "\n",
       "    macro_f1  macro_precision  macro_recall  weighted_f1  weighted_precision  \\\n",
       "10  0.729893         0.733927      0.726412     0.803083            0.802119   \n",
       "2   0.728305         0.734357      0.722844     0.802591            0.801212   \n",
       "6   0.724020         0.730592      0.718254     0.799584            0.798097   \n",
       "26  0.724020         0.730592      0.718254     0.799584            0.798097   \n",
       "22  0.722437         0.723031      0.721913     0.796537            0.796470   \n",
       "18  0.711204         0.714500      0.708107     0.788621            0.787707   \n",
       "30  0.711204         0.714500      0.708107     0.788621            0.787707   \n",
       "4   0.708807         0.733814      0.690534     0.788879            0.786820   \n",
       "24  0.708807         0.733814      0.690534     0.788879            0.786820   \n",
       "14  0.707967         0.712828      0.703438     0.785430            0.784258   \n",
       "9   0.706439         0.793473      0.662359     0.788428            0.801250   \n",
       "8   0.706168         0.731985      0.686963     0.787265            0.785231   \n",
       "16  0.703110         0.719451      0.691943     0.786712            0.785097   \n",
       "28  0.703110         0.719451      0.691943     0.786712            0.785097   \n",
       "20  0.697505         0.710938      0.687732     0.784155            0.782031   \n",
       "0   0.696358         0.722241      0.677824     0.781179            0.778831   \n",
       "12  0.693948         0.713456      0.681265     0.778399            0.777025   \n",
       "25  0.684711         0.778643      0.641550     0.775179            0.788909   \n",
       "5   0.684711         0.778643      0.641550     0.775179            0.788909   \n",
       "1   0.683973         0.786771      0.637981     0.774109            0.791356   \n",
       "21  0.680158         0.781548      0.635747     0.772481            0.788734   \n",
       "17  0.654559         0.765019      0.612439     0.754866            0.773446   \n",
       "29  0.654559         0.765019      0.612439     0.754866            0.773446   \n",
       "13  0.643963         0.756890      0.601302     0.746000            0.765352   \n",
       "11  0.425190         0.774690      0.422158     0.608885            0.732466   \n",
       "23  0.424945         0.729050      0.422537     0.609742            0.710497   \n",
       "19  0.423141         0.701842      0.421497     0.608681            0.697303   \n",
       "31  0.423141         0.701842      0.421497     0.608681            0.697303   \n",
       "7   0.421091         0.770968      0.419825     0.606864            0.730511   \n",
       "27  0.421091         0.770968      0.419825     0.606864            0.730511   \n",
       "15  0.414729         0.679428      0.416129     0.603450            0.684915   \n",
       "3   0.413012         0.768899      0.414456     0.601183            0.728253   \n",
       "\n",
       "    weighted_recall  \n",
       "10         0.804333  \n",
       "2          0.804333  \n",
       "6          0.801537  \n",
       "26         0.801537  \n",
       "22         0.796646  \n",
       "18         0.789658  \n",
       "30         0.789658  \n",
       "4          0.795248  \n",
       "24         0.795248  \n",
       "14         0.786862  \n",
       "9          0.803634  \n",
       "8          0.793850  \n",
       "16         0.791055  \n",
       "28         0.791055  \n",
       "20         0.788260  \n",
       "0          0.788260  \n",
       "12         0.783368  \n",
       "25         0.793152  \n",
       "5          0.793152  \n",
       "1          0.793152  \n",
       "21         0.791754  \n",
       "17         0.777079  \n",
       "29         0.777079  \n",
       "13         0.769392  \n",
       "11         0.691824  \n",
       "23         0.691824  \n",
       "19         0.689727  \n",
       "31         0.689727  \n",
       "7          0.691125  \n",
       "27         0.691125  \n",
       "15         0.686233  \n",
       "3          0.687631  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sort and show as a full DataFrame\n",
    "sorted_results_bow = traditional_ml_bow.sort_values(by=\"macro_f1\", ascending=False)\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# Now display the full DataFrame\n",
    "display(sorted_results_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18cd6590",
   "metadata": {},
   "source": [
    "2) TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf5521ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting TF-IDF vectorizer for text_no_lemma_no_stem_with_stopwords...\n",
      "Fitting TF-IDF vectorizer for text_lemma_no_stem_with_stopwords...\n",
      "Fitting TF-IDF vectorizer for text_no_lemma_stem_with_stopwords...\n",
      "Fitting TF-IDF vectorizer for text_no_lemma_no_stem_no_stopwords...\n",
      "Fitting TF-IDF vectorizer for text_lemma_no_stem_no_stopwords...\n",
      "Fitting TF-IDF vectorizer for text_no_lemma_stem_no_stopwords...\n",
      "Fitting TF-IDF vectorizer for text_lemma_stem_with_stopwords...\n",
      "Fitting TF-IDF vectorizer for text_lemma_stem_no_stopwords...\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=15000)\n",
    "tfidf_vectors = {}\n",
    "\n",
    "for column_name, _ in combinations:\n",
    "    print(f\"Fitting TF-IDF vectorizer for {column_name}...\")\n",
    "    tfidf_vectorizer.fit(train_df[column_name])\n",
    "\n",
    "    tfidf_vectors[column_name] = {\n",
    "        \"train\": tfidf_vectorizer.transform(train_df[column_name]),\n",
    "        \"val\": tfidf_vectorizer.transform(val_df[column_name]),\n",
    "        \"test\": tfidf_vectorizer.transform(test_df[column_name])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d37d55ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating TF-IDF for: text_no_lemma_no_stem_with_stopwords\n",
      "Training SVC...\n",
      "Training XGB...\n",
      "Training LogisticRegression...\n",
      "Training KNN...\n",
      "\n",
      "Evaluating TF-IDF for: text_lemma_no_stem_with_stopwords\n",
      "Training SVC...\n",
      "Training XGB...\n",
      "Training LogisticRegression...\n",
      "Training KNN...\n",
      "\n",
      "Evaluating TF-IDF for: text_no_lemma_stem_with_stopwords\n",
      "Training SVC...\n",
      "Training XGB...\n",
      "Training LogisticRegression...\n",
      "Training KNN...\n",
      "\n",
      "Evaluating TF-IDF for: text_no_lemma_no_stem_no_stopwords\n",
      "Training SVC...\n",
      "Training XGB...\n",
      "Training LogisticRegression...\n",
      "Training KNN...\n",
      "\n",
      "Evaluating TF-IDF for: text_lemma_no_stem_no_stopwords\n",
      "Training SVC...\n",
      "Training XGB...\n",
      "Training LogisticRegression...\n",
      "Training KNN...\n",
      "\n",
      "Evaluating TF-IDF for: text_no_lemma_stem_no_stopwords\n",
      "Training SVC...\n",
      "Training XGB...\n",
      "Training LogisticRegression...\n",
      "Training KNN...\n",
      "\n",
      "Evaluating TF-IDF for: text_lemma_stem_with_stopwords\n",
      "Training SVC...\n",
      "Training XGB...\n",
      "Training LogisticRegression...\n",
      "Training KNN...\n",
      "\n",
      "Evaluating TF-IDF for: text_lemma_stem_no_stopwords\n",
      "Training SVC...\n",
      "Training XGB...\n",
      "Training LogisticRegression...\n",
      "Training KNN...\n"
     ]
    }
   ],
   "source": [
    "# Model training and evaluation\n",
    "results_tfidf = []\n",
    "\n",
    "for column_name, _ in combinations:\n",
    "    print(f\"\\nEvaluating TF-IDF for: {column_name}\")\n",
    "    X_train = tfidf_vectors[column_name][\"train\"]\n",
    "    X_val = tfidf_vectors[column_name][\"val\"]\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training {model_name}...\")\n",
    "        model_instance = clone(model)\n",
    "        model_instance.fit(X_train, y_train)\n",
    "        y_pred = model_instance.predict(X_val)\n",
    "\n",
    "        report = classification_report(y_val, y_pred, output_dict=True)\n",
    "\n",
    "        results_tfidf.append({\n",
    "            \"variant\": column_name,\n",
    "            \"model\": model_name,\n",
    "            \"accuracy\": report[\"accuracy\"],\n",
    "            \"macro_f1\": report[\"macro avg\"][\"f1-score\"],\n",
    "            \"macro_precision\": report[\"macro avg\"][\"precision\"],\n",
    "            \"macro_recall\": report[\"macro avg\"][\"recall\"],\n",
    "            \"weighted_f1\": report[\"weighted avg\"][\"f1-score\"],\n",
    "            \"weighted_precision\": report[\"weighted avg\"][\"precision\"],\n",
    "            \"weighted_recall\": report[\"weighted avg\"][\"recall\"],\n",
    "        })\n",
    "\n",
    "# Results DataFrame\n",
    "traditional_ml_tfidf = pd.DataFrame(results_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e51379ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variant</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro_f1</th>\n",
       "      <th>macro_precision</th>\n",
       "      <th>macro_recall</th>\n",
       "      <th>weighted_f1</th>\n",
       "      <th>weighted_precision</th>\n",
       "      <th>weighted_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.815514</td>\n",
       "      <td>0.731998</td>\n",
       "      <td>0.778293</td>\n",
       "      <td>0.703326</td>\n",
       "      <td>0.806934</td>\n",
       "      <td>0.808865</td>\n",
       "      <td>0.815514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.794549</td>\n",
       "      <td>0.726905</td>\n",
       "      <td>0.717443</td>\n",
       "      <td>0.737918</td>\n",
       "      <td>0.797449</td>\n",
       "      <td>0.801580</td>\n",
       "      <td>0.794549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.791055</td>\n",
       "      <td>0.722747</td>\n",
       "      <td>0.714299</td>\n",
       "      <td>0.732550</td>\n",
       "      <td>0.793628</td>\n",
       "      <td>0.797305</td>\n",
       "      <td>0.791055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.787561</td>\n",
       "      <td>0.720361</td>\n",
       "      <td>0.710561</td>\n",
       "      <td>0.731926</td>\n",
       "      <td>0.790626</td>\n",
       "      <td>0.795116</td>\n",
       "      <td>0.787561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.787561</td>\n",
       "      <td>0.720361</td>\n",
       "      <td>0.710561</td>\n",
       "      <td>0.731926</td>\n",
       "      <td>0.790626</td>\n",
       "      <td>0.795116</td>\n",
       "      <td>0.787561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.809224</td>\n",
       "      <td>0.718657</td>\n",
       "      <td>0.778195</td>\n",
       "      <td>0.686690</td>\n",
       "      <td>0.798557</td>\n",
       "      <td>0.803592</td>\n",
       "      <td>0.809224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.806429</td>\n",
       "      <td>0.716831</td>\n",
       "      <td>0.769087</td>\n",
       "      <td>0.687642</td>\n",
       "      <td>0.796493</td>\n",
       "      <td>0.799723</td>\n",
       "      <td>0.806429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.806429</td>\n",
       "      <td>0.716831</td>\n",
       "      <td>0.769087</td>\n",
       "      <td>0.687642</td>\n",
       "      <td>0.796493</td>\n",
       "      <td>0.799723</td>\n",
       "      <td>0.806429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.781971</td>\n",
       "      <td>0.710177</td>\n",
       "      <td>0.700950</td>\n",
       "      <td>0.721531</td>\n",
       "      <td>0.785385</td>\n",
       "      <td>0.790555</td>\n",
       "      <td>0.781971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.779175</td>\n",
       "      <td>0.703476</td>\n",
       "      <td>0.695583</td>\n",
       "      <td>0.712576</td>\n",
       "      <td>0.782105</td>\n",
       "      <td>0.786011</td>\n",
       "      <td>0.779175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.779175</td>\n",
       "      <td>0.703476</td>\n",
       "      <td>0.695583</td>\n",
       "      <td>0.712576</td>\n",
       "      <td>0.782105</td>\n",
       "      <td>0.786011</td>\n",
       "      <td>0.779175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.798742</td>\n",
       "      <td>0.702859</td>\n",
       "      <td>0.745125</td>\n",
       "      <td>0.678140</td>\n",
       "      <td>0.789097</td>\n",
       "      <td>0.789579</td>\n",
       "      <td>0.798742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.797345</td>\n",
       "      <td>0.702200</td>\n",
       "      <td>0.745910</td>\n",
       "      <td>0.676204</td>\n",
       "      <td>0.787496</td>\n",
       "      <td>0.788126</td>\n",
       "      <td>0.797345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.797345</td>\n",
       "      <td>0.702200</td>\n",
       "      <td>0.745910</td>\n",
       "      <td>0.676204</td>\n",
       "      <td>0.787496</td>\n",
       "      <td>0.788126</td>\n",
       "      <td>0.797345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.772187</td>\n",
       "      <td>0.693545</td>\n",
       "      <td>0.686310</td>\n",
       "      <td>0.701839</td>\n",
       "      <td>0.775084</td>\n",
       "      <td>0.778840</td>\n",
       "      <td>0.772187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.791754</td>\n",
       "      <td>0.690145</td>\n",
       "      <td>0.745385</td>\n",
       "      <td>0.663118</td>\n",
       "      <td>0.780304</td>\n",
       "      <td>0.784250</td>\n",
       "      <td>0.791754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.791055</td>\n",
       "      <td>0.687158</td>\n",
       "      <td>0.767445</td>\n",
       "      <td>0.646310</td>\n",
       "      <td>0.774816</td>\n",
       "      <td>0.785059</td>\n",
       "      <td>0.791055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.788260</td>\n",
       "      <td>0.678494</td>\n",
       "      <td>0.770458</td>\n",
       "      <td>0.635840</td>\n",
       "      <td>0.769970</td>\n",
       "      <td>0.783068</td>\n",
       "      <td>0.788260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.788260</td>\n",
       "      <td>0.678494</td>\n",
       "      <td>0.770458</td>\n",
       "      <td>0.635840</td>\n",
       "      <td>0.769970</td>\n",
       "      <td>0.783068</td>\n",
       "      <td>0.788260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.783368</td>\n",
       "      <td>0.667776</td>\n",
       "      <td>0.763118</td>\n",
       "      <td>0.625007</td>\n",
       "      <td>0.763354</td>\n",
       "      <td>0.777473</td>\n",
       "      <td>0.783368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.775681</td>\n",
       "      <td>0.666082</td>\n",
       "      <td>0.739270</td>\n",
       "      <td>0.628942</td>\n",
       "      <td>0.758951</td>\n",
       "      <td>0.766108</td>\n",
       "      <td>0.775681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.776380</td>\n",
       "      <td>0.652027</td>\n",
       "      <td>0.740427</td>\n",
       "      <td>0.614012</td>\n",
       "      <td>0.755425</td>\n",
       "      <td>0.765978</td>\n",
       "      <td>0.776380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.776380</td>\n",
       "      <td>0.652027</td>\n",
       "      <td>0.740427</td>\n",
       "      <td>0.614012</td>\n",
       "      <td>0.755425</td>\n",
       "      <td>0.765978</td>\n",
       "      <td>0.776380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>XGB</td>\n",
       "      <td>0.763802</td>\n",
       "      <td>0.631598</td>\n",
       "      <td>0.734190</td>\n",
       "      <td>0.592920</td>\n",
       "      <td>0.739667</td>\n",
       "      <td>0.754626</td>\n",
       "      <td>0.763802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>text_lemma_no_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.686233</td>\n",
       "      <td>0.400782</td>\n",
       "      <td>0.832377</td>\n",
       "      <td>0.407397</td>\n",
       "      <td>0.594207</td>\n",
       "      <td>0.757238</td>\n",
       "      <td>0.686233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>text_lemma_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.686233</td>\n",
       "      <td>0.400782</td>\n",
       "      <td>0.832377</td>\n",
       "      <td>0.407397</td>\n",
       "      <td>0.594207</td>\n",
       "      <td>0.757238</td>\n",
       "      <td>0.686233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text_no_lemma_no_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.684137</td>\n",
       "      <td>0.399420</td>\n",
       "      <td>0.798271</td>\n",
       "      <td>0.406317</td>\n",
       "      <td>0.592871</td>\n",
       "      <td>0.739143</td>\n",
       "      <td>0.684137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>text_no_lemma_stem_with_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.684137</td>\n",
       "      <td>0.397212</td>\n",
       "      <td>0.806693</td>\n",
       "      <td>0.405141</td>\n",
       "      <td>0.591853</td>\n",
       "      <td>0.744217</td>\n",
       "      <td>0.684137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>text_no_lemma_stem_no_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.683438</td>\n",
       "      <td>0.394090</td>\n",
       "      <td>0.776564</td>\n",
       "      <td>0.403605</td>\n",
       "      <td>0.590705</td>\n",
       "      <td>0.729386</td>\n",
       "      <td>0.683438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>text_lemma_no_stem_no_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.682041</td>\n",
       "      <td>0.390326</td>\n",
       "      <td>0.745448</td>\n",
       "      <td>0.401290</td>\n",
       "      <td>0.588483</td>\n",
       "      <td>0.714901</td>\n",
       "      <td>0.682041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>text_lemma_stem_no_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.682041</td>\n",
       "      <td>0.390326</td>\n",
       "      <td>0.745448</td>\n",
       "      <td>0.401290</td>\n",
       "      <td>0.588483</td>\n",
       "      <td>0.714901</td>\n",
       "      <td>0.682041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>text_no_lemma_no_stem_no_stopwords</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.677149</td>\n",
       "      <td>0.376216</td>\n",
       "      <td>0.725946</td>\n",
       "      <td>0.392809</td>\n",
       "      <td>0.579423</td>\n",
       "      <td>0.702528</td>\n",
       "      <td>0.677149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 variant               model  accuracy  \\\n",
       "8      text_no_lemma_stem_with_stopwords                 SVC  0.815514   \n",
       "10     text_no_lemma_stem_with_stopwords  LogisticRegression  0.794549   \n",
       "2   text_no_lemma_no_stem_with_stopwords  LogisticRegression  0.791055   \n",
       "6      text_lemma_no_stem_with_stopwords  LogisticRegression  0.787561   \n",
       "26        text_lemma_stem_with_stopwords  LogisticRegression  0.787561   \n",
       "0   text_no_lemma_no_stem_with_stopwords                 SVC  0.809224   \n",
       "4      text_lemma_no_stem_with_stopwords                 SVC  0.806429   \n",
       "24        text_lemma_stem_with_stopwords                 SVC  0.806429   \n",
       "22       text_no_lemma_stem_no_stopwords  LogisticRegression  0.781971   \n",
       "18       text_lemma_no_stem_no_stopwords  LogisticRegression  0.779175   \n",
       "30          text_lemma_stem_no_stopwords  LogisticRegression  0.779175   \n",
       "20       text_no_lemma_stem_no_stopwords                 SVC  0.798742   \n",
       "28          text_lemma_stem_no_stopwords                 SVC  0.797345   \n",
       "16       text_lemma_no_stem_no_stopwords                 SVC  0.797345   \n",
       "14    text_no_lemma_no_stem_no_stopwords  LogisticRegression  0.772187   \n",
       "12    text_no_lemma_no_stem_no_stopwords                 SVC  0.791754   \n",
       "9      text_no_lemma_stem_with_stopwords                 XGB  0.791055   \n",
       "5      text_lemma_no_stem_with_stopwords                 XGB  0.788260   \n",
       "25        text_lemma_stem_with_stopwords                 XGB  0.788260   \n",
       "1   text_no_lemma_no_stem_with_stopwords                 XGB  0.783368   \n",
       "21       text_no_lemma_stem_no_stopwords                 XGB  0.775681   \n",
       "17       text_lemma_no_stem_no_stopwords                 XGB  0.776380   \n",
       "29          text_lemma_stem_no_stopwords                 XGB  0.776380   \n",
       "13    text_no_lemma_no_stem_no_stopwords                 XGB  0.763802   \n",
       "7      text_lemma_no_stem_with_stopwords                 KNN  0.686233   \n",
       "27        text_lemma_stem_with_stopwords                 KNN  0.686233   \n",
       "3   text_no_lemma_no_stem_with_stopwords                 KNN  0.684137   \n",
       "11     text_no_lemma_stem_with_stopwords                 KNN  0.684137   \n",
       "23       text_no_lemma_stem_no_stopwords                 KNN  0.683438   \n",
       "19       text_lemma_no_stem_no_stopwords                 KNN  0.682041   \n",
       "31          text_lemma_stem_no_stopwords                 KNN  0.682041   \n",
       "15    text_no_lemma_no_stem_no_stopwords                 KNN  0.677149   \n",
       "\n",
       "    macro_f1  macro_precision  macro_recall  weighted_f1  weighted_precision  \\\n",
       "8   0.731998         0.778293      0.703326     0.806934            0.808865   \n",
       "10  0.726905         0.717443      0.737918     0.797449            0.801580   \n",
       "2   0.722747         0.714299      0.732550     0.793628            0.797305   \n",
       "6   0.720361         0.710561      0.731926     0.790626            0.795116   \n",
       "26  0.720361         0.710561      0.731926     0.790626            0.795116   \n",
       "0   0.718657         0.778195      0.686690     0.798557            0.803592   \n",
       "4   0.716831         0.769087      0.687642     0.796493            0.799723   \n",
       "24  0.716831         0.769087      0.687642     0.796493            0.799723   \n",
       "22  0.710177         0.700950      0.721531     0.785385            0.790555   \n",
       "18  0.703476         0.695583      0.712576     0.782105            0.786011   \n",
       "30  0.703476         0.695583      0.712576     0.782105            0.786011   \n",
       "20  0.702859         0.745125      0.678140     0.789097            0.789579   \n",
       "28  0.702200         0.745910      0.676204     0.787496            0.788126   \n",
       "16  0.702200         0.745910      0.676204     0.787496            0.788126   \n",
       "14  0.693545         0.686310      0.701839     0.775084            0.778840   \n",
       "12  0.690145         0.745385      0.663118     0.780304            0.784250   \n",
       "9   0.687158         0.767445      0.646310     0.774816            0.785059   \n",
       "5   0.678494         0.770458      0.635840     0.769970            0.783068   \n",
       "25  0.678494         0.770458      0.635840     0.769970            0.783068   \n",
       "1   0.667776         0.763118      0.625007     0.763354            0.777473   \n",
       "21  0.666082         0.739270      0.628942     0.758951            0.766108   \n",
       "17  0.652027         0.740427      0.614012     0.755425            0.765978   \n",
       "29  0.652027         0.740427      0.614012     0.755425            0.765978   \n",
       "13  0.631598         0.734190      0.592920     0.739667            0.754626   \n",
       "7   0.400782         0.832377      0.407397     0.594207            0.757238   \n",
       "27  0.400782         0.832377      0.407397     0.594207            0.757238   \n",
       "3   0.399420         0.798271      0.406317     0.592871            0.739143   \n",
       "11  0.397212         0.806693      0.405141     0.591853            0.744217   \n",
       "23  0.394090         0.776564      0.403605     0.590705            0.729386   \n",
       "19  0.390326         0.745448      0.401290     0.588483            0.714901   \n",
       "31  0.390326         0.745448      0.401290     0.588483            0.714901   \n",
       "15  0.376216         0.725946      0.392809     0.579423            0.702528   \n",
       "\n",
       "    weighted_recall  \n",
       "8          0.815514  \n",
       "10         0.794549  \n",
       "2          0.791055  \n",
       "6          0.787561  \n",
       "26         0.787561  \n",
       "0          0.809224  \n",
       "4          0.806429  \n",
       "24         0.806429  \n",
       "22         0.781971  \n",
       "18         0.779175  \n",
       "30         0.779175  \n",
       "20         0.798742  \n",
       "28         0.797345  \n",
       "16         0.797345  \n",
       "14         0.772187  \n",
       "12         0.791754  \n",
       "9          0.791055  \n",
       "5          0.788260  \n",
       "25         0.788260  \n",
       "1          0.783368  \n",
       "21         0.775681  \n",
       "17         0.776380  \n",
       "29         0.776380  \n",
       "13         0.763802  \n",
       "7          0.686233  \n",
       "27         0.686233  \n",
       "3          0.684137  \n",
       "11         0.684137  \n",
       "23         0.683438  \n",
       "19         0.682041  \n",
       "31         0.682041  \n",
       "15         0.677149  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sorted_results_tfidf = traditional_ml_tfidf.sort_values(by=\"macro_f1\", ascending=False)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "display(sorted_results_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ef72d9",
   "metadata": {},
   "source": [
    "We chose text_no_lemma_stem_with_stopwords by majority vote."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
