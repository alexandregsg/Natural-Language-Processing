{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "742995a1",
   "metadata": {},
   "source": [
    "## Preprocessing and feature extraction notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b71ba8f",
   "metadata": {},
   "source": [
    "### Group 8 Members\n",
    "#### Spring Semester 2024-2025\n",
    "- Alexandre Gonçalves - 20240738\n",
    "- Bráulio Damba - 20240007\n",
    "- Hugo Fonseca - 20240520\n",
    "- Ricardo Pereira - 20240745\n",
    "- Victoria Goon - 20240550"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89774444",
   "metadata": {},
   "source": [
    "## 1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83bd6cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/ricardo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ricardo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ricardo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/ricardo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Text extraction \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from collections import Counter\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models import FastText\n",
    "\n",
    "import scipy.sparse\n",
    "from scipy import sparse\n",
    "\n",
    "import contractions\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import pickle\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Deep Learning libraries\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Bidirectional, Dropout, Flatten, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Set pd options to display all columns and rows\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_rows\", 30)\n",
    "pd.set_option('display.max_colwidth', None)  # Show full text without truncation\n",
    "\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"xgboost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "911d1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory (where the notebook is)\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# Construct full paths to the CSV files\n",
    "train_path = os.path.join(BASE_DIR, \"data\", \"train.csv\")\n",
    "test_path = os.path.join(BASE_DIR, \"data\", \"test.csv\")\n",
    "\n",
    "# Load the datasets\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc4fbaa",
   "metadata": {},
   "source": [
    "## 2 - Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f494ad",
   "metadata": {},
   "source": [
    "Filtering text with less than 3 words, that are noise and have no sentimental value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255b3db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 42 tweets (0.44% of original data)\n",
      "\n",
      "Sample of removed tweets (< 3 words):\n",
      "                    AMRN\n",
      "Wipro赢得Marelli的多年战略性IT协议\n",
      " https://t.co/575AH1YRkF\n",
      "                    BWAY\n",
      "                 @TicToc\n"
     ]
    }
   ],
   "source": [
    "original_train_df = df_train.copy()\n",
    "original_train_size = len(original_train_df)\n",
    "\n",
    "def word_count(text):\n",
    "    if isinstance(text, str):\n",
    "        return len(text.strip().split())\n",
    "    return 0\n",
    "\n",
    "# Identify tweets to remove (< 3 words)\n",
    "removed_tweets = original_train_df[original_train_df[\"text\"].apply(word_count) < 3]\n",
    "\n",
    "df_train = original_train_df[original_train_df[\"text\"].apply(word_count) >= 3].reset_index(drop=True)\n",
    "\n",
    "new_train_size = len(df_train)\n",
    "removed = original_train_size - new_train_size\n",
    "percent_removed = (removed / original_train_size) * 100\n",
    "\n",
    "print(f\"Removed {removed} tweets ({percent_removed:.2f}% of original data)\")\n",
    "\n",
    "print(\"\\nSample of removed tweets (< 3 words):\")\n",
    "print(removed_tweets[\"text\"].sample(n=min(5, len(removed_tweets)), random_state=42).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd58a796",
   "metadata": {},
   "source": [
    "### 2.1 - Text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d0438",
   "metadata": {},
   "source": [
    "**Symeonidis, Effrosynidis, and Arampatzis (2018) perform a comparative evaluation of pre-processing techniques and their interactions for twitter sentiment analysis** perform a comparative study on several pre-processing techniques for twitter sentiment analysis.\n",
    "\n",
    "Based on this approach, we decided to:\n",
    "\n",
    "1) **Handle contractions:** Strings like “won’t” and “don’t” will be replaced by “will not” and “do not”, respectively. If we do not replace contractions, the tokenization process would create the tokens “don” and “’t” (for the case of “don’t”), with the second one not being particularly helpful as it will match with more than the other not’s in texts.\n",
    "\n",
    "2) **URL/User Mention Replacement**: In tweets, the majority of sentences contain a URL, a user mention, and/or a hashtag symbol. Their presence does not contain any sentiment and one approach is to replace them in pre processing with tags as, e.g. **Agarwal et al. (2011)** do. In our project, we use the tags ‘URL’ and ‘USER’.\n",
    "\n",
    "2) **Replace the numbers** with the [NUM] symbol, as many researchers argue that in sentiment analysis, keeping numbers is not really useful;\n",
    "\n",
    "3) **Replace punctuation repetition** , as it normalizes language and generalizes vocabulary to represent sentiment **(Balahur, 2013)** ;\n",
    "\n",
    "4) **Stopword removal:** Stopwords are function words with high frequencies of presence across all sentences. It is considered needless to analyze \n",
    "them, due to the fact that they do not contain much useful information for Sentiment Analysis.\n",
    "\n",
    "5) **Lemmatization:** This method analyzes a word morphologically and removes its inflectional ending, producing its base form or lemma as it is \n",
    "found in a dictionary. Lemmatization is used by **Guzman and Maalej (2014)** to reduce the number of feature descriptors for user sentiment extraction. \n",
    "\n",
    "6) **Stemming:** It is the process of removing the endings of the words in order to detect their root form or stem. By doing so, many words are merged and the dimensionality is reduced. It is a widely used method that generally yields good results. In our project, the Porter Stemmer ( **Porter, 1980**) is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6228d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://www.nltk.org/api/nltk.stem.WordNetLemmatizer.html?highlight=wordnet\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Source: https://www.nltk.org/api/nltk.tokenize.casual.html\n",
    "# Difference between TweetTokenizer and Word_Tokenize: https://stackoverflow.com/questions/61919670/how-nltk-tweettokenizer-different-from-nltk-word-tokenize\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "# Source: https://www.nltk.org/_modules/nltk/stem/porter.html\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Set of English stop words from NLTK\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "198b9ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_column(text,lemmatizer=None, stemmer=None, remove_stopwords=None):\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace URLs and user mentions\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"URL\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"USER\", text)\n",
    "\n",
    "    # Expand contractions (we use contractions library for this)\n",
    "    # Contractions library Source: https://pypi.org/project/contractions/\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # # Replace numbers with [NUM]\n",
    "    # text = re.sub(r\"\\d+(\\.\\d+)?\", \"[NUM]\", text)\n",
    "\n",
    "    # Convert to tickers (e.g., $AAPL to [TICKER])\n",
    "    text = re.sub(r\"\\$[a-z]{1,5}\", \"[TICKER]\", text)\n",
    "\n",
    "    #Remove numbers\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "    # Normalize punctuation repetitions\n",
    "    text = re.sub(r\"([!?\\.])\\1+\", r\"\\1\", text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    # Remove stop words and punctuation\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
    "    else:\n",
    "        tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Lemmatization OR stemming \n",
    "    if lemmatizer is not None and stemmer is None:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    elif stemmer is not None and lemmatizer is None:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    elif lemmatizer is not None and stemmer is not None:\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Else, leave tokens as is\n",
    "\n",
    "    # Source: https://www.nltk.org/api/nltk.tokenize.treebank.html \n",
    "    # TreebankWordDetokenizer from NLTK takes care of the correct spacing and formatting, \n",
    "    # and we get a well-formed sentence that looks like natural English (e.g. without TreebankWordDetokinzer: This is an example tweet ! , With: This is an example tweet!)\n",
    "    return TreebankWordDetokenizer().detokenize(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad1cefa",
   "metadata": {},
   "source": [
    "#### Reason why we perform pre-processing with `clean_text_column` before the Train/Val/Test Split:\n",
    "\n",
    "Doing this does **not** cause Data Leakage because the `clean_text_column` function performs **rule-based text transformations** such as:\n",
    "\n",
    "- Removing URLs, user mentions, numbers, and tickers with regular expressions\n",
    "- Expanding contractions\n",
    "- Lemmatizing or stemming tokens\n",
    "- etc. \n",
    "\n",
    "All these steps:\n",
    "- Do NOT learn or fit any parameters** from the data.\n",
    "- Do NOT extract information** (such as frequency statistics, word distributions, or labels) that could bias the model.\n",
    "- Are simply applying the same set of rules to each text string independently, regardless of which dataset (train, val, test) it comes from.\n",
    "\n",
    "**Therefore:**  \n",
    "Applying this cleaning function to the entire dataset *before* splitting will NOT cause data leakage, because it does not expose our model to any information from the test/validation sets that it could \"cheat\" with during training or evaluation.\n",
    "\n",
    "**Only steps that \"fit\" on the full data (like building a vocabulary, fitting a vectorizer, or computing mean/variance for scaling) should be done *after* splitting to avoid leakage.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d406a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train.copy()\n",
    "df_test_copy = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87aa04a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_combinations = []\n",
    "\n",
    "for lem, stm, rm_stop in product([None, lemmatizer], [None, stemmer], [False, True]):\n",
    "    name = []\n",
    "    name.append('lemma' if lem else 'no_lemma')\n",
    "    name.append('stem' if stm else 'no_stem')\n",
    "    name.append('no_stopwords' if rm_stop else 'with_stopwords')\n",
    "    preproc_combinations.append({\n",
    "        \"lemmatizer\": lem,\n",
    "        \"stemmer\": stm,\n",
    "        \"remove_stopwords\": rm_stop,\n",
    "        \"name\": '_'.join(name)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad009d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_preproc_combinations(df, combinations, text_col=\"text\"):\n",
    "    for combo in combinations:\n",
    "        column_name = f\"text_{combo['name']}\"\n",
    "        print(f\"Processing {column_name}...\")\n",
    "        df[column_name] = df[text_col].apply(\n",
    "            lambda x: clean_text_column(\n",
    "                x, \n",
    "                lemmatizer=combo['lemmatizer'], \n",
    "                stemmer=combo['stemmer'], \n",
    "                remove_stopwords=combo['remove_stopwords']\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1a1b60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text_no_lemma_no_stem_with_stopwords...\n",
      "Processing text_no_lemma_no_stem_no_stopwords...\n",
      "Processing text_no_lemma_stem_with_stopwords...\n",
      "Processing text_no_lemma_stem_no_stopwords...\n",
      "Processing text_lemma_no_stem_with_stopwords...\n",
      "Processing text_lemma_no_stem_no_stopwords...\n",
      "Processing text_lemma_stem_with_stopwords...\n",
      "Processing text_lemma_stem_no_stopwords...\n",
      "Processing text_no_lemma_no_stem_with_stopwords...\n",
      "Processing text_no_lemma_no_stem_no_stopwords...\n",
      "Processing text_no_lemma_stem_with_stopwords...\n",
      "Processing text_no_lemma_stem_no_stopwords...\n",
      "Processing text_lemma_no_stem_with_stopwords...\n",
      "Processing text_lemma_no_stem_no_stopwords...\n",
      "Processing text_lemma_stem_with_stopwords...\n",
      "Processing text_lemma_stem_no_stopwords...\n"
     ]
    }
   ],
   "source": [
    "df_train_copy = apply_preproc_combinations(df_train_copy, preproc_combinations)\n",
    "df_test_copy  = apply_preproc_combinations(df_test_copy, preproc_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b61a932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>text_no_lemma_no_stem_with_stopwords</th>\n",
       "      <th>text_no_lemma_no_stem_no_stopwords</th>\n",
       "      <th>text_no_lemma_stem_with_stopwords</th>\n",
       "      <th>text_no_lemma_stem_no_stopwords</th>\n",
       "      <th>text_lemma_no_stem_with_stopwords</th>\n",
       "      <th>text_lemma_no_stem_no_stopwords</th>\n",
       "      <th>text_lemma_stem_with_stopwords</th>\n",
       "      <th>text_lemma_stem_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT</td>\n",
       "      <td>0</td>\n",
       "      <td>TICKER jpmorgan reels in expectations on beyond meat URL</td>\n",
       "      <td>TICKER jpmorgan reels expectations beyond meat URL</td>\n",
       "      <td>ticker jpmorgan reel in expect on beyond meat url</td>\n",
       "      <td>ticker jpmorgan reel expect beyond meat url</td>\n",
       "      <td>TICKER jpmorgan reel in expectation on beyond meat URL</td>\n",
       "      <td>TICKER jpmorgan reel expectation beyond meat URL</td>\n",
       "      <td>TICKER jpmorgan reel in expectation on beyond meat URL</td>\n",
       "      <td>TICKER jpmorgan reel expectation beyond meat URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$CCL $RCL - Nomura points to bookings weakness at Carnival and Royal Caribbean https://t.co/yGjpT2ReD3</td>\n",
       "      <td>0</td>\n",
       "      <td>TICKER TICKER nomura points to bookings weakness at carnival and royal caribbean URL</td>\n",
       "      <td>TICKER TICKER nomura points bookings weakness carnival royal caribbean URL</td>\n",
       "      <td>ticker ticker nomura point to book weak at carniv and royal caribbean url</td>\n",
       "      <td>ticker ticker nomura point book weak carniv royal caribbean url</td>\n",
       "      <td>TICKER TICKER nomura point to booking weakness at carnival and royal caribbean URL</td>\n",
       "      <td>TICKER TICKER nomura point booking weakness carnival royal caribbean URL</td>\n",
       "      <td>TICKER TICKER nomura point to booking weakness at carnival and royal caribbean URL</td>\n",
       "      <td>TICKER TICKER nomura point booking weakness carnival royal caribbean URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$CX - Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook https://t.co/KN1g4AWFIb</td>\n",
       "      <td>0</td>\n",
       "      <td>TICKER cemex cut at credit suisse j p morgan on weak building outlook URL</td>\n",
       "      <td>TICKER cemex cut credit suisse j p morgan weak building outlook URL</td>\n",
       "      <td>ticker cemex cut at credit suiss j p morgan on weak build outlook url</td>\n",
       "      <td>ticker cemex cut credit suiss j p morgan weak build outlook url</td>\n",
       "      <td>TICKER cemex cut at credit suisse j p morgan on weak building outlook URL</td>\n",
       "      <td>TICKER cemex cut credit suisse j p morgan weak building outlook URL</td>\n",
       "      <td>TICKER cemex cut at credit suisse j p morgan on weak building outlook URL</td>\n",
       "      <td>TICKER cemex cut credit suisse j p morgan weak building outlook URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$ESS: BTIG Research cuts to Neutral https://t.co/MCyfTsXc2N</td>\n",
       "      <td>0</td>\n",
       "      <td>TICKER]: btig research cuts to neutral URL</td>\n",
       "      <td>TICKER]: btig research cuts neutral URL</td>\n",
       "      <td>ticker]: btig research cut to neutral url</td>\n",
       "      <td>ticker]: btig research cut neutral url</td>\n",
       "      <td>TICKER]: btig research cut to neutral URL</td>\n",
       "      <td>TICKER]: btig research cut neutral URL</td>\n",
       "      <td>TICKER]: btig research cut to neutral URL</td>\n",
       "      <td>TICKER]: btig research cut neutral URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$FNKO - Funko slides after Piper Jaffray PT cut https://t.co/z37IJmCQzB</td>\n",
       "      <td>0</td>\n",
       "      <td>TICKER funko slides after piper jaffray pt cut URL</td>\n",
       "      <td>TICKER funko slides piper jaffray pt cut URL</td>\n",
       "      <td>ticker funko slide after piper jaffray pt cut url</td>\n",
       "      <td>ticker funko slide piper jaffray pt cut url</td>\n",
       "      <td>TICKER funko slide after piper jaffray pt cut URL</td>\n",
       "      <td>TICKER funko slide piper jaffray pt cut URL</td>\n",
       "      <td>TICKER funko slide after piper jaffray pt cut URL</td>\n",
       "      <td>TICKER funko slide piper jaffray pt cut URL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                     text  \\\n",
       "0                           $BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT   \n",
       "1  $CCL $RCL - Nomura points to bookings weakness at Carnival and Royal Caribbean https://t.co/yGjpT2ReD3   \n",
       "2          $CX - Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook https://t.co/KN1g4AWFIb   \n",
       "3                                             $ESS: BTIG Research cuts to Neutral https://t.co/MCyfTsXc2N   \n",
       "4                                 $FNKO - Funko slides after Piper Jaffray PT cut https://t.co/z37IJmCQzB   \n",
       "\n",
       "   label  \\\n",
       "0      0   \n",
       "1      0   \n",
       "2      0   \n",
       "3      0   \n",
       "4      0   \n",
       "\n",
       "                                                   text_no_lemma_no_stem_with_stopwords  \\\n",
       "0                              TICKER jpmorgan reels in expectations on beyond meat URL   \n",
       "1  TICKER TICKER nomura points to bookings weakness at carnival and royal caribbean URL   \n",
       "2             TICKER cemex cut at credit suisse j p morgan on weak building outlook URL   \n",
       "3                                            TICKER]: btig research cuts to neutral URL   \n",
       "4                                    TICKER funko slides after piper jaffray pt cut URL   \n",
       "\n",
       "                                           text_no_lemma_no_stem_no_stopwords  \\\n",
       "0                          TICKER jpmorgan reels expectations beyond meat URL   \n",
       "1  TICKER TICKER nomura points bookings weakness carnival royal caribbean URL   \n",
       "2         TICKER cemex cut credit suisse j p morgan weak building outlook URL   \n",
       "3                                     TICKER]: btig research cuts neutral URL   \n",
       "4                                TICKER funko slides piper jaffray pt cut URL   \n",
       "\n",
       "                                           text_no_lemma_stem_with_stopwords  \\\n",
       "0                          ticker jpmorgan reel in expect on beyond meat url   \n",
       "1  ticker ticker nomura point to book weak at carniv and royal caribbean url   \n",
       "2      ticker cemex cut at credit suiss j p morgan on weak build outlook url   \n",
       "3                                  ticker]: btig research cut to neutral url   \n",
       "4                          ticker funko slide after piper jaffray pt cut url   \n",
       "\n",
       "                                   text_no_lemma_stem_no_stopwords  \\\n",
       "0                      ticker jpmorgan reel expect beyond meat url   \n",
       "1  ticker ticker nomura point book weak carniv royal caribbean url   \n",
       "2  ticker cemex cut credit suiss j p morgan weak build outlook url   \n",
       "3                           ticker]: btig research cut neutral url   \n",
       "4                      ticker funko slide piper jaffray pt cut url   \n",
       "\n",
       "                                                    text_lemma_no_stem_with_stopwords  \\\n",
       "0                              TICKER jpmorgan reel in expectation on beyond meat URL   \n",
       "1  TICKER TICKER nomura point to booking weakness at carnival and royal caribbean URL   \n",
       "2           TICKER cemex cut at credit suisse j p morgan on weak building outlook URL   \n",
       "3                                           TICKER]: btig research cut to neutral URL   \n",
       "4                                   TICKER funko slide after piper jaffray pt cut URL   \n",
       "\n",
       "                                            text_lemma_no_stem_no_stopwords  \\\n",
       "0                          TICKER jpmorgan reel expectation beyond meat URL   \n",
       "1  TICKER TICKER nomura point booking weakness carnival royal caribbean URL   \n",
       "2       TICKER cemex cut credit suisse j p morgan weak building outlook URL   \n",
       "3                                    TICKER]: btig research cut neutral URL   \n",
       "4                               TICKER funko slide piper jaffray pt cut URL   \n",
       "\n",
       "                                                       text_lemma_stem_with_stopwords  \\\n",
       "0                              TICKER jpmorgan reel in expectation on beyond meat URL   \n",
       "1  TICKER TICKER nomura point to booking weakness at carnival and royal caribbean URL   \n",
       "2           TICKER cemex cut at credit suisse j p morgan on weak building outlook URL   \n",
       "3                                           TICKER]: btig research cut to neutral URL   \n",
       "4                                   TICKER funko slide after piper jaffray pt cut URL   \n",
       "\n",
       "                                               text_lemma_stem_no_stopwords  \n",
       "0                          TICKER jpmorgan reel expectation beyond meat URL  \n",
       "1  TICKER TICKER nomura point booking weakness carnival royal caribbean URL  \n",
       "2       TICKER cemex cut credit suisse j p morgan weak building outlook URL  \n",
       "3                                    TICKER]: btig research cut neutral URL  \n",
       "4                               TICKER funko slide piper jaffray pt cut URL  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62e8aea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_lemma_no_stem_with_stopwords</th>\n",
       "      <th>text_no_lemma_no_stem_no_stopwords</th>\n",
       "      <th>text_no_lemma_stem_with_stopwords</th>\n",
       "      <th>text_no_lemma_stem_no_stopwords</th>\n",
       "      <th>text_lemma_no_stem_with_stopwords</th>\n",
       "      <th>text_lemma_no_stem_no_stopwords</th>\n",
       "      <th>text_lemma_stem_with_stopwords</th>\n",
       "      <th>text_lemma_stem_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ETF assets to surge tenfold in 10 years to $50 trillion, Bank of America predicts</td>\n",
       "      <td>etf assets to surge tenfold in years to trillion bank of america predicts</td>\n",
       "      <td>etf assets surge tenfold years trillion bank america predicts</td>\n",
       "      <td>etf asset to surg tenfold in year to trillion bank of america predict</td>\n",
       "      <td>etf asset surg tenfold year trillion bank america predict</td>\n",
       "      <td>etf asset to surge tenfold in year to trillion bank of america predicts</td>\n",
       "      <td>etf asset surge tenfold year trillion bank america predicts</td>\n",
       "      <td>etf asset to surge tenfold in year to trillion bank of america predicts</td>\n",
       "      <td>etf asset surge tenfold year trillion bank america predicts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Here’s What Hedge Funds Think Evolution Petroleum Corporation (EPM)</td>\n",
       "      <td>here is what hedge funds think evolution petroleum corporation epm</td>\n",
       "      <td>hedge funds think evolution petroleum corporation epm</td>\n",
       "      <td>here is what hedg fund think evolut petroleum corpor epm</td>\n",
       "      <td>hedg fund think evolut petroleum corpor epm</td>\n",
       "      <td>here is what hedge fund think evolution petroleum corporation epm</td>\n",
       "      <td>hedge fund think evolution petroleum corporation epm</td>\n",
       "      <td>here is what hedge fund think evolution petroleum corporation epm</td>\n",
       "      <td>hedge fund think evolution petroleum corporation epm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>$PVH - Phillips-Van Heusen Q3 2020 Earnings Preview https://t.co/kNhCYwVnBX</td>\n",
       "      <td>TICKER phillips-van heusen q earnings preview URL</td>\n",
       "      <td>TICKER phillips-van heusen q earnings preview URL</td>\n",
       "      <td>ticker phillips-van heusen q earn preview url</td>\n",
       "      <td>ticker phillips-van heusen q earn preview url</td>\n",
       "      <td>TICKER phillips-van heusen q earnings preview URL</td>\n",
       "      <td>TICKER phillips-van heusen q earnings preview URL</td>\n",
       "      <td>TICKER phillips-van heusen q earnings preview URL</td>\n",
       "      <td>TICKER phillips-van heusen q earnings preview URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>China is in the process of waiving retaliatory tariffs on imports of U.S. pork and soy by domestic companies, a pro… https://t.co/08mZU9TrBX</td>\n",
       "      <td>china is in the process of waiving retaliatory tariffs on imports of you s pork and soy by domestic companies a pro … URL</td>\n",
       "      <td>china process waiving retaliatory tariffs imports pork soy domestic companies pro … URL</td>\n",
       "      <td>china is in the process of waiv retaliatori tariff on import of you s pork and soy by domest compani a pro … url</td>\n",
       "      <td>china process waiv retaliatori tariff import pork soy domest compani pro … url</td>\n",
       "      <td>china is in the process of waiving retaliatory tariff on import of you s pork and soy by domestic company a pro … URL</td>\n",
       "      <td>china process waiving retaliatory tariff import pork soy domestic company pro … URL</td>\n",
       "      <td>china is in the process of waiving retaliatory tariff on import of you s pork and soy by domestic company a pro … URL</td>\n",
       "      <td>china process waiving retaliatory tariff import pork soy domestic company pro … URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Highlight: “When growth is scarce, investors seem very willing to pay up for growth stock\" @PNCBank's… https://t.co/rO4fBOkBG9</td>\n",
       "      <td>highlight “ when growth is scarce investors seem very willing to pay up for growth stock USER's … URL</td>\n",
       "      <td>highlight “ growth scarce investors seem willing pay growth stock USER's … URL</td>\n",
       "      <td>highlight “ when growth is scarc investor seem veri will to pay up for growth stock user' … url</td>\n",
       "      <td>highlight “ growth scarc investor seem will pay growth stock user' … url</td>\n",
       "      <td>highlight “ when growth is scarce investor seem very willing to pay up for growth stock USER's … URL</td>\n",
       "      <td>highlight “ growth scarce investor seem willing pay growth stock USER's … URL</td>\n",
       "      <td>highlight “ when growth is scarce investor seem very willing to pay up for growth stock USER's … URL</td>\n",
       "      <td>highlight “ growth scarce investor seem willing pay growth stock USER's … URL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0   0   \n",
       "1   1   \n",
       "2   2   \n",
       "3   3   \n",
       "4   4   \n",
       "\n",
       "                                                                                                                                           text  \\\n",
       "0                                                             ETF assets to surge tenfold in 10 years to $50 trillion, Bank of America predicts   \n",
       "1                                                                           Here’s What Hedge Funds Think Evolution Petroleum Corporation (EPM)   \n",
       "2                                                                   $PVH - Phillips-Van Heusen Q3 2020 Earnings Preview https://t.co/kNhCYwVnBX   \n",
       "3  China is in the process of waiving retaliatory tariffs on imports of U.S. pork and soy by domestic companies, a pro… https://t.co/08mZU9TrBX   \n",
       "4                Highlight: “When growth is scarce, investors seem very willing to pay up for growth stock\" @PNCBank's… https://t.co/rO4fBOkBG9   \n",
       "\n",
       "                                                                                        text_no_lemma_no_stem_with_stopwords  \\\n",
       "0                                                  etf assets to surge tenfold in years to trillion bank of america predicts   \n",
       "1                                                         here is what hedge funds think evolution petroleum corporation epm   \n",
       "2                                                                          TICKER phillips-van heusen q earnings preview URL   \n",
       "3  china is in the process of waiving retaliatory tariffs on imports of you s pork and soy by domestic companies a pro … URL   \n",
       "4                      highlight “ when growth is scarce investors seem very willing to pay up for growth stock USER's … URL   \n",
       "\n",
       "                                                        text_no_lemma_no_stem_no_stopwords  \\\n",
       "0                            etf assets surge tenfold years trillion bank america predicts   \n",
       "1                                    hedge funds think evolution petroleum corporation epm   \n",
       "2                                        TICKER phillips-van heusen q earnings preview URL   \n",
       "3  china process waiving retaliatory tariffs imports pork soy domestic companies pro … URL   \n",
       "4           highlight “ growth scarce investors seem willing pay growth stock USER's … URL   \n",
       "\n",
       "                                                                                  text_no_lemma_stem_with_stopwords  \\\n",
       "0                                             etf asset to surg tenfold in year to trillion bank of america predict   \n",
       "1                                                          here is what hedg fund think evolut petroleum corpor epm   \n",
       "2                                                                     ticker phillips-van heusen q earn preview url   \n",
       "3  china is in the process of waiv retaliatori tariff on import of you s pork and soy by domest compani a pro … url   \n",
       "4                   highlight “ when growth is scarc investor seem veri will to pay up for growth stock user' … url   \n",
       "\n",
       "                                                  text_no_lemma_stem_no_stopwords  \\\n",
       "0                       etf asset surg tenfold year trillion bank america predict   \n",
       "1                                     hedg fund think evolut petroleum corpor epm   \n",
       "2                                   ticker phillips-van heusen q earn preview url   \n",
       "3  china process waiv retaliatori tariff import pork soy domest compani pro … url   \n",
       "4        highlight “ growth scarc investor seem will pay growth stock user' … url   \n",
       "\n",
       "                                                                                       text_lemma_no_stem_with_stopwords  \\\n",
       "0                                                etf asset to surge tenfold in year to trillion bank of america predicts   \n",
       "1                                                      here is what hedge fund think evolution petroleum corporation epm   \n",
       "2                                                                      TICKER phillips-van heusen q earnings preview URL   \n",
       "3  china is in the process of waiving retaliatory tariff on import of you s pork and soy by domestic company a pro … URL   \n",
       "4                   highlight “ when growth is scarce investor seem very willing to pay up for growth stock USER's … URL   \n",
       "\n",
       "                                                       text_lemma_no_stem_no_stopwords  \\\n",
       "0                          etf asset surge tenfold year trillion bank america predicts   \n",
       "1                                 hedge fund think evolution petroleum corporation epm   \n",
       "2                                    TICKER phillips-van heusen q earnings preview URL   \n",
       "3  china process waiving retaliatory tariff import pork soy domestic company pro … URL   \n",
       "4        highlight “ growth scarce investor seem willing pay growth stock USER's … URL   \n",
       "\n",
       "                                                                                          text_lemma_stem_with_stopwords  \\\n",
       "0                                                etf asset to surge tenfold in year to trillion bank of america predicts   \n",
       "1                                                      here is what hedge fund think evolution petroleum corporation epm   \n",
       "2                                                                      TICKER phillips-van heusen q earnings preview URL   \n",
       "3  china is in the process of waiving retaliatory tariff on import of you s pork and soy by domestic company a pro … URL   \n",
       "4                   highlight “ when growth is scarce investor seem very willing to pay up for growth stock USER's … URL   \n",
       "\n",
       "                                                          text_lemma_stem_no_stopwords  \n",
       "0                          etf asset surge tenfold year trillion bank america predicts  \n",
       "1                                 hedge fund think evolution petroleum corporation epm  \n",
       "2                                    TICKER phillips-van heusen q earnings preview URL  \n",
       "3  china process waiving retaliatory tariff import pork soy domestic company pro … URL  \n",
       "4        highlight “ growth scarce investor seem willing pay growth stock USER's … URL  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30faf7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cleaned = df_train_copy.copy()\n",
    "df_test_cleaned = df_test_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "036895ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using stratify to maintain the distribution of classes in the train, validation, and test sets \n",
    "# As our dataset is quite small, we use 80% for training, and split the remaining 20% into validation and test sets (10% each).\n",
    "\n",
    "train_df, val_test_df = train_test_split(df_train_cleaned, test_size=0.2, stratify=df_train_cleaned['label'], random_state=42)\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, stratify=val_test_df['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cf98293",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['label']\n",
    "y_val = val_df['label']\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7b8f045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_no_lemma_no_stem_with_stopwords</th>\n",
       "      <th>text_no_lemma_no_stem_no_stopwords</th>\n",
       "      <th>text_no_lemma_stem_with_stopwords</th>\n",
       "      <th>text_no_lemma_stem_no_stopwords</th>\n",
       "      <th>text_lemma_no_stem_with_stopwords</th>\n",
       "      <th>text_lemma_no_stem_no_stopwords</th>\n",
       "      <th>text_lemma_stem_with_stopwords</th>\n",
       "      <th>text_lemma_stem_no_stopwords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7473</th>\n",
       "      <td>$FTS: Fortis announced the appointment of David Hutchens as Chief Operating Officer, Fortis, effective January 1,... https://t.co/90PWjTanjp</td>\n",
       "      <td>TICKER]: fortis announced the appointment of david hutchens as chief operating officer fortis effective january URL</td>\n",
       "      <td>TICKER]: fortis announced appointment david hutchens chief operating officer fortis effective january URL</td>\n",
       "      <td>ticker]: forti announc the appoint of david hutchen as chief oper offic forti effect januari url</td>\n",
       "      <td>ticker]: forti announc appoint david hutchen chief oper offic forti effect januari url</td>\n",
       "      <td>TICKER]: fortis announced the appointment of david hutchens a chief operating officer fortis effective january URL</td>\n",
       "      <td>TICKER]: fortis announced appointment david hutchens chief operating officer fortis effective january URL</td>\n",
       "      <td>TICKER]: fortis announced the appointment of david hutchens a chief operating officer fortis effective january URL</td>\n",
       "      <td>TICKER]: fortis announced appointment david hutchens chief operating officer fortis effective january URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9279</th>\n",
       "      <td>Ebay stock up 5.4%in Monday premarket trading</td>\n",
       "      <td>ebay stock up in monday premarket trading</td>\n",
       "      <td>ebay stock monday premarket trading</td>\n",
       "      <td>ebay stock up in monday premarket trade</td>\n",
       "      <td>ebay stock monday premarket trade</td>\n",
       "      <td>ebay stock up in monday premarket trading</td>\n",
       "      <td>ebay stock monday premarket trading</td>\n",
       "      <td>ebay stock up in monday premarket trading</td>\n",
       "      <td>ebay stock monday premarket trading</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7027</th>\n",
       "      <td>Nasdaq Private Market Sets New Annual Transaction Record in 2019 - StreetInsider.com</td>\n",
       "      <td>nasdaq private market sets new annual transaction record in streetinsider.com</td>\n",
       "      <td>nasdaq private market sets new annual transaction record streetinsider.com</td>\n",
       "      <td>nasdaq privat market set new annual transact record in streetinsider.com</td>\n",
       "      <td>nasdaq privat market set new annual transact record streetinsider.com</td>\n",
       "      <td>nasdaq private market set new annual transaction record in streetinsider.com</td>\n",
       "      <td>nasdaq private market set new annual transaction record streetinsider.com</td>\n",
       "      <td>nasdaq private market set new annual transaction record in streetinsider.com</td>\n",
       "      <td>nasdaq private market set new annual transaction record streetinsider.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8900</th>\n",
       "      <td>$CBAY (-76.9% pre) CymaBay Therapeutics Halts Clinical Development of Seladelpar - GN https://t.co/GZdaOP9sfB</td>\n",
       "      <td>TICKER pre cymabay therapeutics halts clinical development of seladelpar gn URL</td>\n",
       "      <td>TICKER pre cymabay therapeutics halts clinical development seladelpar gn URL</td>\n",
       "      <td>ticker pre cymabay therapeut halt clinic develop of seladelpar gn url</td>\n",
       "      <td>ticker pre cymabay therapeut halt clinic develop seladelpar gn url</td>\n",
       "      <td>TICKER pre cymabay therapeutic halt clinical development of seladelpar gn URL</td>\n",
       "      <td>TICKER pre cymabay therapeutic halt clinical development seladelpar gn URL</td>\n",
       "      <td>TICKER pre cymabay therapeutic halt clinical development of seladelpar gn URL</td>\n",
       "      <td>TICKER pre cymabay therapeutic halt clinical development seladelpar gn URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>Dunkin' Brands lifts dividend and buyback as 2020 outlook shy of estimates</td>\n",
       "      <td>dunkin brands lifts dividend and buyback as outlook shy of estimates</td>\n",
       "      <td>dunkin brands lifts dividend buyback outlook shy estimates</td>\n",
       "      <td>dunkin brand lift dividend and buyback as outlook shi of estim</td>\n",
       "      <td>dunkin brand lift dividend buyback outlook shi estim</td>\n",
       "      <td>dunkin brand lift dividend and buyback a outlook shy of estimate</td>\n",
       "      <td>dunkin brand lift dividend buyback outlook shy estimate</td>\n",
       "      <td>dunkin brand lift dividend and buyback a outlook shy of estimate</td>\n",
       "      <td>dunkin brand lift dividend buyback outlook shy estimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8516</th>\n",
       "      <td>Here Are 2 Key Catalysts for Kroger Stock in Fiscal 2020</td>\n",
       "      <td>here are key catalysts for kroger stock in fiscal</td>\n",
       "      <td>key catalysts kroger stock fiscal</td>\n",
       "      <td>here are key catalyst for kroger stock in fiscal</td>\n",
       "      <td>key catalyst kroger stock fiscal</td>\n",
       "      <td>here are key catalyst for kroger stock in fiscal</td>\n",
       "      <td>key catalyst kroger stock fiscal</td>\n",
       "      <td>here are key catalyst for kroger stock in fiscal</td>\n",
       "      <td>key catalyst kroger stock fiscal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3033</th>\n",
       "      <td>U.S. gasoline futures tumbled to their lowest level since 1999-with some pump prices already below $1-as coronaviru… https://t.co/Tt9W1qMrJj</td>\n",
       "      <td>you s gasoline futures tumbled to their lowest level since  with some pump prices already below  as coronaviru … URL</td>\n",
       "      <td>gasoline futures tumbled lowest level since  pump prices already  coronaviru … URL</td>\n",
       "      <td>you s gasolin futur tumbl to their lowest level sinc  with some pump price alreadi below  as coronaviru … url</td>\n",
       "      <td>gasolin futur tumbl lowest level sinc  pump price alreadi  coronaviru … url</td>\n",
       "      <td>you s gasoline future tumbled to their lowest level since  with some pump price already below  a coronaviru … URL</td>\n",
       "      <td>gasoline future tumbled lowest level since  pump price already  coronaviru … URL</td>\n",
       "      <td>you s gasoline future tumbled to their lowest level since  with some pump price already below  a coronaviru … URL</td>\n",
       "      <td>gasoline future tumbled lowest level since  pump price already  coronaviru … URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>Frozen Wells Fargo Bonuses Show a Peril for Bankers After Crisis</td>\n",
       "      <td>frozen wells fargo bonuses show a peril for bankers after crisis</td>\n",
       "      <td>frozen wells fargo bonuses show peril bankers crisis</td>\n",
       "      <td>frozen well fargo bonus show a peril for banker after crisi</td>\n",
       "      <td>frozen well fargo bonus show peril banker crisi</td>\n",
       "      <td>frozen well fargo bonus show a peril for banker after crisis</td>\n",
       "      <td>frozen well fargo bonus show peril banker crisis</td>\n",
       "      <td>frozen well fargo bonus show a peril for banker after crisis</td>\n",
       "      <td>frozen well fargo bonus show peril banker crisis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7767</th>\n",
       "      <td>Why 51job Shares Dropped 15% Last Month</td>\n",
       "      <td>why job shares dropped last month</td>\n",
       "      <td>job shares dropped last month</td>\n",
       "      <td>whi job share drop last month</td>\n",
       "      <td>job share drop last month</td>\n",
       "      <td>why job share dropped last month</td>\n",
       "      <td>job share dropped last month</td>\n",
       "      <td>why job share dropped last month</td>\n",
       "      <td>job share dropped last month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4672</th>\n",
       "      <td>HR Confidential: I fired her. Then she revealed all of the office's sordid sex-and-drugs stories. (via @CNBCMakeIt) https://t.co/Pkyk6k5nkg</td>\n",
       "      <td>hr confidential i fired her then she revealed all of the office's sordid sex-and-drugs stories via USER URL</td>\n",
       "      <td>hr confidential fired revealed office's sordid sex-and-drugs stories via USER URL</td>\n",
       "      <td>hr confidenti i fire her then she reveal all of the office' sordid sex-and-drug stori via user url</td>\n",
       "      <td>hr confidenti fire reveal office' sordid sex-and-drug stori via user url</td>\n",
       "      <td>hr confidential i fired her then she revealed all of the office's sordid sex-and-drugs story via USER URL</td>\n",
       "      <td>hr confidential fired revealed office's sordid sex-and-drugs story via USER URL</td>\n",
       "      <td>hr confidential i fired her then she revealed all of the office's sordid sex-and-drugs story via USER URL</td>\n",
       "      <td>hr confidential fired revealed office's sordid sex-and-drugs story via USER URL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7600 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                text  \\\n",
       "7473    $FTS: Fortis announced the appointment of David Hutchens as Chief Operating Officer, Fortis, effective January 1,... https://t.co/90PWjTanjp   \n",
       "9279                                                                                                   Ebay stock up 5.4%in Monday premarket trading   \n",
       "7027                                                            Nasdaq Private Market Sets New Annual Transaction Record in 2019 - StreetInsider.com   \n",
       "8900                                   $CBAY (-76.9% pre) CymaBay Therapeutics Halts Clinical Development of Seladelpar - GN https://t.co/GZdaOP9sfB   \n",
       "2236                                                                      Dunkin' Brands lifts dividend and buyback as 2020 outlook shy of estimates   \n",
       "...                                                                                                                                              ...   \n",
       "8516                                                                                        Here Are 2 Key Catalysts for Kroger Stock in Fiscal 2020   \n",
       "3033  U.S. gasoline futures tumbled to their lowest level since 1999-with some pump prices already below $1-as coronaviru… https://t.co/Tt9W1qMrJj   \n",
       "810                                                                                 Frozen Wells Fargo Bonuses Show a Peril for Bankers After Crisis   \n",
       "7767                                                                                                         Why 51job Shares Dropped 15% Last Month   \n",
       "4672     HR Confidential: I fired her. Then she revealed all of the office's sordid sex-and-drugs stories. (via @CNBCMakeIt) https://t.co/Pkyk6k5nkg   \n",
       "\n",
       "                                                                                        text_no_lemma_no_stem_with_stopwords  \\\n",
       "7473     TICKER]: fortis announced the appointment of david hutchens as chief operating officer fortis effective january URL   \n",
       "9279                                                                               ebay stock up in monday premarket trading   \n",
       "7027                                           nasdaq private market sets new annual transaction record in streetinsider.com   \n",
       "8900                                         TICKER pre cymabay therapeutics halts clinical development of seladelpar gn URL   \n",
       "2236                                                    dunkin brands lifts dividend and buyback as outlook shy of estimates   \n",
       "...                                                                                                                      ...   \n",
       "8516                                                                       here are key catalysts for kroger stock in fiscal   \n",
       "3033  you s gasoline futures tumbled to their lowest level since  with some pump prices already below  as coronaviru … URL   \n",
       "810                                                         frozen wells fargo bonuses show a peril for bankers after crisis   \n",
       "7767                                                                                       why job shares dropped last month   \n",
       "4672             hr confidential i fired her then she revealed all of the office's sordid sex-and-drugs stories via USER URL   \n",
       "\n",
       "                                                                             text_no_lemma_no_stem_no_stopwords  \\\n",
       "7473  TICKER]: fortis announced appointment david hutchens chief operating officer fortis effective january URL   \n",
       "9279                                                                        ebay stock monday premarket trading   \n",
       "7027                                 nasdaq private market sets new annual transaction record streetinsider.com   \n",
       "8900                               TICKER pre cymabay therapeutics halts clinical development seladelpar gn URL   \n",
       "2236                                                 dunkin brands lifts dividend buyback outlook shy estimates   \n",
       "...                                                                                                         ...   \n",
       "8516                                                                          key catalysts kroger stock fiscal   \n",
       "3033                       gasoline futures tumbled lowest level since  pump prices already  coronaviru … URL   \n",
       "810                                                        frozen wells fargo bonuses show peril bankers crisis   \n",
       "7767                                                                              job shares dropped last month   \n",
       "4672                          hr confidential fired revealed office's sordid sex-and-drugs stories via USER URL   \n",
       "\n",
       "                                                                                    text_no_lemma_stem_with_stopwords  \\\n",
       "7473                 ticker]: forti announc the appoint of david hutchen as chief oper offic forti effect januari url   \n",
       "9279                                                                          ebay stock up in monday premarket trade   \n",
       "7027                                         nasdaq privat market set new annual transact record in streetinsider.com   \n",
       "8900                                            ticker pre cymabay therapeut halt clinic develop of seladelpar gn url   \n",
       "2236                                                   dunkin brand lift dividend and buyback as outlook shi of estim   \n",
       "...                                                                                                               ...   \n",
       "8516                                                                 here are key catalyst for kroger stock in fiscal   \n",
       "3033  you s gasolin futur tumbl to their lowest level sinc  with some pump price alreadi below  as coronaviru … url   \n",
       "810                                                       frozen well fargo bonus show a peril for banker after crisi   \n",
       "7767                                                                                    whi job share drop last month   \n",
       "4672               hr confidenti i fire her then she reveal all of the office' sordid sex-and-drug stori via user url   \n",
       "\n",
       "                                                             text_no_lemma_stem_no_stopwords  \\\n",
       "7473  ticker]: forti announc appoint david hutchen chief oper offic forti effect januari url   \n",
       "9279                                                       ebay stock monday premarket trade   \n",
       "7027                   nasdaq privat market set new annual transact record streetinsider.com   \n",
       "8900                      ticker pre cymabay therapeut halt clinic develop seladelpar gn url   \n",
       "2236                                    dunkin brand lift dividend buyback outlook shi estim   \n",
       "...                                                                                      ...   \n",
       "8516                                                        key catalyst kroger stock fiscal   \n",
       "3033           gasolin futur tumbl lowest level sinc  pump price alreadi  coronaviru … url   \n",
       "810                                          frozen well fargo bonus show peril banker crisi   \n",
       "7767                                                               job share drop last month   \n",
       "4672                hr confidenti fire reveal office' sordid sex-and-drug stori via user url   \n",
       "\n",
       "                                                                                        text_lemma_no_stem_with_stopwords  \\\n",
       "7473   TICKER]: fortis announced the appointment of david hutchens a chief operating officer fortis effective january URL   \n",
       "9279                                                                            ebay stock up in monday premarket trading   \n",
       "7027                                         nasdaq private market set new annual transaction record in streetinsider.com   \n",
       "8900                                        TICKER pre cymabay therapeutic halt clinical development of seladelpar gn URL   \n",
       "2236                                                     dunkin brand lift dividend and buyback a outlook shy of estimate   \n",
       "...                                                                                                                   ...   \n",
       "8516                                                                     here are key catalyst for kroger stock in fiscal   \n",
       "3033  you s gasoline future tumbled to their lowest level since  with some pump price already below  a coronaviru … URL   \n",
       "810                                                          frozen well fargo bonus show a peril for banker after crisis   \n",
       "7767                                                                                     why job share dropped last month   \n",
       "4672            hr confidential i fired her then she revealed all of the office's sordid sex-and-drugs story via USER URL   \n",
       "\n",
       "                                                                                text_lemma_no_stem_no_stopwords  \\\n",
       "7473  TICKER]: fortis announced appointment david hutchens chief operating officer fortis effective january URL   \n",
       "9279                                                                        ebay stock monday premarket trading   \n",
       "7027                                  nasdaq private market set new annual transaction record streetinsider.com   \n",
       "8900                                 TICKER pre cymabay therapeutic halt clinical development seladelpar gn URL   \n",
       "2236                                                    dunkin brand lift dividend buyback outlook shy estimate   \n",
       "...                                                                                                         ...   \n",
       "8516                                                                           key catalyst kroger stock fiscal   \n",
       "3033                         gasoline future tumbled lowest level since  pump price already  coronaviru … URL   \n",
       "810                                                            frozen well fargo bonus show peril banker crisis   \n",
       "7767                                                                               job share dropped last month   \n",
       "4672                            hr confidential fired revealed office's sordid sex-and-drugs story via USER URL   \n",
       "\n",
       "                                                                                           text_lemma_stem_with_stopwords  \\\n",
       "7473   TICKER]: fortis announced the appointment of david hutchens a chief operating officer fortis effective january URL   \n",
       "9279                                                                            ebay stock up in monday premarket trading   \n",
       "7027                                         nasdaq private market set new annual transaction record in streetinsider.com   \n",
       "8900                                        TICKER pre cymabay therapeutic halt clinical development of seladelpar gn URL   \n",
       "2236                                                     dunkin brand lift dividend and buyback a outlook shy of estimate   \n",
       "...                                                                                                                   ...   \n",
       "8516                                                                     here are key catalyst for kroger stock in fiscal   \n",
       "3033  you s gasoline future tumbled to their lowest level since  with some pump price already below  a coronaviru … URL   \n",
       "810                                                          frozen well fargo bonus show a peril for banker after crisis   \n",
       "7767                                                                                     why job share dropped last month   \n",
       "4672            hr confidential i fired her then she revealed all of the office's sordid sex-and-drugs story via USER URL   \n",
       "\n",
       "                                                                                   text_lemma_stem_no_stopwords  \n",
       "7473  TICKER]: fortis announced appointment david hutchens chief operating officer fortis effective january URL  \n",
       "9279                                                                        ebay stock monday premarket trading  \n",
       "7027                                  nasdaq private market set new annual transaction record streetinsider.com  \n",
       "8900                                 TICKER pre cymabay therapeutic halt clinical development seladelpar gn URL  \n",
       "2236                                                    dunkin brand lift dividend buyback outlook shy estimate  \n",
       "...                                                                                                         ...  \n",
       "8516                                                                           key catalyst kroger stock fiscal  \n",
       "3033                         gasoline future tumbled lowest level since  pump price already  coronaviru … URL  \n",
       "810                                                            frozen well fargo bonus show peril banker crisis  \n",
       "7767                                                                               job share dropped last month  \n",
       "4672                            hr confidential fired revealed office's sordid sex-and-drugs story via USER URL  \n",
       "\n",
       "[7600 rows x 9 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c41b648",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final = train_df[\"text_no_lemma_stem_with_stopwords\"].to_numpy()\n",
    "X_val_final = val_df[\"text_no_lemma_stem_with_stopwords\"].to_numpy()\n",
    "X_test_final = test_df[\"text_no_lemma_stem_with_stopwords\"].to_numpy()\n",
    "np.save(\"train_text_no_lemma_stem_with_stopwords.npy\", X_train_final)\n",
    "np.save(\"val_text_no_lemma_stem_with_stopwords.npy\", X_val_final)\n",
    "np.save(\"test_text_no_lemma_stem_with_stopwords.npy\", X_test_final)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b109bce",
   "metadata": {},
   "source": [
    "## 3 - Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564dccaf",
   "metadata": {},
   "source": [
    "We decided to follow a general pipeline, where based on the feature extraction technique we employ , and that it is adequate to the classification model we first define:\n",
    "\n",
    "- 3.1. - Statistical Methods: Bag of Words, and TF-IDF -> 3.1.1 Classification models: SVC, XGB, Logistic Regression and KNN -> 3.1.2 Hyperparamter Tuning for the best feature extraction technique and for the best model\n",
    "\n",
    "- 3.2. - Fixed Word Embedding Encoders -> Word2Vec, FastText , Glove-Twitter -> 3.2.1 Classification Models -> Keep the best traditional ML model from 3.1 and add BiLSTM , BiGRU , BiLSTM + Attention , BiGRU + Attention  (Source: https://sbert.net/docs/sentence_transformer/pretrained_models.html) \n",
    "\n",
    "- 3.3. - Contextual Word Embedding Encoders -> ELMO (mean and concat) -> 3.3.1 Classification Models -> Keep the best traditional ML model from 3.1 and add BiLSTM , BiGRU , BiLSTM + Attention , BiGRU + Attention \n",
    "\n",
    "- 3.4. - Sentence Encoders -> all-mpnet-base-v2 , all-distilroberta-v1 , all-MiniLM-L12-v2 , paraphrase-multilingual-mpnet-base-v2 -> 3.4.1 Classification Models -> Keep the best traditional ML model from 3.1 and add BiLSTM , BiGRU , BiLSTM + Attention , BiGRU + Attention\n",
    "\n",
    "- 3.5 -> Transformers -> BERT base, BERT Large, XLNET base, XLNET large, Roberta Base, Roberta Large distilbert large, distilbert base, ALBERT x large-v1 , ALBERT-xxlarge-v2 , XLM-MLM-en-2048 , BART-LARGE  \n",
    "\n",
    "- 3.6 -> Domain Specific Transformers: FinBert , BERTweet , FinTwitBERT (Source: https://huggingface.co/StephanAkkerman/FinTwitBERT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72636b18",
   "metadata": {},
   "source": [
    "As, stated above, after doing the encoding for each feature extraction technique, we call the SMOTE function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cb8708",
   "metadata": {},
   "source": [
    "Moreover, one important note is that as we have 8 different text pre-processing combinations, and we have several models it can become quite computationally expensive to run all the pre-processing combinations. So we tested the best text variant in a different notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bcdd94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_combinations  = [\n",
    "    \"text_no_lemma_stem_with_stopwords\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f82294",
   "metadata": {},
   "source": [
    "### 3.1 - Statistical Methods - Bag of Words and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112b47cc",
   "metadata": {},
   "source": [
    "We use .npz to store sparse matrices (like BOW or TF-IDF) and .npy to store dense arrays (like embeddings) for later use in different notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62d6d7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting bow vectorizer for text_no_lemma_stem_with_stopwords...\n",
      "Fitting tfidf vectorizer for text_no_lemma_stem_with_stopwords...\n"
     ]
    }
   ],
   "source": [
    "for vec_type, VecClass in [('bow', CountVectorizer), ('tfidf', TfidfVectorizer)]:\n",
    "    for column_name in final_combinations:\n",
    "        print(f\"Fitting {vec_type} vectorizer for {column_name}...\")\n",
    "        vectorizer = VecClass(ngram_range=(1,2), max_features=15000)\n",
    "        vectorizer.fit(train_df[column_name])\n",
    "        X_train = vectorizer.transform(train_df[column_name])\n",
    "        X_val   = vectorizer.transform(val_df[column_name])\n",
    "        X_test  = vectorizer.transform(test_df[column_name])\n",
    "        sparse.save_npz(f\"{vec_type}_{column_name}_train.npz\", X_train)\n",
    "        sparse.save_npz(f\"{vec_type}_{column_name}_val.npz\", X_val)\n",
    "        sparse.save_npz(f\"{vec_type}_{column_name}_test.npz\", X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b463428",
   "metadata": {},
   "source": [
    "### 3.2 - Fixed Word Embedding Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bef3529",
   "metadata": {},
   "source": [
    "### Word2Vec and Fast-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "248c893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_to_seq(tokens, model, max_len, embed_dim):\n",
    "    seq = np.zeros((max_len, embed_dim), dtype='float32')\n",
    "    for i, token in enumerate(tokens[:max_len]):\n",
    "        if token in model.wv:\n",
    "            seq[i] = model.wv[token]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "514d5e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training w2v for text_no_lemma_stem_with_stopwords...\n",
      "Training ft for text_no_lemma_stem_with_stopwords...\n"
     ]
    }
   ],
   "source": [
    "ft_vectors = {}\n",
    "ft_models = {}\n",
    "vector_size = 200\n",
    "max_sequence_length = 32 \n",
    "\n",
    "for enc_type, ModelClass in [('w2v', Word2Vec), ('ft', FastText)]:\n",
    "    for column_name in final_combinations:\n",
    "        print(f\"Training {enc_type} for {column_name}...\")\n",
    "\n",
    "        # Prepare tokenized sentences\n",
    "        train_sentences = [tweet.split() for tweet in train_df[column_name]]\n",
    "        val_sentences   = [tweet.split() for tweet in val_df[column_name]]\n",
    "        test_sentences  = [tweet.split() for tweet in test_df[column_name]]\n",
    "\n",
    "        # Train model on train split only\n",
    "        model = ModelClass(sentences=train_sentences, vector_size=vector_size, window=10, min_count=1, workers=7)\n",
    "\n",
    "        # Get sequence vectors (3D arrays)\n",
    "        X_train = np.stack([\n",
    "            tweet_to_seq(tokens, model, max_sequence_length, vector_size)\n",
    "            for tokens in train_sentences\n",
    "        ])\n",
    "        X_val = np.stack([\n",
    "            tweet_to_seq(tokens, model, max_sequence_length, vector_size)\n",
    "            for tokens in val_sentences\n",
    "        ])\n",
    "        X_test = np.stack([\n",
    "            tweet_to_seq(tokens, model, max_sequence_length, vector_size)\n",
    "            for tokens in test_sentences\n",
    "        ])\n",
    "\n",
    "        # Save as .npy\n",
    "        np.save(f\"{enc_type}_seq_{column_name}_train.npy\", X_train)\n",
    "        np.save(f\"{enc_type}_seq_{column_name}_val.npy\", X_val)\n",
    "        np.save(f\"{enc_type}_seq_{column_name}_test.npy\", X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8af61b",
   "metadata": {},
   "source": [
    "### GLOVE-Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "439bf163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading GloVe model with 25 dimensions...\n",
      "Processing GloVe SEQUENCE 25D for text_no_lemma_stem_with_stopwords...\n",
      "\n",
      "Loading GloVe model with 50 dimensions...\n",
      "Processing GloVe SEQUENCE 50D for text_no_lemma_stem_with_stopwords...\n",
      "\n",
      "Loading GloVe model with 100 dimensions...\n",
      "Processing GloVe SEQUENCE 100D for text_no_lemma_stem_with_stopwords...\n",
      "\n",
      "Loading GloVe model with 200 dimensions...\n",
      "Processing GloVe SEQUENCE 200D for text_no_lemma_stem_with_stopwords...\n"
     ]
    }
   ],
   "source": [
    "glove_configs = {\n",
    "    25: \"glove.twitter.27B.25d.txt\",\n",
    "    50: \"glove.twitter.27B.50d.txt\",\n",
    "    100: \"glove.twitter.27B.100d.txt\",\n",
    "    200: \"glove.twitter.27B.200d.txt\"\n",
    "}\n",
    "\n",
    "class GloveWrapper:\n",
    "    def __init__(self, glove_dict):\n",
    "        self.wv = glove_dict\n",
    "\n",
    "def load_glove_model(filepath):\n",
    "    embeddings = {}\n",
    "    with open(filepath, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            parts = line.rstrip().split(\" \")\n",
    "            word = parts[0]\n",
    "            vector = np.asarray(parts[1:], dtype=\"float32\")\n",
    "            embeddings[word] = vector\n",
    "    return GloveWrapper(embeddings)\n",
    "\n",
    "for dim, glove_path in glove_configs.items():\n",
    "    print(f\"\\nLoading GloVe model with {dim} dimensions...\")\n",
    "    glove_model = load_glove_model(glove_path)\n",
    "\n",
    "    for column_name in final_combinations:\n",
    "        print(f\"Processing GloVe SEQUENCE {dim}D for {column_name}...\")\n",
    "\n",
    "        train_sentences = [tweet.split() for tweet in train_df[column_name]]\n",
    "        val_sentences   = [tweet.split() for tweet in val_df[column_name]]\n",
    "        test_sentences  = [tweet.split() for tweet in test_df[column_name]]\n",
    "\n",
    "        X_train = np.stack([tweet_to_seq(tokens, glove_model, max_sequence_length, dim) for tokens in train_sentences])\n",
    "        X_val   = np.stack([tweet_to_seq(tokens, glove_model, max_sequence_length, dim) for tokens in val_sentences])\n",
    "        X_test  = np.stack([tweet_to_seq(tokens, glove_model, max_sequence_length, dim) for tokens in test_sentences])\n",
    "\n",
    "        np.save(f\"glove_seq_{column_name}_{dim}d_train.npy\", X_train)\n",
    "        np.save(f\"glove_seq_{column_name}_{dim}d_val.npy\", X_val)\n",
    "        np.save(f\"glove_seq_{column_name}_{dim}d_test.npy\", X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7e3324",
   "metadata": {},
   "source": [
    "### 3.3 - Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b1de7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with mpnet_base_v2 for train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 119/119 [00:10<00:00, 11.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with mpnet_base_v2 for val set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 15/15 [00:01<00:00, 10.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with mpnet_base_v2 for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 15/15 [00:01<00:00, 11.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with distilroberta_v1 for train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 119/119 [00:05<00:00, 21.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with distilroberta_v1 for val set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 15/15 [00:00<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with distilroberta_v1 for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 15/15 [00:00<00:00, 22.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with minilm_l12_v2 for train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 119/119 [00:04<00:00, 29.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with minilm_l12_v2 for val set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 15/15 [00:00<00:00, 29.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with minilm_l12_v2 for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 15/15 [00:00<00:00, 29.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with all_minilm_l6_v2 for train set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 119/119 [00:01<00:00, 67.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with all_minilm_l6_v2 for val set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 15/15 [00:00<00:00, 60.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with all_minilm_l6_v2 for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 15/15 [00:00<00:00, 64.76it/s]\n"
     ]
    }
   ],
   "source": [
    "sentence_transformers = dict(\n",
    "    mpnet_base_v2 = SentenceTransformer('sentence-transformers/all-mpnet-base-v2'),\n",
    "    distilroberta_v1 = SentenceTransformer('sentence-transformers/all-distilroberta-v1'),\n",
    "    minilm_l12_v2 = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2'),\n",
    "    all_minilm_l6_v2 = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    ")\n",
    "\n",
    "for model_name, model in sentence_transformers.items():\n",
    "    for column_name in final_combinations:\n",
    "        for split_name, df in zip(['train', 'val', 'test'], [train_df, val_df, test_df]):\n",
    "            print(f\"Encoding {column_name} with {model_name} for {split_name} set...\")\n",
    "            text_data = df[column_name].astype(str).tolist()\n",
    "            embeddings = model.encode(text_data, batch_size=64, show_progress_bar=True)\n",
    "            np.save(f\"{model_name}_{column_name}_{split_name}.npy\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3900b5c",
   "metadata": {},
   "source": [
    "### 3.4 - Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d6f00",
   "metadata": {},
   "source": [
    "In this first approach with transformers we use them to apply **feature extraction** ,  and the usual ways to extract embeddings are through **mean pooling** or **CLS pooling**. We opt for the **mean pooling** aproach as it is more robust to the majority of the architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee997273",
   "metadata": {},
   "source": [
    "#### 1. Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e50b311",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_names = [\n",
    "    'bert-large-uncased', \n",
    "    'roberta-large',\n",
    "    'facebook/bart-large'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "272db9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513bad60",
   "metadata": {},
   "source": [
    "We decided to implement mean pooling for all the transformers, as according to multiple sources:\n",
    "\n",
    "- In terms of embedding quality, mean pooling is generally more robust for tasks requiring comprehensive context. (If the model is fine-tuned for a specific task, the [CLS] token can outperform pooling by focusing on task-relevant features. For instance, in sentiment analysis, a fine-tuned [CLS] token might better isolate emotional cues than a mean-pooled vector. However, if the model isn’t fine-tuned for the target task—or if the task differs significantly from pretraining objectives—the [CLS] token’s quality may degrade, making mean pooling safer for general-purpose use -> USe CLS for specific domain transformers)\n",
    "\n",
    "- “Mean pooling is often more robust than using the [CLS] token for creating fixed-size sentence embeddings, especially for models not explicitly trained for classification tasks.” (Source: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c1897a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_pooling(model, tokenizer, texts, batch_size=32, max_length=128):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            inputs = tokenizer(batch, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(device)\n",
    "            outputs = model(**inputs)\n",
    "            attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "            masked_embeddings = outputs.last_hidden_state * attention_mask\n",
    "            sum_embeddings = masked_embeddings.sum(dim=1)\n",
    "            sum_mask = attention_mask.sum(dim=1)\n",
    "            mean_pooled = sum_embeddings / sum_mask\n",
    "            all_embeddings.append(mean_pooled.cpu().numpy())\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a78cdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with bert-large-uncased for train set...\n",
      "Encoding text_no_lemma_stem_with_stopwords with bert-large-uncased for val set...\n",
      "Encoding text_no_lemma_stem_with_stopwords with bert-large-uncased for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with roberta-large for train set...\n",
      "Encoding text_no_lemma_stem_with_stopwords with roberta-large for val set...\n",
      "Encoding text_no_lemma_stem_with_stopwords with roberta-large for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with facebook/bart-large for train set...\n",
      "Encoding text_no_lemma_stem_with_stopwords with facebook/bart-large for val set...\n",
      "Encoding text_no_lemma_stem_with_stopwords with facebook/bart-large for test set...\n"
     ]
    }
   ],
   "source": [
    "split_names = ['train', 'val', 'test']\n",
    "dfs = [train_df, val_df, test_df]\n",
    "\n",
    "for model_name in transformer_names:\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = model.to(device) \n",
    "\n",
    "    for column_name in final_combinations:\n",
    "        if not column_name.startswith(\"text_\"): continue\n",
    "        for split_name, df in zip(split_names, dfs):\n",
    "            print(f\"Encoding {column_name} with {model_name} for {split_name} set...\")\n",
    "            texts = df[column_name].astype(str).tolist()\n",
    "            embeddings = mean_pooling(model, tokenizer, texts)\n",
    "            np.save(f\"{model_name.replace('/','-')}_{column_name}_{split_name}_meanpooled.npy\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc95f35",
   "metadata": {},
   "source": [
    "### 3.5 -Domain Specific Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06f1e111",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_models = {\n",
    "    \"finbert\": \"ProsusAI/finbert\",\n",
    "    \"berttweet\": \"vinai/bertweet-base\",\n",
    "    \"fintwitbert\": \"yiyanghkust/finbert-tone\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25ad2490",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "tokenizers = {}\n",
    "for key, name in domain_models.items():\n",
    "    tokenizers[key] = AutoTokenizer.from_pretrained(name)\n",
    "    models[key] = AutoModel.from_pretrained(name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b200e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding text_no_lemma_stem_with_stopwords with finbert for train set...\n",
      "Encoding text_no_lemma_stem_with_stopwords with finbert for val set...\n",
      "Encoding text_no_lemma_stem_with_stopwords with finbert for test set...\n",
      "Encoding text_no_lemma_stem_with_stopwords with berttweet for train set...\n",
      "Encoding text_no_lemma_stem_with_stopwords with berttweet for val set...\n",
      "Encoding text_no_lemma_stem_with_stopwords with berttweet for test set...\n",
      "Encoding text_no_lemma_stem_with_stopwords with fintwitbert for train set...\n",
      "Encoding text_no_lemma_stem_with_stopwords with fintwitbert for val set...\n",
      "Encoding text_no_lemma_stem_with_stopwords with fintwitbert for test set...\n"
     ]
    }
   ],
   "source": [
    "split_names = ['train', 'val', 'test']\n",
    "dfs = [train_df, val_df, test_df]\n",
    "\n",
    "for model_key, model in models.items():\n",
    "    tokenizer = tokenizers[model_key]\n",
    "    for column_name in final_combinations:\n",
    "        if not column_name.startswith(\"text_\"): continue\n",
    "        for split_name, df in zip(split_names, dfs):\n",
    "            print(f\"Encoding {column_name} with {model_key} for {split_name} set...\")\n",
    "            texts = df[column_name].astype(str).tolist()\n",
    "            embeddings = mean_pooling(model, tokenizer, texts)\n",
    "            np.save(f\"{model_key}_{column_name}_{split_name}_meanpooled.npy\", embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
