{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7902e420",
   "metadata": {},
   "source": [
    "## Best model notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414e20d2",
   "metadata": {},
   "source": [
    "### Group 8 Members\n",
    "#### Spring Semester 2024-2025\n",
    "- Alexandre Gonçalves - 20240738\n",
    "- Bráulio Damba - 20240007\n",
    "- Hugo Fonseca - 20240520\n",
    "- Ricardo Pereira - 20240745\n",
    "- Victoria Goon - 20240550"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644bdd93",
   "metadata": {},
   "source": [
    "## 0 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0314c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /Users/ricardo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ricardo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ricardo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/ricardo/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "# Text extraction \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from collections import Counter\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from gensim.models import FastText\n",
    "\n",
    "import scipy.sparse\n",
    "from scipy import sparse\n",
    "\n",
    "import contractions\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "import pickle\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Deep Learning libraries\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, LSTM, Bidirectional, Dropout, Flatten, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Set pd options to display all columns and rows\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "pd.set_option(\"display.max_rows\", 30)\n",
    "pd.set_option('display.max_colwidth', None)  \n",
    "\n",
    "import glob\n",
    "from scipy.sparse import load_npz\n",
    "\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Bidirectional, GRU, Flatten, Dense\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.base import clone\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BartTokenizer,\n",
    "    BartForSequenceClassification,\n",
    "    BartConfig,\n",
    ")\n",
    "import optuna\n",
    "\n",
    "import json\n",
    "\n",
    "# Download required resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"xgboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2681ed",
   "metadata": {},
   "source": [
    "### Import the encoded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292e5606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base directory (where the notebook is)\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "\n",
    "# Construct full paths to the CSV files\n",
    "train_path = os.path.join(BASE_DIR, \"data\", \"train.csv\")\n",
    "test_path = os.path.join(BASE_DIR, \"data\", \"test.csv\")\n",
    "\n",
    "# Load the datasets\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2175b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '.'  # Use '.' if running in the current directory, or the path to your folder\n",
    "\n",
    "# List all .npy and .npz files\n",
    "npy_files = glob.glob(os.path.join(folder, '*.npy'))\n",
    "npz_files = glob.glob(os.path.join(folder, '*.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dd6f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_embeddings = {}\n",
    "\n",
    "for f in npy_files:\n",
    "    name = os.path.splitext(os.path.basename(f))[0]\n",
    "    npy_embeddings[name] = np.load(f, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b42615f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "npz_embeddings = {}\n",
    "\n",
    "for f in npz_files:\n",
    "    name = os.path.splitext(os.path.basename(f))[0]\n",
    "    try:\n",
    "        npz_embeddings[name] = load_npz(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to load '{f}' as a sparse matrix. Error: {e}\")\n",
    "        try:\n",
    "            npz_embeddings[name] = np.load(f)\n",
    "            print(f\"Loaded '{f}' as dense array (np.load).\")\n",
    "        except Exception as e2:\n",
    "            print(f\"Failed to load '{f}' as dense array too. Skipping. Error: {e2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d15404b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using stratify to maintain the distribution of classes in the train, validation, and test sets \n",
    "# As our dataset is quite small, we use 80% for training, and split the remaining 20% into validation and test sets (10% each).\n",
    "\n",
    "train_df, val_test_df = train_test_split(df_train, test_size=0.2, stratify=df_train['label'], random_state=42)\n",
    "val_df, test_df = train_test_split(val_test_df, test_size=0.5, stratify=val_test_df['label'], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39b95e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['label']\n",
    "y_val = val_df['label']\n",
    "y_test = test_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d44c6a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters loaded: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 5, 'weight_decay': 0.0, 'warmup_steps': 0, 'gradient_accumulation_steps': 1, 'dropout': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Load from file\n",
    "with open(\"best_bart_params.json\", \"r\") as f:\n",
    "    best_params = json.load(f)\n",
    "\n",
    "print(\"Best parameters loaded:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825ab68c",
   "metadata": {},
   "source": [
    "## 1 - Hyperparameter tuning BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b543dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\", use_fast=True)\n",
    "\n",
    "# Dataset construction\n",
    "train_df = Dataset.from_dict({\n",
    "    \"text_no_lemma_stem_with_stopwords\": npy_embeddings[\"train_text_no_lemma_stem_with_stopwords\"],\n",
    "    \"label\": y_train.tolist()\n",
    "})\n",
    "val_df = Dataset.from_dict({\n",
    "    \"text_no_lemma_stem_with_stopwords\": npy_embeddings[\"val_text_no_lemma_stem_with_stopwords\"],\n",
    "    \"label\": y_val.tolist()\n",
    "})\n",
    "test_df = Dataset.from_dict({\n",
    "    \"text_no_lemma_stem_with_stopwords\": npy_embeddings[\"test_text_no_lemma_stem_with_stopwords\"],\n",
    "    \"label\": y_test.tolist()\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54753faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization \n",
    "def tokenize_dataset(dataset):\n",
    "    return dataset.map(\n",
    "        lambda examples: tokenizer(\n",
    "            examples[\"text_no_lemma_stem_with_stopwords\"],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=64\n",
    "        ),\n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "# Metric for optimization\n",
    "def compute_metrics(pred):\n",
    "    try:\n",
    "        logits = pred.predictions[0] if isinstance(pred.predictions, tuple) else pred.predictions\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        labels = pred.label_ids\n",
    "        return {\"macro_f1\": f1_score(labels, preds, average=\"macro\")}\n",
    "    except Exception as e:\n",
    "        print(f\"Metric computation error: {e}\")\n",
    "        return {\"macro_f1\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb359578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna search space\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_categorical(\"learning_rate\", [1e-5, 2e-5, 3e-5, 5e-5]),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16]),\n",
    "        \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [3, 4, 5]),\n",
    "        \"weight_decay\": trial.suggest_categorical(\"weight_decay\", [0.0, 0.01]),\n",
    "        \"warmup_steps\": trial.suggest_categorical(\"warmup_steps\", [0, 100, 500]),\n",
    "        \"gradient_accumulation_steps\": trial.suggest_categorical(\"gradient_accumulation_steps\", [1, 2]),\n",
    "        \"dropout\": trial.suggest_categorical(\"dropout\", [0.1, 0.2]),\n",
    "    }\n",
    "\n",
    "# Model init\n",
    "def model_init(trial):\n",
    "    dropout = trial.suggest_categorical(\"dropout\", [0.1, 0.2])\n",
    "    config = BartConfig.from_pretrained(\"facebook/bart-base\", num_labels=3, attention_dropout=dropout, dropout=dropout)\n",
    "    return BartForSequenceClassification.from_pretrained(\"facebook/bart-base\", config=config)\n",
    "\n",
    "# TrainingArguments\n",
    "def build_training_args(trial):\n",
    "    return TrainingArguments(\n",
    "        output_dir=\"./results_bart_tune\",\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=trial.suggest_categorical(\"learning_rate\", [1e-5, 2e-5, 3e-5, 5e-5]),\n",
    "        per_device_train_batch_size=trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16]),\n",
    "        num_train_epochs=trial.suggest_categorical(\"num_train_epochs\", [3, 4, 5]),\n",
    "        weight_decay=trial.suggest_categorical(\"weight_decay\", [0.0, 0.01]),\n",
    "        warmup_steps=trial.suggest_categorical(\"warmup_steps\", [0, 100, 500]),\n",
    "        gradient_accumulation_steps=trial.suggest_categorical(\"gradient_accumulation_steps\", [1, 2]),\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"macro_f1\",\n",
    "        greater_is_better=True,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "\n",
    "def objective(trial):\n",
    "    train_dataset = tokenize_dataset(train_df).rename_column(\"label\", \"labels\")\n",
    "    val_dataset = tokenize_dataset(val_df).rename_column(\"label\", \"labels\")\n",
    "\n",
    "    train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "    val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model_init=lambda: model_init(trial),\n",
    "        args=build_training_args(trial),\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_result = trainer.evaluate()\n",
    "    return eval_result[\"eval_macro_f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2af06a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 00:51:36,250] A new study created in memory with name: no-name-9be3e78d-57c4-4b58-b786-561890b1ac5a\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 13716.77 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11663.17 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3820' max='3820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3820/3820 10:38, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.638800</td>\n",
       "      <td>0.460890</td>\n",
       "      <td>0.787788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.462700</td>\n",
       "      <td>0.441597</td>\n",
       "      <td>0.818824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.392400</td>\n",
       "      <td>0.429596</td>\n",
       "      <td>0.825238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.334500</td>\n",
       "      <td>0.432973</td>\n",
       "      <td>0.830969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 01:02:23,916] Trial 0 finished with value: 0.8309686677181131 and parameters: {'learning_rate': 1e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 4, 'weight_decay': 0.01, 'warmup_steps': 100, 'gradient_accumulation_steps': 1, 'dropout': 0.1}. Best is trial 0 with value: 0.8309686677181131.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 13923.08 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11654.88 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='717' max='717' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [717/717 05:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.841800</td>\n",
       "      <td>0.604960</td>\n",
       "      <td>0.748586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.478839</td>\n",
       "      <td>0.768831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.568300</td>\n",
       "      <td>0.456748</td>\n",
       "      <td>0.786488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 01:08:02,000] Trial 1 finished with value: 0.7864876891056505 and parameters: {'learning_rate': 1e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 3, 'weight_decay': 0.01, 'warmup_steps': 100, 'gradient_accumulation_steps': 2, 'dropout': 0.2}. Best is trial 0 with value: 0.8309686677181131.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 13890.78 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11233.04 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2865' max='2865' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2865/2865 07:53, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.707600</td>\n",
       "      <td>0.501660</td>\n",
       "      <td>0.768498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.512100</td>\n",
       "      <td>0.525204</td>\n",
       "      <td>0.794742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.422600</td>\n",
       "      <td>0.503320</td>\n",
       "      <td>0.808938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 01:16:04,675] Trial 2 finished with value: 0.8089379880363371 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 3, 'weight_decay': 0.01, 'warmup_steps': 100, 'gradient_accumulation_steps': 1, 'dropout': 0.2}. Best is trial 0 with value: 0.8309686677181131.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 13877.73 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11320.24 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1434' max='1434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1434/1434 06:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.703800</td>\n",
       "      <td>0.463265</td>\n",
       "      <td>0.780278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.472600</td>\n",
       "      <td>0.422532</td>\n",
       "      <td>0.811223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.301900</td>\n",
       "      <td>0.380167</td>\n",
       "      <td>0.836306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 01:22:45,745] Trial 3 finished with value: 0.8363058639654385 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 3, 'weight_decay': 0.0, 'warmup_steps': 500, 'gradient_accumulation_steps': 2, 'dropout': 0.1}. Best is trial 3 with value: 0.8363058639654385.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 13777.64 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11711.27 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3820' max='3820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3820/3820 10:32, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.638800</td>\n",
       "      <td>0.460891</td>\n",
       "      <td>0.787788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.462700</td>\n",
       "      <td>0.441598</td>\n",
       "      <td>0.818824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.392400</td>\n",
       "      <td>0.429640</td>\n",
       "      <td>0.825238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.334500</td>\n",
       "      <td>0.432878</td>\n",
       "      <td>0.828969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 01:33:27,956] Trial 4 finished with value: 0.8289689473624101 and parameters: {'learning_rate': 1e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 4, 'weight_decay': 0.01, 'warmup_steps': 100, 'gradient_accumulation_steps': 1, 'dropout': 0.1}. Best is trial 3 with value: 0.8363058639654385.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 14017.17 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11650.51 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1912' max='1912' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1912/1912 08:09, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.693000</td>\n",
       "      <td>0.458175</td>\n",
       "      <td>0.775632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.520700</td>\n",
       "      <td>0.515046</td>\n",
       "      <td>0.790318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.432500</td>\n",
       "      <td>0.416983</td>\n",
       "      <td>0.820058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.414531</td>\n",
       "      <td>0.816970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 01:41:47,065] Trial 5 finished with value: 0.8200583693947268 and parameters: {'learning_rate': 3e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 4, 'weight_decay': 0.01, 'warmup_steps': 0, 'gradient_accumulation_steps': 1, 'dropout': 0.2}. Best is trial 3 with value: 0.8363058639654385.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 13846.31 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11502.11 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1434' max='1434' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1434/1434 06:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.706200</td>\n",
       "      <td>0.487944</td>\n",
       "      <td>0.771932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.503200</td>\n",
       "      <td>0.524727</td>\n",
       "      <td>0.795593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.390500</td>\n",
       "      <td>0.451313</td>\n",
       "      <td>0.810678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 01:48:30,523] Trial 6 finished with value: 0.8106780662970721 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 3, 'weight_decay': 0.01, 'warmup_steps': 100, 'gradient_accumulation_steps': 2, 'dropout': 0.2}. Best is trial 3 with value: 0.8363058639654385.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 14045.71 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11226.96 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1912' max='1912' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1912/1912 08:09, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.733200</td>\n",
       "      <td>0.466438</td>\n",
       "      <td>0.775077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.491400</td>\n",
       "      <td>0.402788</td>\n",
       "      <td>0.830141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>0.373111</td>\n",
       "      <td>0.841026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.276400</td>\n",
       "      <td>0.389027</td>\n",
       "      <td>0.836698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 01:56:48,380] Trial 7 finished with value: 0.8410262724963858 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 4, 'weight_decay': 0.01, 'warmup_steps': 500, 'gradient_accumulation_steps': 1, 'dropout': 0.1}. Best is trial 7 with value: 0.8410262724963858.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 14090.85 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11502.61 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1912' max='1912' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1912/1912 08:07, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.630400</td>\n",
       "      <td>0.440314</td>\n",
       "      <td>0.791306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.402500</td>\n",
       "      <td>0.423279</td>\n",
       "      <td>0.815952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.267300</td>\n",
       "      <td>0.441139</td>\n",
       "      <td>0.824498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.189100</td>\n",
       "      <td>0.519605</td>\n",
       "      <td>0.833499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 02:05:05,709] Trial 8 finished with value: 0.8334989994313995 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 4, 'weight_decay': 0.01, 'warmup_steps': 100, 'gradient_accumulation_steps': 1, 'dropout': 0.1}. Best is trial 7 with value: 0.8410262724963858.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 13928.27 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11642.54 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1912' max='1912' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1912/1912 08:46, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.640300</td>\n",
       "      <td>0.443667</td>\n",
       "      <td>0.788231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.431800</td>\n",
       "      <td>0.404021</td>\n",
       "      <td>0.818580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.329000</td>\n",
       "      <td>0.381881</td>\n",
       "      <td>0.835817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.258100</td>\n",
       "      <td>0.398963</td>\n",
       "      <td>0.835808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 02:14:02,497] Trial 9 finished with value: 0.8358174806741087 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 4, 'weight_decay': 0.01, 'warmup_steps': 100, 'gradient_accumulation_steps': 2, 'dropout': 0.1}. Best is trial 7 with value: 0.8410262724963858.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 14089.44 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11326.81 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2390' max='2390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2390/2390 09:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.733200</td>\n",
       "      <td>0.466350</td>\n",
       "      <td>0.777262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.494000</td>\n",
       "      <td>0.397394</td>\n",
       "      <td>0.825229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.357800</td>\n",
       "      <td>0.374417</td>\n",
       "      <td>0.845337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.278000</td>\n",
       "      <td>0.431926</td>\n",
       "      <td>0.833636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.220800</td>\n",
       "      <td>0.424164</td>\n",
       "      <td>0.843684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 02:24:03,106] Trial 10 finished with value: 0.845336916582828 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 5, 'weight_decay': 0.0, 'warmup_steps': 500, 'gradient_accumulation_steps': 1, 'dropout': 0.1}. Best is trial 10 with value: 0.845336916582828.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 14124.01 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11618.27 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2390' max='2390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2390/2390 09:51, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.733200</td>\n",
       "      <td>0.466350</td>\n",
       "      <td>0.777262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>0.397678</td>\n",
       "      <td>0.830349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.358200</td>\n",
       "      <td>0.377443</td>\n",
       "      <td>0.842340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.277700</td>\n",
       "      <td>0.439039</td>\n",
       "      <td>0.832428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.222100</td>\n",
       "      <td>0.424668</td>\n",
       "      <td>0.838477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 02:34:04,840] Trial 11 finished with value: 0.8423397640276044 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 5, 'weight_decay': 0.0, 'warmup_steps': 500, 'gradient_accumulation_steps': 1, 'dropout': 0.1}. Best is trial 10 with value: 0.845336916582828.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 14210.22 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11657.74 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2390' max='2390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2390/2390 09:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.733200</td>\n",
       "      <td>0.466348</td>\n",
       "      <td>0.777262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.494100</td>\n",
       "      <td>0.397555</td>\n",
       "      <td>0.829625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.357300</td>\n",
       "      <td>0.373614</td>\n",
       "      <td>0.846039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.279300</td>\n",
       "      <td>0.437355</td>\n",
       "      <td>0.831194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.221700</td>\n",
       "      <td>0.422925</td>\n",
       "      <td>0.842644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 02:44:10,234] Trial 12 finished with value: 0.8460390703674294 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 5, 'weight_decay': 0.0, 'warmup_steps': 500, 'gradient_accumulation_steps': 1, 'dropout': 0.1}. Best is trial 12 with value: 0.8460390703674294.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 14035.56 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11572.07 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2390' max='2390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2390/2390 09:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.733200</td>\n",
       "      <td>0.466352</td>\n",
       "      <td>0.777262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>0.397537</td>\n",
       "      <td>0.826481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.356400</td>\n",
       "      <td>0.377076</td>\n",
       "      <td>0.845246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.278300</td>\n",
       "      <td>0.435076</td>\n",
       "      <td>0.834183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.221000</td>\n",
       "      <td>0.421978</td>\n",
       "      <td>0.839911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 02:54:14,656] Trial 13 finished with value: 0.8452461291325379 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 5, 'weight_decay': 0.0, 'warmup_steps': 500, 'gradient_accumulation_steps': 1, 'dropout': 0.1}. Best is trial 12 with value: 0.8460390703674294.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 13951.58 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11691.87 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2390' max='2390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2390/2390 09:56, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.733200</td>\n",
       "      <td>0.466363</td>\n",
       "      <td>0.777262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.494100</td>\n",
       "      <td>0.395911</td>\n",
       "      <td>0.825790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.357400</td>\n",
       "      <td>0.375841</td>\n",
       "      <td>0.848531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.278100</td>\n",
       "      <td>0.435420</td>\n",
       "      <td>0.839134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.221400</td>\n",
       "      <td>0.422327</td>\n",
       "      <td>0.843383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 03:04:20,745] Trial 14 finished with value: 0.8485314473052892 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 5, 'weight_decay': 0.0, 'warmup_steps': 500, 'gradient_accumulation_steps': 1, 'dropout': 0.1}. Best is trial 14 with value: 0.8485314473052892.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 14059.82 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11673.55 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2390' max='2390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2390/2390 09:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.602400</td>\n",
       "      <td>0.444756</td>\n",
       "      <td>0.795174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.418200</td>\n",
       "      <td>0.439587</td>\n",
       "      <td>0.819462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.329800</td>\n",
       "      <td>0.397937</td>\n",
       "      <td>0.837045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.427904</td>\n",
       "      <td>0.836327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>0.414799</td>\n",
       "      <td>0.849389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 03:14:25,330] Trial 15 finished with value: 0.8493888441221237 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 5, 'weight_decay': 0.0, 'warmup_steps': 0, 'gradient_accumulation_steps': 1, 'dropout': 0.1}. Best is trial 15 with value: 0.8493888441221237.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 14047.45 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11444.47 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2390' max='2390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2390/2390 09:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.602400</td>\n",
       "      <td>0.444751</td>\n",
       "      <td>0.795174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.418200</td>\n",
       "      <td>0.439539</td>\n",
       "      <td>0.819462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.329800</td>\n",
       "      <td>0.397917</td>\n",
       "      <td>0.837045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.427974</td>\n",
       "      <td>0.835001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>0.414612</td>\n",
       "      <td>0.849389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 03:24:28,807] Trial 16 finished with value: 0.8493888441221237 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 5, 'weight_decay': 0.0, 'warmup_steps': 0, 'gradient_accumulation_steps': 1, 'dropout': 0.1}. Best is trial 15 with value: 0.8493888441221237.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 14089.42 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11670.86 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2390' max='2390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2390/2390 09:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.602400</td>\n",
       "      <td>0.444767</td>\n",
       "      <td>0.795174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.418200</td>\n",
       "      <td>0.439764</td>\n",
       "      <td>0.819462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.329800</td>\n",
       "      <td>0.398267</td>\n",
       "      <td>0.835877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.428271</td>\n",
       "      <td>0.835170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>0.414892</td>\n",
       "      <td>0.849389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 03:34:30,796] Trial 17 finished with value: 0.8493888441221237 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 5, 'weight_decay': 0.0, 'warmup_steps': 0, 'gradient_accumulation_steps': 1, 'dropout': 0.1}. Best is trial 15 with value: 0.8493888441221237.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 14130.26 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11372.04 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1195' max='1195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1195/1195 08:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.717100</td>\n",
       "      <td>0.485494</td>\n",
       "      <td>0.762149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.531700</td>\n",
       "      <td>0.548557</td>\n",
       "      <td>0.763370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.435400</td>\n",
       "      <td>0.453943</td>\n",
       "      <td>0.806307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.376700</td>\n",
       "      <td>0.461829</td>\n",
       "      <td>0.823120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.434850</td>\n",
       "      <td>0.828782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 03:43:36,200] Trial 18 finished with value: 0.8287824741103872 and parameters: {'learning_rate': 5e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 5, 'weight_decay': 0.0, 'warmup_steps': 0, 'gradient_accumulation_steps': 2, 'dropout': 0.2}. Best is trial 15 with value: 0.8493888441221237.\n",
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 14048.56 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11627.89 examples/s]\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/4189195025.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2390' max='2390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2390/2390 09:55, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.602400</td>\n",
       "      <td>0.444764</td>\n",
       "      <td>0.795174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.418200</td>\n",
       "      <td>0.439712</td>\n",
       "      <td>0.819462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.329900</td>\n",
       "      <td>0.397680</td>\n",
       "      <td>0.837045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.428252</td>\n",
       "      <td>0.836739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>0.415244</td>\n",
       "      <td>0.849389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='120' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/120 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-14 03:53:41,704] Trial 19 finished with value: 0.8493888441221237 and parameters: {'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 5, 'weight_decay': 0.0, 'warmup_steps': 0, 'gradient_accumulation_steps': 1, 'dropout': 0.1}. Best is trial 15 with value: 0.8493888441221237.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found:\n",
      "{'learning_rate': 2e-05, 'per_device_train_batch_size': 16, 'num_train_epochs': 5, 'weight_decay': 0.0, 'warmup_steps': 0, 'gradient_accumulation_steps': 1, 'dropout': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Run Optuna search\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Display best hyperparameters\n",
    "print(\"Best hyperparameters found:\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e76e7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters saved to best_bart_params.json\n"
     ]
    }
   ],
   "source": [
    "# Save to file\n",
    "with open(\"best_bart_params.json\", \"w\") as f:\n",
    "    json.dump(best_params, f)\n",
    "\n",
    "print(\"Best parameters saved to best_bart_params.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3477216a",
   "metadata": {},
   "source": [
    "## 2 - BART with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03d7642e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7634/7634 [00:00<00:00, 14283.06 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11247.88 examples/s]\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_6229/216467845.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  final_trainer = Trainer(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2390' max='2390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2390/2390 10:04, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.601400</td>\n",
       "      <td>0.431282</td>\n",
       "      <td>0.794180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.422900</td>\n",
       "      <td>0.417210</td>\n",
       "      <td>0.825260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.332000</td>\n",
       "      <td>0.401112</td>\n",
       "      <td>0.827630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.264800</td>\n",
       "      <td>0.431616</td>\n",
       "      <td>0.841673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.213400</td>\n",
       "      <td>0.428017</td>\n",
       "      <td>0.831715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7756    0.8403    0.8067       144\n",
      "           1     0.8324    0.7760    0.8032       192\n",
      "           2     0.9144    0.9159    0.9151       618\n",
      "\n",
      "    accuracy                         0.8763       954\n",
      "   macro avg     0.8408    0.8441    0.8417       954\n",
      "weighted avg     0.8769    0.8763    0.8762       954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize \n",
    "train_dataset = tokenize_dataset(train_df).rename_column(\"label\", \"labels\")\n",
    "val_dataset = tokenize_dataset(val_df).rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "best_config = BartConfig.from_pretrained(\n",
    "    \"facebook/bart-base\",\n",
    "    num_labels=3,\n",
    "    attention_dropout=best_params[\"dropout\"],\n",
    "    dropout=best_params[\"dropout\"],\n",
    ")\n",
    "best_model = BartForSequenceClassification.from_pretrained(\"facebook/bart-base\", config=best_config).to(device)\n",
    "\n",
    "# Set training arguments\n",
    "best_args = TrainingArguments(\n",
    "    output_dir=\"./results_bart_final\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_params[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=best_params[\"num_train_epochs\"],\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    "    warmup_steps=best_params[\"warmup_steps\"],\n",
    "    gradient_accumulation_steps=best_params[\"gradient_accumulation_steps\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Final Trainer\n",
    "final_trainer = Trainer(\n",
    "    model=best_model,\n",
    "    args=best_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "# Train on best configuration\n",
    "final_trainer.train()\n",
    "\n",
    "# Predict and evaluate\n",
    "preds = final_trainer.predict(val_dataset)\n",
    "logits = preds.predictions[0] if isinstance(preds.predictions, tuple) else preds.predictions\n",
    "y_pred = np.argmax(logits, axis=1)\n",
    "y_true = np.array(val_df[\"label\"])\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a950b9",
   "metadata": {},
   "source": [
    "## 3 - Dealing with Class Imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84909a78",
   "metadata": {},
   "source": [
    "### 3.2 - BART with best hyperparameters with random undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0117617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3462/3462 [00:00<00:00, 12517.98 examples/s]\n",
      "Map: 100%|██████████| 954/954 [00:00<00:00, 11535.84 examples/s]\n",
      "Some weights of BartForSequenceClassification were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/var/folders/vb/r_cbg99j0dj7dml3jnpfyl9h0000gn/T/ipykernel_21134/607631107.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  final_trainer = Trainer(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1085' max='1085' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1085/1085 05:47, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Macro F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.835100</td>\n",
       "      <td>0.559184</td>\n",
       "      <td>0.742565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.573500</td>\n",
       "      <td>0.487785</td>\n",
       "      <td>0.785934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.450900</td>\n",
       "      <td>0.495297</td>\n",
       "      <td>0.786821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.365700</td>\n",
       "      <td>0.582156</td>\n",
       "      <td>0.768443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.310700</td>\n",
       "      <td>0.547270</td>\n",
       "      <td>0.784043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/transformers/configuration_utils.py:394: UserWarning: Some non-default generation parameters are set in the model config. These should go into either a) `model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).This warning will become an exception in the future.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight'].\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6200    0.8611    0.7209       144\n",
      "           1     0.7256    0.8125    0.7666       192\n",
      "           2     0.9369    0.8172    0.8729       618\n",
      "\n",
      "    accuracy                         0.8229       954\n",
      "   macro avg     0.7608    0.8303    0.7868       954\n",
      "weighted avg     0.8465    0.8229    0.8286       954\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer function\n",
    "text_key = \"text_no_lemma_stem_with_stopwords\"\n",
    "\n",
    "def tokenize_dataset(dataset, text_key):\n",
    "    return dataset.map(lambda e: tokenizer(e[text_key], padding=\"max_length\", truncation=True, max_length=64), batched=True)\n",
    "\n",
    "# Create original training DataFrame\n",
    "train_texts = npy_embeddings[\"train_text_no_lemma_stem_with_stopwords\"]\n",
    "train_labels = y_train\n",
    "train_df_raw = pd.DataFrame({\n",
    "    \"text\": train_texts,\n",
    "    \"label\": train_labels\n",
    "})\n",
    "\n",
    "# Random Undersampling\n",
    "min_class_size = train_df_raw['label'].value_counts().min()\n",
    "df_undersampled = pd.concat([\n",
    "    resample(group, replace=False, n_samples=min_class_size, random_state=42)\n",
    "    for _, group in train_df_raw.groupby('label')\n",
    "])\n",
    "df_undersampled = df_undersampled.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Convert to HuggingFace Datasets\n",
    "train_dataset = Dataset.from_pandas(df_undersampled.rename(columns={\"text\": text_key}))\n",
    "val_dataset = Dataset.from_dict({\n",
    "    text_key: npy_embeddings[\"val_text_no_lemma_stem_with_stopwords\"],\n",
    "    \"label\": y_val.tolist()\n",
    "})\n",
    "\n",
    "# Tokenize \n",
    "train_dataset = tokenize_dataset(train_dataset, text_key).rename_column(\"label\", \"labels\")\n",
    "val_dataset = tokenize_dataset(val_dataset, text_key).rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Load model with best config from Optuna \n",
    "best_config = BartConfig.from_pretrained(\n",
    "    \"facebook/bart-base\",\n",
    "    num_labels=3,\n",
    "    attention_dropout=best_params[\"dropout\"],\n",
    "    dropout=best_params[\"dropout\"]\n",
    ")\n",
    "best_model = BartForSequenceClassification.from_pretrained(\"facebook/bart-base\", config=best_config).to(device)\n",
    "\n",
    "best_args = TrainingArguments(\n",
    "    output_dir=\"./results_bart_undersample\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_params[\"per_device_train_batch_size\"],\n",
    "    num_train_epochs=best_params[\"num_train_epochs\"],\n",
    "    weight_decay=best_params[\"weight_decay\"],\n",
    "    warmup_steps=best_params[\"warmup_steps\"],\n",
    "    gradient_accumulation_steps=best_params[\"gradient_accumulation_steps\"],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"macro_f1\",\n",
    "    greater_is_better=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "final_trainer = Trainer(\n",
    "    model=best_model,\n",
    "    args=best_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")\n",
    "\n",
    "final_trainer.train()\n",
    "\n",
    "preds = final_trainer.predict(val_dataset)\n",
    "logits = preds.predictions[0] if isinstance(preds.predictions, tuple) else preds.predictions\n",
    "y_pred = np.argmax(logits, axis=1)\n",
    "y_true = np.array(y_val)\n",
    "\n",
    "print(classification_report(y_true, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fef545",
   "metadata": {},
   "source": [
    "As we can see, applying the random undersampling didnt improve the overall performance of the BART transformer, so we will evaluate performance on test, and later on making the predictions using the **BART transformer with the best parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4bd077",
   "metadata": {},
   "source": [
    "## 4 - Checking BART with best hyperparameters on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf1711a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 955/955 [00:00<00:00, 11959.61 examples/s]\n",
      "/Users/ricardo/miniconda3/envs/nlp_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Bearish     0.7200    0.7500    0.7347       144\n",
      "     Bullish     0.8361    0.7927    0.8138       193\n",
      "     Neutral     0.8971    0.9029    0.9000       618\n",
      "\n",
      "    accuracy                         0.8576       955\n",
      "   macro avg     0.8177    0.8152    0.8162       955\n",
      "weighted avg     0.8581    0.8576    0.8577       955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_map = {0: \"Bearish\", 1: \"Bullish\", 2: \"Neutral\"}\n",
    "\n",
    "df_test_raw = test_df.to_pandas().reset_index(drop=True)\n",
    "\n",
    "# Track valid rows\n",
    "valid_indices = []\n",
    "valid_texts = []\n",
    "\n",
    "for i, text in enumerate(df_test_raw[\"text_no_lemma_stem_with_stopwords\"]):\n",
    "    if isinstance(text, str) and text.strip():\n",
    "        valid_indices.append(i)\n",
    "        valid_texts.append(text)\n",
    "\n",
    "# Tokenize only valid rows\n",
    "valid_dataset = Dataset.from_pandas(pd.DataFrame({\n",
    "    \"text_no_lemma_stem_with_stopwords\": valid_texts\n",
    "}))\n",
    "valid_dataset = tokenize_dataset(valid_dataset)\n",
    "valid_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "test_preds = final_trainer.predict(valid_dataset)\n",
    "test_logits = test_preds.predictions[0] if isinstance(test_preds.predictions, tuple) else test_preds.predictions\n",
    "test_y_pred_valid = np.argmax(test_logits, axis=1)\n",
    "\n",
    "# Reconstruct full-length predictions\n",
    "test_y_pred = [None] * len(df_test_raw)\n",
    "for idx, pred in zip(valid_indices, test_y_pred_valid):\n",
    "    test_y_pred[idx] = pred\n",
    "\n",
    "df_test_raw[\"true_label\"] = y_test.values\n",
    "df_test_raw[\"pred_label\"] = test_y_pred\n",
    "\n",
    "df_eval = df_test_raw[df_test_raw[\"pred_label\"].notna()].copy()\n",
    "df_eval[\"pred_label\"] = df_eval[\"pred_label\"].astype(int)\n",
    "\n",
    "test_y_true = df_eval[\"true_label\"]\n",
    "test_y_pred = df_eval[\"pred_label\"]\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(test_y_true, test_y_pred, target_names=label_map.values(), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1aea76c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate the confusion matrix\n",
    "cm = confusion_matrix(test_y_true, test_y_pred)\n",
    "\n",
    "class_names = [\"Bearish\", \"Bullish\", \"Neutral\"]\n",
    "\n",
    "# Plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "disp.plot(cmap=\"Blues\", ax=ax, values_format='d')\n",
    "\n",
    "plt.title(\"Confusion Matrix - Test Set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a70ba0",
   "metadata": {},
   "source": [
    "The confusion matrix reveals that the BART model performs best at identifying Neutral and Bullish tweets, correctly classifying approximately 90% of Neutral and 78% of Bullish samples. However, the model shows some difficulty distinguishing Bearish tweets, with a notable portion being misclassified as Neutral. This confusion might stem from the linguistic similarities between Neutral and Bearish expressions, especially in financial contexts where subtle phrasing can blur the sentiment. Additionally, if the training data had fewer Bearish examples, the model may not have learned enough distinctive patterns for that class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07c9fdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Misclassified examples for true class 'Bearish':\n",
      "\n",
      "- Text: opec deal is much weaker than it look\n",
      "  True: Bearish, Predicted: Neutral\n",
      "\n",
      "- Text: fed policymak work to limit damag as pandem put you s economi on paus url url\n",
      "  True: Bearish, Predicted: Neutral\n",
      "\n",
      "- Text: la jolla pharmaceut to reassess continu develop of ljpc base on clinic result\n",
      "  True: Bearish, Predicted: Neutral\n",
      "\n",
      "- Text: ticker western digit may have troubl push stock price higher subscrib to seek alpha for more … url\n",
      "  True: Bearish, Predicted: Bullish\n",
      "\n",
      "- Text: introduc lannett compani nyse lci the stock that tank\n",
      "  True: Bearish, Predicted: Neutral\n",
      "\n",
      "\n",
      "Misclassified examples for true class 'Bullish':\n",
      "\n",
      "- Text: whi intesa sanpaolo s p a s bit isp high p e ratio is not necessarili a bad thing\n",
      "  True: Bullish, Predicted: Neutral\n",
      "\n",
      "- Text: octob architectur bill index final sign of strength read more and get updat on ani stock … url\n",
      "  True: Bullish, Predicted: Neutral\n",
      "\n",
      "- Text: ticker move good today just play the movement but do not get stuck hold the bag\n",
      "  True: Bullish, Predicted: Neutral\n",
      "\n",
      "- Text: whi mti food group inc s tse mti high p e ratio is not necessarili a bad thing\n",
      "  True: Bullish, Predicted: Neutral\n",
      "\n",
      "- Text: one analyst say td could unlock signific valu by sell it minor stake in ameritrad after it purchas … url\n",
      "  True: Bullish, Predicted: Neutral\n",
      "\n",
      "\n",
      "Misclassified examples for true class 'Neutral':\n",
      "\n",
      "- Text: appl cancel the premier of the banker from the afi festiv as it look into unspecifi concern about the … url\n",
      "  True: Neutral, Predicted: Bearish\n",
      "\n",
      "- Text: ‘ phase ’ trade deal is progress say hill co ’ s ceo\n",
      "  True: Neutral, Predicted: Bullish\n",
      "\n",
      "- Text: anoth bridg ha collaps in itali url\n",
      "  True: Neutral, Predicted: Bearish\n",
      "\n",
      "- Text: ticker foot locker cannot get much better follow thi and ani other stock on seek alpha url url\n",
      "  True: Neutral, Predicted: Bullish\n",
      "\n",
      "- Text: emir pare back wide-bodi airbu order amid fleet review\n",
      "  True: Neutral, Predicted: Bearish\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Identify misclassified samples \n",
    "misclassified = df_eval[test_y_true != test_y_pred]\n",
    "\n",
    "# Show 5 misclassified examples per true class\n",
    "for label_id in sorted(test_y_true.unique()):\n",
    "    print(f\"\\nMisclassified examples for true class '{label_map[label_id]}':\\n\")\n",
    "    \n",
    "    subset = misclassified[misclassified[\"true_label\"] == label_id]\n",
    "    sample = subset.sample(n=min(5, len(subset)), random_state=42)\n",
    "    \n",
    "    for _, row in sample.iterrows():\n",
    "        print(f\"- Text: {row['text_no_lemma_stem_with_stopwords']}\")\n",
    "        print(f\"  True: {label_map[row['true_label']]}, Predicted: {label_map[row['pred_label']]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812a34e7",
   "metadata": {},
   "source": [
    "## 5 - Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6ce8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2388/2388 [00:01<00:00, 2275.86 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test predictions saved to pred_bart.csv\n"
     ]
    }
   ],
   "source": [
    "df_test = df_test.rename(columns={\"text\": \"text_no_lemma_stem_with_stopwords\"})\n",
    "\n",
    "hf_test_dataset = Dataset.from_pandas(df_test)\n",
    "\n",
    "# Tokenize the test dataset\n",
    "hf_test_dataset = hf_test_dataset.map(\n",
    "    lambda examples: tokenizer(\n",
    "        examples[\"text_no_lemma_stem_with_stopwords\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=64\n",
    "    ),\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# Prepare for model input\n",
    "hf_test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "preds = final_trainer.predict(hf_test_dataset)\n",
    "logits = preds.predictions[0] if isinstance(preds.predictions, tuple) else preds.predictions\n",
    "test_predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "# Export predictions to CSV\n",
    "df_submission = pd.DataFrame({\n",
    "    \"id\": df_test[\"id\"],  \n",
    "    \"label\": test_predictions\n",
    "})\n",
    "df_submission.to_csv(\"pred_bart.csv\", index=False)\n",
    "\n",
    "print(\"Test predictions saved to pred_bart.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
